

```{r}
library(tidyverse)
```



```{r}
# Example of how decision flowcharts can mislead
flowchart_problems <- tibble(
    assumption = c("Normality", "Homogeneity of variance", "Independence"),
    reality = c("Ecological data often skewed/zero-inflated", 
                "Variance typically increases with mean in count data",
                "Spatial/temporal autocorrelation common in field studies"),
    consequence = c("Inappropriate transformations applied",
                    "Inflated Type I error rates",
                    "Underestimated uncertainty in parameter estimates")
)

knitr::kable(flowchart_problems, caption = "Common violated assumptions in ecotoxicological flowcharts")
```



## Incorrect Pathways Lead to Erroneous Conclusions

```{r}
# Simulation demonstrating how following a flawed decision path leads to incorrect conclusions
set.seed(123)
simulate_wrong_path <- function(n = 100) {
  # Generate non-normal data with heterogeneous variance
  treatment <- rep(c("Control", "Low", "Medium", "High"), each = n/4)
  means <- c(10, 12, 15, 25)
  sds <- c(2, 3, 5, 10)  # Increasing variance with mean
  
  # Generate data
  response <- numeric(n)
  for(i in 1:n) {
    group <- which(c("Control", "Low", "Medium", "High") == treatment[i])
    response[i] <- rlnorm(1, log(means[group]), 0.3 * sds[group]/means[group])
  }
  
  data <- tibble(treatment = factor(treatment), response = response)
  
  # Wrong path: ANOVA despite heterogeneity
  wrong_model <- aov(response ~ treatment, data = data)
  wrong_p <- summary(wrong_model)[[1]]["treatment", "Pr(>F)"]
  
  # Right path: Use robust method
  library(robustbase)
  right_model <- oneway.test(response ~ treatment, data = data, var.equal = FALSE)
  right_p <- right_model$p.value
  
  return(list(data = data, 
              wrong_p = wrong_p, 
              right_p = right_p,
              difference = abs(wrong_p - right_p)))
}

results <- simulate_wrong_path()
results
```

## Robust Alternatives to Traditional Approaches

```{r}
# Table of robust alternatives
robust_alternatives <- tibble(
  traditional = c("ANOVA", "t-test", "Linear regression", 
                  "Parametric NOEC/LOEC", "Parametric ECx"),
  robust_alternative = c("Welch ANOVA / Robust ANOVA", 
                        "Wilcoxon test / Bootstrap methods",
                        "Robust regression / Quantile regression",
                        "Non-parametric trend tests (Jonckheere-Terpstra)",
                        "Bootstrapped ECx with confidence intervals"),
  advantages = c("Handles heteroscedasticity", 
                "No normality assumption required",
                "Resistant to outliers and influential points",
                "More powerful for monotonic trends",
                "Accounts for parameter uncertainty")
)

kable(robust_alternatives, caption = "Robust alternatives to traditional statistical methods")
```

## Importance of Sanity Checks in Automated Analysis

```{r}
# Example function demonstrating sanity checks for dose-response modeling
sanity_check_drc <- function(data, model_fit) {
  checks <- list()
  
  # Check 1: Does the data have sufficient dose levels?
  checks$enough_doses <- length(unique(data$dose)) >= 5
  
  # Check 2: Is there a control group?
  checks$has_control <- 0 %in% unique(data$dose)
  
  # Check 3: Are the parameter estimates biologically plausible?
  params <- coef(model_fit)
  checks$plausible_params <- all(params > 0)  # Simplified example
  
  # Check 4: Does the model converge?
  checks$converged <- !is.null(model_fit$converged) && model_fit$converged
  
  # Check 5: Residual patterns
  checks$residual_pattern <- shapiro.test(residuals(model_fit))$p.value > 0.05
  
  # Visual check (would return a plot in practice)
  checks$visual <- "Examine residual plots and dose-response curve"
  
  return(checks)
}

# Example usage (not run)
# library(drc)
# data <- data.frame(dose = rep(c(0, 1, 10, 100), each = 5),
#                   response = c(rnorm(5, 100, 10), rnorm(5, 90, 10),
#                               rnorm(5, 50, 15), rnorm(5, 10, 5)))
# model <- drm(response ~ dose, data = data, fct = LL.4())
# sanity_check_drc(data, model)
```

## Power is Not Always Better: Misapplication of Statistical Methods

```{r}
# Example: Simulating inappropriate application of equivalence testing
simulate_equivalence_problem <- function(n = 30, true_diff = 0) {
  # Generate data for standard difference test design
  control <- rnorm(n, mean = 100, sd = 15)
  treatment <- rnorm(n, mean = 100 + true_diff, sd = 15)
  
  # Traditional t-test (appropriate)
  t_result <- t.test(treatment, control)
  
  # Inappropriate TOST equivalence test with arbitrary bounds
  # that are too narrow for the study design
  equiv_margin <- 5  # Arbitrary small margin
  tost_lower <- t.test(treatment, control, 
                      alternative = "greater", 
                      mu = -equiv_margin)
  tost_upper <- t.test(treatment, control, 
                      alternative = "less", 
                      mu = equiv_margin)
  
  # TOST is significant if both tests are significant
  equiv_result <- max(tost_lower$p.value, tost_upper$p.value) < 0.05
  
  return(list(
    diff_test_p = t_result$p.value,
    equiv_test_result = equiv_result,
    inappropriate_reason = "Equivalence margins not based on biological relevance or study design"
  ))
}

# Example: Inappropriate application of CPCAT to non-Poisson data
simulate_cpcat_problem <- function(n = 50) {
  # Generate overdispersed count data (negative binomial, not Poisson)
  control <- rnbinom(n, size = 2, mu = 10)  # Overdispersed
  treatment <- rnbinom(n, size = 2, mu = 12)  # Overdispersed
  
  # Check for Poisson assumption
  control_var_mean_ratio <- var(control) / mean(control)
  treatment_var_mean_ratio <- var(treatment) / mean(treatment)
  
  return(list(
    control_var_mean_ratio = control_var_mean_ratio,
    treatment_var_mean_ratio = treatment_var_mean_ratio,
    is_poisson = all(c(control_var_mean_ratio, treatment_var_mean_ratio) < 1.5),
    cpcat_appropriate = all(c(control_var_mean_ratio, treatment_var_mean_ratio) < 1.5),
    recommendation = ifelse(
      all(c(control_var_mean_ratio, treatment_var_mean_ratio) < 1.5),
      "CPCAT may be appropriate",
      "Use negative binomial GLM or other methods for overdispersed counts"
    )
  ))
}

cpcat_example <- simulate_cpcat_problem()
cpcat_example
```


To illustrate the relationship between dose-response patterns, variance decomposition, and the implications for Type I error rates in the context of Generalized Linear Mixed Models (GLMM), Linear Mixed Models (LMM), and Generalized Additive Mixed Models (GAMM), we can use simulated datasets. The examples will highlight how aggregating data can lead to misleading conclusions regarding power and Type I error rates.

Example Setup
We will simulate a dataset where we have a continuous response variable influenced by a dose (continuous predictor), and we will include random effects to account for variability among groups (e.g., different tanks or replicates). We will compare results from individual data to block averages.

Load Necessary Libraries
```{r}
library(tidyverse)
library(lme4) # For LMM and GLMM
library(mgcv) # For GAMM
library(broom.mixed) # For tidy output of mixed models
```
### Simulating Data

```{r}
set.seed(123)

# Simulate individual-level data
n_individuals <- 100
n_tanks <- 10
dose <- runif(n_individuals, 0, 100)
tank <- factor(rep(1:n_tanks, each = n_individuals / n_tanks))
random_effect <- rnorm(n_tanks, 0, 5)
response_individual <- 5 + 0.1 * dose + random_effect[tank] + rnorm(n_individuals)

# Create a dataframe
data_individual <- data.frame(dose, tank, response = response_individual)
```


### Analyzing Individual Data
LMM with Individual Data

```{r}
model_individual <- lmer(response ~ dose + (1 | tank), data = data_individual)
summary(model_individual)
```

### Post Hoc Test
Using the emmeans package for post hoc testing:
```{r}
library(emmeans)
post_hoc_individual <- emmeans(model_individual, ~ dose)
summary(post_hoc_individual)
```

### Aggregating Data by Tank
Now, letâ€™s create a dataset where we average the responses for each tank.

```{r}
data_aggregated <- data_individual %>%
  group_by(tank) %>%
  summarize(dose = mean(dose), response = mean(response), .groups = 'drop')
```

### Analyzing Aggregated Data
LMM with Aggregated Data

```{r}
model_aggregated <- lm(response ~ dose , data = data_aggregated)
summary(model_aggregated)
summary(emmeans(model_aggregated, ~ dose))
```

### Comparing Results
Now, we can compare the results from the individual-level analysis and the aggregated analysis. Key points to note:

- Type I Error Rate: By aggregating data, we may reduce the Type I error rate because the variability within groups is reduced. This can lead to more conservative estimates of significance.

- Power of Post Hoc Tests: The power of post hoc tests can be reduced with aggregation, as the loss of individual-level variability can mask true effects, leading to potential false negatives (Type II errors).

### Example of Reduced Type I Error

In the aggregated data, we might find that the p-values for the dose effect are higher, suggesting that we are less likely to incorrectly reject the null hypothesis (Type I error). Conversely, with individual data, we might observe significant effects due to higher variability, leading to a higher Type I error rate.

### Example of Increased Type I Error
If the individual data has high variability (e.g., due to outliers), the model may incorrectly identify significant effects (higher Type I error). In contrast, the aggregated data may show a more stable estimate, leading to fewer false positives.

### Conclusion
This example demonstrates how variance decomposition in mixed models can affect the interpretation of post hoc tests, emphasizing the importance of considering data structure when analyzing dose-response patterns. Always validate your results with both individual and aggregated data to avoid misleading conclusions.

