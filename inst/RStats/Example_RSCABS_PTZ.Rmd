---
title: "RSCABS"
author: "Zhenglei Gao"
date: '2023-04-11'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(drcHelper)
library(tidyverse)
```

## Background

Allen Olmstead
Ecotoxicologist

For the public consultation period on the EU **prothioconazole endocrine assessment**, one of the studies of focus in the FSTRA (fish short term reproduction assay). In particular there was a question concerning the histopathology results (female oocyte atresia, specifically) and whether they were statistically significant. In our report, there is no formal statistical analysis of the data. I was asked about the availability of approaches for this based on my past postdoctoral experience at the EPA lab that developed many of the endocrine assays. For histopath endpoints, the EPA used a Rao-Scott Cochran-Armitage Trend Test By Slices (RSCABS, described in attached paper). 

Using this RSCABS procedure, I looked at the female oocyte atresia data in R. The attached Excel file has the data and the other attached pdf has my notes on this analysis. However, validation is needed if we are to proceed with this approach in any external purpose. 

## RSCABS

RSCABS: designed to analyze histopathological results from standard toxicology experiments, for example the MEOGRT. 

Steps in the testing procedure:

1.The Cochran-Armitage (CA) trend test was used to test a set of organisms for an increase in the presences (score > 0) or absence (score = 0) of an effect with an increase in the dose concentration of the treatments. 
2. The Rao-Scott (RS) adjustment controls for the similarity in each experiment unit / apparatus (*e.g.*, fish tank) by calculating an adjustment to the CA test statistic from correlation of organisms within each apparatuses. 
3. The by slices (BS) part allows for testing at each severity score (*e.g.*, from 1 to 5) instead of just presences or absence. By slices works by splitting the severity scores associated with an endpoint into **two** groups based on the severity score being tested.  The RSCA test statistic is calculated based on these two groups.
4. Carry out a step-down procedure by excluding the highest treatment level in the analysis and recalculate the RSCA test statistic until the test stats is not significant or there is only control group left. 

## EDA

```{r}
dat_bcs1
bcs1 <- dat_bcs1%>%dplyr::select(c(tmt,tank,S0,S1,S2,S3))%>%pivot_longer(cols=c(S0,S1,S2,S2,S3),names_to = "Score",values_to = "count")
theme_set(theme_bw())
ggplot(bcs1,aes(x=tmt,y=count,color=Score,fill=Score))+geom_bar(stat = "identity",position = "stack")+scale_fill_viridis_d()

bcs1 <- bcs1%>%mutate(trt=tmt)
bcs1$trt[bcs1$trt=="SC" | bcs1$trt=="C"] <- "PC" 
ggplot(bcs1,aes(x=trt,y=count,color=Score,fill=Score))+geom_bar(stat = "identity",position = "stack")+scale_fill_viridis_d()


ggplot(bcs1%>%group_by(tmt)%>%mutate(total=sum(count),freq=count/total),aes(x=tmt,y=freq,color=Score,fill=Score))+geom_bar(stat = "identity",position = "stack")+scale_fill_viridis_d()
ggplot(bcs1%>%group_by(trt)%>%mutate(total=sum(count),freq=count/total),aes(x=trt,y=freq,color=Score,fill=Score))+geom_bar(stat = "identity",position = "stack")+scale_fill_viridis_d()

slice2 <- dat_bcs1%>%mutate(S2_3=total-S0_1)%>% dplyr::select(c(tmt,tank,S0_1,S2_3))%>%pivot_longer(cols=c(S0_1,S2_3),names_to = "Score",values_to = "count")



slice2 <- slice2%>%mutate(trt=tmt)
slice2$trt[slice2$trt=="SC" | slice2$trt=="C"] <- "PC" 

ggplot(slice2,aes(x=tmt,y=count,color=Score,fill=Score))+geom_bar(stat = "identity",position = "stack")+scale_fill_viridis_d()

ggplot(slice2,aes(x=trt,y=count,color=Score,fill=Score))+geom_bar(stat = "identity",position = "stack")+scale_fill_viridis_d()

ggplot(slice2%>%group_by(tmt)%>%mutate(total=sum(count),freq=count/total),aes(x=tmt,y=freq,color=Score,fill=Score))+geom_bar(stat = "identity",position = "stack")+scale_fill_viridis_d()

ggplot(slice2%>%group_by(trt)%>%mutate(total=sum(count),freq=count/total),aes(x=trt,y=freq,color=Score,fill=Score))+geom_bar(stat = "identity",position = "stack")+scale_fill_viridis_d()
```



## R script

```{r}
# fucntion to generate the Rao Scott adjusted values
get_RS_adj_val <- function(group, replicate, affected, total) {
# create data frame from input vectors
dat <- tibble(grp = group, rep = replicate, aff = affected, tot = total)
# create aggregates by dose levels
agg <- group_by(dat, grp) %>%
summarize(x = sum(aff), n = sum(tot), m = n()) %>%
mutate(p_hat = x/n,
b = p_hat*(1 - p_hat)/n)
# add aggregates to original data frame
dat <- left_join(dat, agg, by = "grp") %>%
mutate(r2 = (aff - tot*p_hat)^2) # square of residuals
# calculate subgroup variances
subgrp_var <- group_by(dat, grp, m, n) %>%
summarize(sum_r2 = sum(r2), .groups = "drop") %>%
mutate(v = m*sum_r2/n^2/(m - 1))
agg$v <- subgrp_var$v
# calculate adjusted n and x values
mutate(agg, D = ifelse(v/b < 1, 1, v/b),
n_tilde = n/D,
x_tilde = x/D)
}
# function to run the Cochran-Armitage test with adjusted x and n values.
get_CA_Z <- function(adj_x, adj_n) {
d <- 1:length(adj_x)
N <- sum(adj_n)
d_bar <- sum(d*adj_n)/N
p_bar <- sum(adj_x)/N
num <- sum(adj_x*d) - N*p_bar*d_bar
den <- p_bar*(1 - p_bar)*(sum(adj_n*(d)^2) - N*d_bar^2)
num/sqrt(den)
}
# wrapper function for running RSCA test
run_RSCA <- function(group, replicate, affected, total) {
interm_values <- get_RS_adj_val(group, replicate, affected, total)
Z <- get_CA_Z(interm_values$x_tilde, interm_values$n_tilde)
list(interm_values = interm_values, Z = Z)
}
```


```{r}
data("dat_bcs1")
#summarize data
select(dat_bcs1, tmt, tank, S0, S1, S2, S3) %>%
knitr::kable()
# create control only data set
cdat <- filter(dat_bcs1, tmt %in% c("C", "SC"))
# Slice S1
ctrl_s1_p <- run_RSCA(cdat$tmt, cdat$tank, cdat$S0, cdat$total)$Z %>%
abs() %>%
pnorm() %>%
`-`(1, .) %>%
`*`(2) %>%
round(3)
# slice S2
ctrl_s2_p <- run_RSCA(cdat$tmt, cdat$tank, cdat$S0_1, cdat$total)$Z %>%
abs() %>%
pnorm() %>%
`-`(1, .) %>%
`*`(2) %>%
round(3)


# slice S3
ctrl_s3_p <- run_RSCA(cdat$tmt, cdat$tank, cdat$S0_2, cdat$total)$Z %>%
abs() %>%
pnorm() %>%
`-`(1, .) %>%
`*`(2) %>%
round(3)
# combine control treatments
dat <- mutate(dat_bcs1, tmt = ifelse(tmt == "SC", "C", tmt))
# summary data frame
dat_sum <- group_by(dat, tmt) %>%
summarize(across(S0:total, ~ sum(.)))
# Slice S1
s1_p <- run_RSCA(dat$tmt, dat$tank, dat$total-dat$S0, dat$total)$Z %>%
pnorm() %>%
round(3)
dat1<- dat_bcs1%>%filter(tmt!="SC")
dat2 <-  dat_bcs1%>%filter(tmt!="C")
s1c_p <- run_RSCA(dat1$tmt, dat1$tank, dat1$S0, dat1$total)$Z %>%
pnorm() %>%
round(3)
s1c_p
s1sc_p <- run_RSCA(dat2$tmt, dat2$tank, dat2$S0, dat2$total)$Z %>%
pnorm() %>%
round(3)
s1sc_p

# table for slice 1
mutate(dat_sum, `S1-S3` = total - S0,
Percent = round(`S1-S3`/total*100, 0)) %>%
dplyr::select(tmt, S0, `S1-S3`, Percent) %>%
kable()
# slice S2
s2_p <- run_RSCA(dat$tmt, dat$tank, dat$S0_1, dat$total)$Z %>%
pnorm() %>%
round(3)

run_RSCA(dat1$tmt, dat1$tank, dat1$S0_1, dat1$total)$Z %>% pnorm() 
run_RSCA(dat2$tmt, dat2$tank, dat2$S0_1, dat2$total)$Z %>% pnorm() 

# table for slice 2
mutate(dat_sum, `S0-S1` = S0 + S1, `S2-S3` = S2 + S3,
Percent = round(`S2-S3`/total*100, 0)) %>%
select(tmt, `S0-S1`, `S2-S3`, Percent) %>%
kable()
# slice S3
s3_p <- run_RSCA(dat$tmt, dat$tank, dat$S0_2, dat$total)$Z %>%
pnorm() %>%
round(3)

run_RSCA(dat1$tmt, dat1$tank, dat1$S0_2, dat1$total)$Z %>% pnorm() 
run_RSCA(dat2$tmt, dat2$tank, dat2$S0_2, dat2$total)$Z %>% pnorm() 


# table for slice 3
mutate(dat_sum, `S0-S2` = S0 + S1 + S2,
Percent = round(S3/total*100, 0)) %>%
select(tmt, `S0-S2`, S3, Percent) %>%
kable()

```


```{r}
s1_p
s2_p
s3_p
```

## Check with runRSCABS

- Data: A standard data set in the tall format. Every row indicates an organism. The data set must contain columns for the treatment level and every tested histological endpoint.

- Treatment: The name of the column that contains the information about the treatment level. Increasing values indicate higher treatments.

- Replicate: The name of the column that contains the information about the replicate structure. If the replicate is not specified this will default to running "CA" as the test type.

- Effects: The endpoint to be tested. Defaults to all columns that have integers less then 20. The analysis assumes that higher scores indicate a worse outcome.

- test.type: Indicate the type of analysis to be performed. Use "RS" to select the Rao-Scott adjustment to the Cochran-Armitage test and "CA" to ignore the adjustment.


```{r}
genIndividual <- function(Score,count){
  if(count>0){
    return(data.frame(Score1=rep(gsub("S","",Score),count)))
  }else return(NULL)
}

bcs11 <- bcs1 %>% mutate(individual=map2(Score,count,genIndividual)) %>% unnest(c(individual))
bcs11 <- as.data.frame(bcs11)
bcs12 <- bcs11[,c("trt","tank","Score1")]
bcs12$trt <- factor(bcs12$trt)
bcs12$Score1 <- as.numeric(bcs12$Score1)
bcs12$S2 <- bcs12$Score1+5
runRSCABS(bcs12,'trt','tank',test.type='RS')
```


## Alternative Approach



### 1. A nonparametric approach based on relative effect size

```{r eval=FALSE}
library(nparcomp)
res1<-nparcomp(Score1 ~ trt, data=bcs12, asy.method = "mult.t",
type = "Dunnett", info = FALSE,plot.simci = TRUE)
summary(res1) # Reveal the adjusted p-values
res1<-nparcomp(Score1 ~ trt, data=bcs12, asy.method = "mult.t",
type = "Williams",alternative = "greater",info = FALSE)
summary(res1) # Reveal the adjusted p-values

plot.nparcomp.zg <- function (x, ...) 
{
  nc <- length(x$connames)
  text.Ci <- paste(x$input$conf.level * 100, "%", "Simultaneous Confidence Intervals")
  Lowerp <- "|"
  plot(x$Analysis$Estimator, (1:nc)+1, xlim = c(0, 1), pch = 15, 
    axes = FALSE, xlab = "", ylab = "")
  points(x$Analysis$Lower, (1:nc)+1, pch = Lowerp, font = 2, cex = 2)
  points(x$Analysis$Upper, (1:nc)+1, pch = Lowerp, font = 2, cex = 2)
  abline(v = 0.5, lty = 3, lwd = 2)
  for (ss in (1:nc)+1) {
    polygon(x = c(x$Analysis$Lower[ss], x$Analysis$Upper[ss]), 
      y = c(ss, ss), lwd = 2)
  }
  axis(1, at = seq(0, 1, 0.1))
  axis(2, at = 1:(nc+1), labels = c("tmp",x$connames))
  box()
  title(main = c(text.Ci, paste("Type of Contrast:", x$input$type), 
    paste("Method:", x$AsyMethod)))
}

par(oma=c(5,7,2,2),mar = c(3,3,3,3) )
plot.nparcomp.zg(res1) # Plot simultaneous confidence limits
```

### 2. Simple transformation model
### 3. GLM

### 4. Cumulative link model

```{r eval=F}
library(ordinal)

bcs13 <- bcs12
bcs13$trt <- factor(bcs13$trt)
bcs13$Score1 <- factor(bcs12$Score1)
modCLM <- clm(Score1 ~ trt, data=bcs13)

propCI <-exp(confint(modCLM, level=1-0.05/2)) # Bonferroni odds ratio CI
```


A new permutation test for analyzing histopathologic findings with severity based on their decomposition
into multiple correlated proportions for multiple Dunnett or Williams-type contrasts is presented. The
focus is on problem-adequate interpretation and use for small ni designs with possibly reduced variance
in the control. An asymptotic variant for simple quasilinear models is also available, allowing a wide
class of possible dose-response dependencies to be evaluated. Due to the use of the CRAN packages
coin, tukeytrend and multcomp, the evaluation of real data is relatively easy.
The next future work is simulation for small ni designs with different data conditions up to zero-variance
in the control.



```{r}
library("coin")
library("multcomp")
Co1 <- function(data) trafo(data, factor_trafo = function(x)
model.matrix(~x - 1) %*% t(contrMat(table(x), "Dunnett")))
bcs12 <- bcs12%>% mutate(EP1=ifelse(Score1>0,1,0),EP2=ifelse(Score1>1,1,0),EP3=ifelse(Score1>2,1,0))
Codu <-independence_test(EP1 +EP2+EP3~ trt, data = bcs12, teststat = "maximum",
distribution = "approximate", xtrafo=Co1, alternative="greater")
Codu <-independence_test(EP1 +EP2+EP3~ trt, data = bcs12, teststat = "maximum",
distribution = "approximate", xtrafo=Co1, alternative="less")
pvalCODU <-pvalue(Codu, method="single-step")
pvalCODU

CoW <- function(data) trafo(data, factor_trafo = function(x)
model.matrix(~x - 1) %*% t(contrMat(table(x), "Williams")))
Cowi <-independence_test(EP1 +EP2+EP3~ trt, data = bcs12, teststat = "maximum",
distribution = "approximate", xtrafo=CoW, alternative="greater")
pvalCOWI <-pvalue(Cowi, method="single-step")
pvalCOWI
```


```{r LHex1, eval=F}
data(exampleHistData)
subIndex<-which(exampleHistData$Generation=="F1" &
exampleHistData$Genotypic_Sex=="Male" &
exampleHistData$Age=="8_wk")
LH<-exampleHistData[subIndex, ]
lh<-LH[, c(2,6)]
lh$Gon<-as.numeric(lh$Gon_Phenotype)
lh$EP1<-ifelse(lh$Gon >1,1,0)
lh$EP2<-ifelse(lh$Gon >2,1,0)
lh$EP3<-ifelse(lh$Gon >3,1,0)
lh$treat<-as.factor(lh$Treatment)
lhh<-droplevels(lh[lh$treat!=6, ])
Lhh<-droplevels(lhh[lhh$treat!=3, ])
library("coin")
library("multcomp")
Co1 <- function(data) trafo(data, factor_trafo = function(x)
model.matrix(~x - 1) %*% t(contrMat(table(x), "Dunnett")))
Codu <-independence_test(EP1 +EP2+EP3~ treat, data = Lhh, teststat = "maximum",
distribution = "approximate", xtrafo=Co1, alternative="greater")
pvalCODU <-pvalue(Codu, method="single-step")
pvalCODU
```


```{r LHex2, eval=F}
CoW <- function(data) trafo(data, factor_trafo = function(x)
model.matrix(~x - 1) %*% t(contrMat(table(x), "Williams")))
Cowi <-independence_test(EP1 +EP2+EP3~ treat, data = Lhh, teststat = "maximum",
distribution = "approximate", xtrafo=CoW, alternative="greater")
pvalCOWI <-pvalue(Cowi, method="single-step")
pvalCOWI
```

Monotone increasing trends exist for any category, whereas the strongest for 1, (2,3,5).

```{r LHex3, eval=F}
Adu <-independence_test(Gon+EP1 +EP2+EP3~ treat, data = Lhh, teststat = "maximum",
distribution = "approximate", xtrafo=Co1, alternative="greater")
pvalADU <-pvalue(Adu, method="single-step")
pvalADU
```


```{r LHex4, eval=F}
library("coin")
library("multcomp")
green$treat<-as.factor(green$Dose)
Co1 <- function(data) trafo(data, factor_trafo = function(x)
model.matrix(~x - 1) %*% t(contrMat(table(x), "Dunnett")))
gCodu <-independence_test(S12 + S23 ~ treat, data = green, teststat = "maximum",
distribution = "approximate", xtrafo=Co1, alternative="greater")
pvalGCODU <-pvalue(gCodu, method="single-step")
```

## Validation by SAS

```
[Yesterday 20:11] Chen Meng

Hi Zhenglei, I was able to verify the last two p-values 0.072 and 0.161.
[Yesterday 20:11] Chen Meng
however for the first one, the p-value generated by SAS is 0.484
```

