---
title: "Dunnett's Test Validation Report (Fixed Reference Item Issue)"
author: "Zhenglei Gao"  
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    theme: united
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(testthat)
library(drcHelper)
library(dplyr)
library(ggplot2)
library(knitr)
library(kableExtra)
```

## Introduction

This report validates the Dunnett test with the **critical fix** for excluding "Reference item" test groups, which should not participate in Dunnett tests (Control vs Test item comparisons only).

**Key Fix Applied:**
- **Excluded "Reference item" groups** from all test data before analysis
- This resolves the dose-mean misalignment issues in MOCK08/15-001 study

## Test Environment

```{r environment}
session_info <- sessionInfo()
print(paste("R version:", session_info$R.version$version.string))
print(paste("drcHelper version:", packageVersion("drcHelper")))
print(paste("Platform:", session_info$platform))
```

## Load Test Data

```{r load-data}
# Load test data (using original data, not corrections)
load('../../../data/test_cases_data.rda')
load('../../../data/test_cases_res.rda')

cat("Test cases data rows:", nrow(test_cases_data), "\n")
cat("Expected results rows:", nrow(test_cases_res), "\n")

# Apply Reference item filter
cat("Filtering out Reference item groups...\n")
before_count <- nrow(test_cases_data)
test_cases_data <- test_cases_data[test_cases_data[['Test group']] != 'Reference item', ]
after_count <- nrow(test_cases_data)
cat("Removed", before_count - after_count, "Reference item rows\n")
cat("Remaining data rows:", after_count, "\n")
```

## Helper Functions

```{r helper-functions}
# Convert dose helper function
convert_dose <- function(dose_val) {
  if (is.na(dose_val) || dose_val == "Control") {
    return(0)
  } else {
    return(as.numeric(dose_val))
  }
}

# Find expected values with corrected logic
find_expected_values <- function(data_row, results_df) {
  study_id <- data_row$`Study ID`
  endpoint <- data_row$Endpoint
  measurement_var <- data_row$`Measurement Variable`
  alternative <- data_row$alternative
  
  # Use different matching logic for MOCK0065 vs other studies
  if (study_id == "MOCK0065") {
    # MOCK0065: Use 3-field matching (Study + Endpoint + Measurement Variable)
    expected_rows <- results_df[
      results_df$`Study ID` == study_id &
      results_df$Endpoint == endpoint &
      results_df$`Measurement \r\nvaribale` == measurement_var,
    ]
  } else {
    # Other studies: Use 2-field matching (Study + Endpoint)
    expected_rows <- results_df[
      results_df$`Study ID` == study_id &
      results_df$Endpoint == endpoint,
    ]
  }
  
  return(expected_rows)
}

# Check if endpoint has count data (endpoint-specific, not study-level)
has_count_data <- function(data, endpoint) {
  endpoint_data <- data[data$Endpoint == endpoint, ]
  
  # Check if this specific endpoint has count columns with non-NA values
  has_dead <- !all(is.na(endpoint_data$Dead))
  has_total <- !all(is.na(endpoint_data$Total))
  has_alive <- !all(is.na(endpoint_data$Alive))
  
  return(has_dead && has_total)
}

# Get study subset with proper filtering (excluding Reference item)
get_study_subset <- function(test_cases_data, study_id, test_endpoint, dose_levels = NULL) {
  subset_data <- test_cases_data[
    test_cases_data[['Study ID']] == study_id & 
    test_cases_data$Endpoint == test_endpoint &
    test_cases_data[['Test group']] != 'Reference item',  # CRITICAL FIX
  ]
  
  if (!is.null(dose_levels)) {
    subset_data <- subset_data[subset_data$Dose %in% dose_levels, ]
  }
  
  return(subset_data)
}
```

## Validation Functions  

```{r validation-functions}
# Validate expected values
validate_expected_values <- function(study_id, function_group_id) {
  # Get expected results for this study and function group
  expected_subset <- test_cases_res[
    test_cases_res[['Study ID']] == study_id &
    test_cases_res[['Function group ID']] == function_group_id,
  ]
  
  if (nrow(expected_subset) == 0) {
    return(list(
      passed = FALSE,
      error = paste("No expected results found for study", study_id, "function group", function_group_id),
      n_comparisons = 0,
      n_passed = 0
    ))
  }
  
  # Count valid expected values (not "-" or empty)
  valid_expected <- sum(!expected_subset[['expected result value']] %in% c("-", "", "NA"))
  total_expected <- nrow(expected_subset)
  
  return(list(
    passed = valid_expected > 0,
    note = paste("Found", valid_expected, "valid expected values out of", total_expected, "total"),
    n_comparisons = total_expected,
    n_passed = valid_expected
  ))
}

# Set tolerances
tolerance <- 1e-6  # For T-statistics  
p_value_tolerance <- 1e-4  # For p-values

# Main validation function with Reference item filtering
run_dunnett_validation <- function(study_id, function_group_id, alternative = "less") {
  
  # Determine test endpoint based on function group
  test_endpoint <- if (grepl("FG002(2[0-2]|4[0-2]|5[0-2]|6[0-2]|7[0-2])", function_group_id)) {
    "Mortality"
  } else if (grepl("FG008", function_group_id)) {
    "Reproduction" 
  } else {
    "Repellency"
  }
  
  # Use different matching logic for MOCK0065 vs other studies  
  if (study_id == "MOCK0065") {
    # MOCK0065: Use 3-field matching
    measurement_var <- if (test_endpoint == "Mortality") "% Dead" else "Growth rate"
    expected_rows <- test_cases_res[
      test_cases_res[['Study ID']] == study_id &
      test_cases_res[['Function group ID']] == function_group_id &
      grepl(measurement_var, test_cases_res[['Measurement \r\nvaribale']], fixed = TRUE),
    ]
  } else {
    # Other studies: Use 2-field matching
    expected_rows <- test_cases_res[
      test_cases_res[['Study ID']] == study_id &
      test_cases_res[['Function group ID']] == function_group_id,
    ]
  }
  
  if (nrow(expected_rows) == 0) {
    return(list(passed = FALSE, error = paste("No expected results found for", study_id, function_group_id)))
  }
  
  # Get study data - CRITICAL: Exclude Reference item groups
  study_data <- test_cases_data[
    test_cases_data[['Study ID']] == study_id & 
    test_cases_data$Endpoint == test_endpoint &
    test_cases_data[['Test group']] != 'Reference item',  # KEY FIX
  ]
  
  if (nrow(study_data) == 0) {
    return(list(passed = FALSE, error = paste("No data found for study", study_id, "endpoint", test_endpoint)))
  }
  
  # Show what doses are included after filtering
  available_doses <- sort(unique(study_data$Dose))
  cat("  Available doses after Reference item filtering:", paste(available_doses, collapse = ", "), "\n")
  
  # Run dunnett test
  tryCatch({
    result <- dunnett_test(study_data, alternative = alternative)
    
    if (is.null(result)) {
      return(list(passed = FALSE, error = "dunnett_test returned NULL"))
    }
    
    # Enhanced validation with detailed comparisons
    validation_results <- data.frame(
      metric = character(),
      expected = numeric(),  
      actual = numeric(),
      diff = numeric(),
      passed = logical(),
      stringsAsFactors = FALSE
    )
    
    # Validate T-statistics
    t_expected <- expected_rows[grepl("t-value", expected_rows[['Brief description']], ignore.case = TRUE), ]
    if (nrow(t_expected) > 0 && !is.null(result$`T-statistic`)) {
      for (i in 1:nrow(t_expected)) {
        dose_str <- t_expected$Dose[i]
        expected_val_str <- t_expected[['expected result value']][i]
        
        # Skip if expected value is invalid
        if (is.na(expected_val_str) || expected_val_str %in% c("-", "", "NA")) next
        
        expected_val <- as.numeric(expected_val_str)
        if (is.na(expected_val)) next
        
        # Find corresponding actual value
        dose_num <- convert_dose(dose_str)
        dose_col <- paste0("dose_", dose_num)
        
        if (dose_col %in% names(result$`T-statistic`)) {
          actual_val <- result$`T-statistic`[[dose_col]]
          diff_val <- abs(actual_val - expected_val)
          passed <- diff_val <= tolerance
          
          validation_results <- rbind(validation_results, data.frame(
            metric = paste("T-statistic at dose", dose_str),
            expected = expected_val,
            actual = actual_val,
            diff = diff_val,
            passed = passed,
            stringsAsFactors = FALSE
          ))
        }
      }
    }
    
    # Validate P-values  
    p_expected <- expected_rows[grepl("p-value", expected_rows[['Brief description']], ignore.case = TRUE), ]
    if (nrow(p_expected) > 0 && !is.null(result$`P-value`)) {
      for (i in 1:nrow(p_expected)) {
        dose_str <- p_expected$Dose[i]
        expected_val_str <- p_expected[['expected result value']][i]
        
        # Skip if expected value is invalid
        if (is.na(expected_val_str) || expected_val_str %in% c("-", "", "NA")) next
        
        expected_val <- as.numeric(expected_val_str)
        if (is.na(expected_val)) next
        
        dose_num <- convert_dose(dose_str)
        dose_col <- paste0("dose_", dose_num)
        
        if (dose_col %in% names(result$`P-value`)) {
          actual_val <- result$`P-value`[[dose_col]]
          diff_val <- abs(actual_val - expected_val)
          passed <- diff_val <= p_value_tolerance
          
          validation_results <- rbind(validation_results, data.frame(
            metric = paste("P-value at dose", dose_str),
            expected = expected_val,
            actual = actual_val,
            diff = diff_val,
            passed = passed,
            stringsAsFactors = FALSE
          ))
        }
      }
    }
    
    # Validate Means
    mean_expected <- expected_rows[grepl("Mean|% |Wasps", expected_rows[['Brief description']]), ]
    if (nrow(mean_expected) > 0 && !is.null(result$Mean)) {
      for (i in 1:nrow(mean_expected)) {
        dose_str <- mean_expected$Dose[i]
        expected_val_str <- mean_expected[['expected result value']][i]
        
        # Skip if expected value is invalid
        if (is.na(expected_val_str) || expected_val_str %in% c("-", "", "NA")) next
        
        expected_val <- as.numeric(expected_val_str)
        if (is.na(expected_val)) next
        
        dose_num <- convert_dose(dose_str)
        dose_col <- paste0("dose_", dose_num)
        
        if (dose_col %in% names(result$Mean)) {
          actual_val <- result$Mean[[dose_col]]
          diff_val <- abs(actual_val - expected_val)
          passed <- diff_val <= tolerance
          
          validation_results <- rbind(validation_results, data.frame(
            metric = paste("Mean at dose", dose_str),
            expected = expected_val,
            actual = actual_val,
            diff = diff_val,
            passed = passed,
            stringsAsFactors = FALSE
          ))
        }
      }
    }
    
    # Overall assessment
    if (nrow(validation_results) > 0) {
      overall_passed <- all(validation_results$passed)
      n_passed <- sum(validation_results$passed)
      n_total <- nrow(validation_results)
    } else {
      overall_passed <- FALSE
      n_passed <- 0
      n_total <- 0
    }
    
    return(list(
      passed = overall_passed,
      details = list(
        n_comparisons = n_total,
        n_passed = n_passed,
        validation_results = validation_results
      )
    ))
    
  }, error = function(e) {
    return(list(passed = FALSE, error = paste("Error in dunnett_test:", e$message)))
  })
}
```

## Test Execution

```{r run-tests}
# Define test cases  
test_function_groups <- data.frame(
  study = c("MOCK0065", "MOCK0065", "MOCK0065", "MOCK0065", "MOCK0065",
            "MOCK08/15-001", "MOCK08/15-001", "MOCK08/15-001", "MOCK08/15-001", 
            "MOCK08/15-001", "MOCK08/15-001"),
  function_group_id = c("FG00220", "FG00241", "FG00242", "FG00261", "FG00262",
                       "FG00221", "FG00222", "FG00271", "FG00272", "FG00811", "FG00821"), 
  alternative = c("less", "less", "greater", "less", "greater", 
                 "less", "two.sided", "less", "greater", "greater", "less"),
  stringsAsFactors = FALSE
)

# Run all validations
test_results <- list()

for (i in 1:nrow(test_function_groups)) {
  fg <- test_function_groups[i, ]
  test_name <- paste(fg$study, fg$function_group_id, fg$alternative, sep = "_")
  
  cat("Running validation for:", test_name, "\n")
  
  result <- run_dunnett_validation(
    study_id = fg$study,
    function_group_id = fg$function_group_id, 
    alternative = fg$alternative
  )
  
  test_results[[test_name]] <- list(
    test = test_name,
    function_group = fg$function_group_id,
    study_id = fg$study,
    alternative = fg$alternative,
    passed = result$passed,
    details = result$details,
    error = result$error
  )
}

# Summary
total_tests <- length(test_results)
passed_tests <- sum(sapply(test_results, function(x) x$passed))

cat("\n=== VALIDATION SUMMARY (REFERENCE ITEM FILTERED) ===\n")
cat("Total tests:", total_tests, "\n")
cat("Passed tests:", passed_tests, "\n") 
cat("Failed tests:", total_tests - passed_tests, "\n")
cat("Success rate:", round(100 * passed_tests / total_tests, 1), "%\n")
```

## Detailed Results

```{r results}
# Create summary table
test_summary <- data.frame(
  Test = character(),
  Function_Group = character(),
  Study = character(),
  Alternative = character(),
  Status = character(),
  Details = character(),
  stringsAsFactors = FALSE
)

for (test_name in names(test_results)) {
  result <- test_results[[test_name]]
  status <- ifelse(result$passed, "✅ PASS", "❌ FAIL")
  
  if (!is.null(result$details)) {
    details <- paste0(result$details$n_passed, "/", result$details$n_comparisons, " comparisons passed")
  } else if (!is.null(result$error)) {
    details <- paste("Error:", result$error)
  } else {
    details <- "No details available"
  }
  
  test_summary <- rbind(test_summary, data.frame(
    Test = test_name,
    Function_Group = result$function_group,
    Study = result$study_id,
    Alternative = result$alternative,
    Status = status,
    Details = details,
    stringsAsFactors = FALSE
  ))
}

# Display summary table
kable(test_summary, caption = "Dunnett Test Validation Results (Reference Item Excluded)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  row_spec(which(grepl("❌ FAIL", test_summary$Status)), background = "#FFCCCC") %>%
  row_spec(which(grepl("✅ PASS", test_summary$Status)), background = "#CCFFCC")
```

## Key Fix Applied

**Problem:** "Reference item" test groups were included in the analysis, causing:
- Extra dose levels (e.g., dose 0.1 in MOCK08/15-001 that only contained Reference item)
- Dose-mean misalignment in expected vs actual comparisons
- Statistical test contamination

**Solution:** Filter out all "Reference item" groups before running Dunnett tests:
```r
study_data <- study_data[study_data[['Test group']] != 'Reference item', ]
```

## Detailed Validation Tables

```{r detailed-tables}
cat("\n=== Detailed Expected vs Actual Comparison (Reference Item Excluded) ===\n")

for(test_name in names(test_results)) {
  result <- test_results[[test_name]]
  
  if(!is.null(result$details$validation_results)) {
    validation_data <- result$details$validation_results
    
    if(nrow(validation_data) > 0) {
      cat("\n**", result$test, "**\n")
      if(!is.null(result$function_group) && !is.null(result$study_id) && !is.null(result$alternative)) {
        cat("Function Group:", result$function_group, "| Study:", result$study_id, "| Alternative:", result$alternative, "\n\n")
      }
      
      # Add tolerance and status columns
      validation_data$Tolerance <- ifelse(grepl("P-value", validation_data$metric), p_value_tolerance, tolerance)
      validation_data$Status <- ifelse(validation_data$passed, "PASS", "FAIL")
      
      # Create formatted table
      print(kable(validation_data[, c("metric", "expected", "actual", "diff", "Tolerance", "Status")], 
                  digits = 6,
                  col.names = c("Metric", "Expected", "Actual", "Abs Diff", "Tolerance", "Status")) %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                     font_size = 12) %>%
        row_spec(which(validation_data$Status == "FAIL"), background = "#FFCCCC") %>%
        row_spec(which(validation_data$Status == "PASS"), background = "#CCFFCC"))
      
      cat("\n")
    }
  }
}
```

## Conclusion

The **Reference Item exclusion fix** should significantly improve validation results by:

1. **Eliminating contaminating dose levels** that don't belong in Dunnett tests
2. **Correct dose-mean alignment** by removing Reference item data points
3. **Proper statistical comparisons** between Control and Test item groups only

This fix addresses the root cause of the dose misalignment issues identified earlier.

```{r session-info}
sessionInfo()
```