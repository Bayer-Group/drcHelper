[{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Advanced_Fitting-a-biphasic-dose-reponse-model.html","id":"stackoverflow","dir":"Articles","previous_headings":"","what":"Stackoverflow","title":"Fitting a biphasic dose-reponse model","text":"## Defining","code":"dat <- structure(list(Concn = c(1e-05, 4e-06, 1.5e-06, 7.5e-07, 2.5e-07,  1e-07, 3.75e-08, 1.63e-08, 6.25e-09, 2.5e-09, 1.06e-09, 4.06e-10,  1.56e-10, 6.25e-11, 2.66e-11, 1.09e-11), CompoundX = c(0.309967,  0.239756, 0.924346, 1.409483, 2.128796, 2.407227, 2.300768, 1.826203,  0.978104, 0.483403, 0.235191, 0.115721, 0.06902, 0.031384, 0.023007,  0.003956), CompoundX.2 = c(0.28848, 0.386004, 0.924336, 1.310479,  2.007222, 2.371517, 2.203162, 1.654133, 1.06907, 0.473238, 0.251971,  0.114867, 0.053681, 0.054416, 0.028945, 0.020866)), class = \"data.frame\", row.names = c(NA,  -16L))  m0<-drm(CompoundX~log(Concn), data = dat, fct = gaussian()) summary(m0) #>  #> Model fitted: Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                 Estimate Std. Error   t-value   p-value     #> b:(Intercept)   2.039632   0.082528   24.7144 5.461e-11 *** #> c:(Intercept)   0.027623   0.038466    0.7181    0.4877     #> d:(Intercept)   2.437808   0.064117   38.0214 5.036e-13 *** #> e:(Intercept) -16.270101   0.044150 -368.5165 < 2.2e-16 *** #> f:(Intercept)   2.152055   0.195211   11.0242 2.767e-07 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  0.07611998 (11 degrees of freedom)   plot(m0, type = \"all\", col= \"black\", log = \"\") #> Warning in min(dose[dose > 0]): no non-missing arguments to min; returning Inf data(metaldata) #Make a subset with the Zn data Zn <- metaldata[metaldata$metal==\"Zn\",] Zn #>    metal    conc         IR #> 1     Zn  0.0369  0.9142857 #> 2     Zn  0.0369  0.9756098 #> 3     Zn  0.0369  0.8974359 #> 4     Zn  0.0925  0.9523810 #> 5     Zn  0.0925  0.8780488 #> 6     Zn  0.0925  1.1666667 #> 7     Zn  0.1859  0.9523810 #> 8     Zn  0.1859  1.1707317 #> 9     Zn  0.1859  1.2115385 #> 10    Zn  0.5693  1.7523810 #> 11    Zn  0.5693  1.6585366 #> 12    Zn  0.5693  1.7948718 #> 13    Zn  0.9684  5.9809524 #> 14    Zn  0.9684  4.6341463 #> 15    Zn  0.9684  4.8461538 #> 16    Zn  1.3836 14.2857143 #> 17    Zn  1.3836 19.7073171 #> 18    Zn  1.3836 22.7051282 #> 19    Zn  1.4472 43.9500000 #> 20    Zn  1.4472 44.0869565 #> 21    Zn  1.4472 43.2000000 #> 22    Zn  2.0371 67.1238095 #> 23    Zn  2.0371 74.0975610 #> 24    Zn  2.0371 86.0192308 #> 25    Zn  3.2111 31.1619048 #> 26    Zn  3.2111 61.2195122 #> 27    Zn  3.2111 64.6153846 #> 28    Zn  4.4946  3.8857143 #> 29    Zn  4.4946  4.8292683 #> 30    Zn  4.4946  5.3846154 #> 31    Zn 10.7532  0.3428571 #> 32    Zn 10.7532  0.5853659 #> 33    Zn 10.7532  0.6730769 #Fitting biphasic dose-response profiles #gaussian function Zn.gau <- drm(IR~conc, data=Zn, fct=gaussian(), na.action=na.omit) summary(Zn.gau) #>  #> Model fitted: Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                 Estimate Std. Error  t-value   p-value     #> b:(Intercept)  0.8861307  0.0093682  94.5893 < 2.2e-16 *** #> c:(Intercept)  2.1659895  1.2820509   1.6895    0.1022     #> d:(Intercept) 75.7461755  3.3920022  22.3308 < 2.2e-16 *** #> e:(Intercept)  2.3418279  0.0086316 271.3077 < 2.2e-16 *** #> f:(Intercept) 13.8801613  2.8981117   4.7894 4.933e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  5.87511 (28 degrees of freedom) #Model checking  plot(fitted(Zn.gau), residuals(Zn.gau), ylim = c(-20, 20)) # Gaussian function with Box-Cox transform Zn.gau2 <- drm(IR~conc, data=Zn, fct=gaussian(), na.action=na.omit, bcVal = 0, bcAdd = 10) summary(Zn.gau2) #>  #> Model fitted: Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                Estimate Std. Error t-value   p-value     #> b:(Intercept)  0.793259   0.075520 10.5040 3.215e-11 *** #> c:(Intercept)  1.613775   0.500908  3.2217  0.003223 **  #> d:(Intercept) 81.080825  10.982113  7.3830 4.859e-08 *** #> e:(Intercept)  2.422227   0.042576 56.8923 < 2.2e-16 *** #> f:(Intercept)  3.115795   0.624967  4.9855 2.885e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  0.17677 (28 degrees of freedom) #>  #> Non-normality/heterogeneity adjustment through optimal Box-Cox transformation #>  #> Specified lambda: 0 #Model checking  plot(fitted(Zn.gau2), residuals(Zn.gau2), ylim = c(-0.4, 0.4)) #lgaussian function Zn.lgau <- drm(IR~conc, data=Zn, fct=lgaussian(), na.action=na.omit) summary(Zn.lgau) #>  #> Model fitted: Log-Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                 Estimate Std. Error  t-value   p-value     #> b:(Intercept)  0.4013399  0.0064603  62.1238 < 2.2e-16 *** #> c:(Intercept)  2.1660776  1.2820555   1.6895    0.1022     #> d:(Intercept) 75.7466002  3.3919724  22.3311 < 2.2e-16 *** #> e:(Intercept)  2.1746086  0.0128388 169.3773 < 2.2e-16 *** #> f:(Intercept)  9.1065547  1.8772570   4.8510 4.168e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  5.87511 (28 degrees of freedom) #Model checking  plot(fitted(Zn.lgau), residuals(Zn.lgau), ylim = c(-20, 20)) # lgaussian function with Box-Cox transform Zn.lgau2 <- drm(IR~conc, data=Zn, fct=lgaussian(), na.action=na.omit, bcVal = 0, bcAdd = 10) summary(Zn.lgau2) #>  #> Model fitted: Log-Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                Estimate Std. Error t-value   p-value     #> b:(Intercept)  0.351996   0.048207  7.3018 5.978e-08 *** #> c:(Intercept)  1.475965   0.597634  2.4697  0.019884 *   #> d:(Intercept) 80.099960   9.894407  8.0955 8.170e-09 *** #> e:(Intercept)  2.245102   0.043678 51.4011 < 2.2e-16 *** #> f:(Intercept)  2.662339   0.829132  3.2110  0.003312 **  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  0.1698429 (28 degrees of freedom) #>  #> Non-normality/heterogeneity adjustment through optimal Box-Cox transformation #>  #> Specified lambda: 0 #Model checking  plot(fitted(Zn.lgau2), residuals(Zn.lgau2), ylim = c(-0.4, 0.4)) #plot the models fitted plot(Zn.gau, type = \"obs\", col= \"black\", log = \"\") plot(Zn.gau, type = \"none\", add = TRUE, col = \"red\")  plot(Zn.gau2, type = \"none\", add = TRUE, col = \"yellow\") plot(Zn.lgau, type = \"none\", add = TRUE, col = \"blue\") plot(Zn.lgau2, type = \"none\", add = TRUE, col = \"green\") #Effective doses (EDp) calculation ED(Zn.lgau2, 50, interval = \"delta\") #>  #> Estimated effective doses #>  #>        Estimate Std. Error   Lower   Upper #> e:1:50  3.34241    0.18363 2.96627 3.71855 ED(Zn.lgau2, -50, interval = \"delta\", bound = FALSE) #>  #> Estimated effective doses #>  #>         Estimate Std. Error    Lower    Upper #> e:1:-50 1.508038   0.082849 1.338329 1.677746 ED(Zn.lgau2, 99.9,interval = \"delta\") #>  #> Estimated effective doses #>  #>          Estimate Std. Error   Lower   Upper #> e:1:99.9  2.32300    0.10458 2.10877 2.53723  #Make a subset with the Cd data Cd <- metaldata[metaldata$metal==\"Cd\",] Cd #>    metal    conc         IR #> 34    Cd  0.1004  0.8761905 #> 35    Cd  0.1004  0.8906250 #> 36    Cd  0.1004  0.8194444 #> 37    Cd  0.2517  0.8000000 #> 38    Cd  0.2517  1.1718750 #> 39    Cd  0.2517  1.0138889 #> 40    Cd  0.5055  1.0285714 #> 41    Cd  0.5055  1.0312500 #> 42    Cd  0.5055  1.0416667 #> 43    Cd  1.5398  2.5523810 #> 44    Cd  1.5398  2.8593750 #> 45    Cd  1.5398  2.0694444 #> 46    Cd  2.6010 18.1333333 #> 47    Cd  2.6010 17.5781250 #> 48    Cd  2.6010 30.6111111 #> 49    Cd  3.6856 39.5428571 #> 50    Cd  3.6856 41.8333333 #> 51    Cd  5.3508 55.7714286 #> 52    Cd  5.3508 47.1944444 #> 53    Cd  8.2131 35.9238095 #> 54    Cd  8.2131 40.0312500 #> 55    Cd  8.2131 35.2638889 #> 56    Cd 11.1672 15.3523810 #> 57    Cd 11.1672 25.1250000 #> 58    Cd 11.1672 20.8888889 #> 59    Cd 23.6932  1.6000000 #> 60    Cd 23.6932  2.2968750 #> 61    Cd 23.6932  1.8750000 #Fitting biphasic dose-response profiles #gaussian function Cd.gau <- drm(IR~conc, data=Cd, fct=gaussian(), na.action=na.omit) summary(Cd.gau) #>  #> Model fitted: Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                Estimate Std. Error t-value p-value     #> b:(Intercept)  4.079689   0.115139 35.4326 < 2e-16 *** #> c:(Intercept)  1.283627   1.275138  1.0067 0.32457     #> d:(Intercept) 42.981885   1.907644 22.5314 < 2e-16 *** #> e:(Intercept)  6.850150   0.070015 97.8387 < 2e-16 *** #> f:(Intercept)  7.805005   3.237173  2.4111 0.02429 *   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  4.4849 (23 degrees of freedom) #Model checking  plot(fitted(Cd.gau), residuals(Cd.gau), ylim = c(-10, 10)) #Gaussian function with Box-Cox transform Cd.gau2 <- drm(IR~conc, data=Cd, fct=gaussian(), na.action=na.omit, bcVal = 0, bcAdd = 10) summary(Cd.gau2) #>  #> Model fitted: Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                Estimate Std. Error  t-value   p-value     #> b:(Intercept)  4.056422   0.090069  45.0368 < 2.2e-16 *** #> c:(Intercept)  1.202271   0.352358   3.4121  0.002388 **  #> d:(Intercept) 42.700145   2.340528  18.2438 3.551e-15 *** #> e:(Intercept)  6.851570   0.053432 128.2305 < 2.2e-16 *** #> f:(Intercept)  7.234677   1.057973   6.8382 5.667e-07 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  0.1088791 (23 degrees of freedom) #>  #> Non-normality/heterogeneity adjustment through optimal Box-Cox transformation #>  #> Specified lambda: 0 #Model checking  plot(fitted(Cd.gau2), residuals(Cd.gau2), ylim = c(-0.2, 0.2)) #lgaussian function Cd.lgau <- drm(IR~conc, data=Cd, fct=lgaussian(), na.action=na.omit) summary(Cd.lgau) #>  #> Model fitted: Log-Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                Estimate Std. Error t-value   p-value     #> b:(Intercept)  0.544683   0.032578 16.7195 2.308e-14 *** #> c:(Intercept)  0.806563   0.920847  0.8759    0.3901     #> d:(Intercept) 51.009281   2.143174 23.8008 < 2.2e-16 *** #> e:(Intercept)  5.315768   0.093498 56.8542 < 2.2e-16 *** #> f:(Intercept)  2.105116   0.261655  8.0454 3.892e-08 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  3.086809 (23 degrees of freedom) #Model checking  plot(fitted(Cd.lgau), residuals(Cd.lgau), ylim = c(-6, 6), xlim = c(0, 50)) #lgaussian function with Box-Cox transform Cd.lgau2 <- drm(IR~conc, data=Cd, fct=lgaussian(), na.action=na.omit, bcVal = 0, bcAdd = 10) summary(Cd.lgau2) #>  #> Model fitted: Log-Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                Estimate Std. Error t-value   p-value     #> b:(Intercept)  0.562495   0.039451 14.2581 6.590e-13 *** #> c:(Intercept)  1.039686   0.314707  3.3037  0.003103 **  #> d:(Intercept) 49.728622   3.290085 15.1147 1.950e-13 *** #> e:(Intercept)  5.340496   0.093375 57.1938 < 2.2e-16 *** #> f:(Intercept)  2.358463   0.293418  8.0379 3.955e-08 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  0.09005292 (23 degrees of freedom) #>  #> Non-normality/heterogeneity adjustment through optimal Box-Cox transformation #>  #> Specified lambda: 0 #Model checking  plot(fitted(Cd.lgau2), residuals(Cd.lgau2), ylim = c(-0.3, 0.3)) #plot the models fitted plot(Cd.gau, type = \"obs\", col= \"black\", log = \"\", ylim = c(0, 60)) plot(Cd.gau, type = \"none\", add = TRUE, col = \"red\")  plot(Cd.gau2, type = \"none\", add = TRUE, col = \"yellow\") plot(Cd.lgau, type = \"none\", add = TRUE, col = \"blue\") plot(Cd.lgau2, type = \"none\", add = TRUE, col = \"green\") #Effective doses (D(p)) calculation ED(Cd.lgau2, -50, interval = \"delta\", bound = FALSE) #>  #> Estimated effective doses #>  #>         Estimate Std. Error   Lower   Upper #> e:1:-50  2.79902    0.10714 2.57737 3.02066 ED(Cd.lgau2, 99.9,interval = \"delta\") #>  #> Estimated effective doses #>  #>          Estimate Std. Error   Lower   Upper #> e:1:99.9  5.56039    0.11333 5.32594 5.79483 ED(Cd.lgau2, 50,interval = \"delta\") #>  #> Estimated effective doses #>  #>        Estimate Std. Error    Lower    Upper #> e:1:50 10.18961    0.39005  9.38273 10.99649  # Make a subset with the ZnCd data ZnCd <- metaldata[metaldata$metal==\"ZnCd\",] ZnCd #>     metal      conc        IR #> 326  ZnCd 0.9671179 13.276596 #> 327  ZnCd 0.9671179 11.744681 #> 328  ZnCd 0.9671179 11.404255 #> 329  ZnCd 0.9671179 11.574468 #> 330  ZnCd 1.9969972 42.808511 #> 331  ZnCd 1.9969972 41.744681 #> 332  ZnCd 1.9969972 40.808511 #> 333  ZnCd 1.9969972 40.255319 #> 334  ZnCd 4.1971350 25.914894 #> 335  ZnCd 4.1971350 23.914894 #> 336  ZnCd 4.1971350 23.361702 #> 337  ZnCd 4.1971350 23.489362 #> 338  ZnCd 5.9729887  9.574468 #> 339  ZnCd 5.9729887  9.702128 #> 340  ZnCd 5.9729887  9.914894 #> 341  ZnCd 5.9729887  9.489362 #> 342  ZnCd 9.1994126  1.787234 #> 343  ZnCd 9.1994126  1.361702 #> 344  ZnCd 9.1994126  1.446809 #> 345  ZnCd 9.1994126  1.702128  #gaussian function ZnCd.gau <- drm(IR~conc, data=ZnCd, fct=gaussian(), na.action=na.omit) summary(ZnCd.gau) #> Warning in sqrt(diag(varMat)): NaNs produced #>  #> Model fitted: Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                 Estimate Std. Error  t-value   p-value     #> b:(Intercept) 8.3214e-03        NaN      NaN       NaN     #> c:(Intercept) 2.2642e+00 1.1019e+00   2.0548   0.05774 .   #> d:(Intercept) 1.0820e+03        NaN      NaN       NaN     #> e:(Intercept) 2.8890e+00 2.3294e-02 124.0227 < 2.2e-16 *** #> f:(Intercept) 4.0544e-01 1.6222e-02  24.9934 1.222e-13 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  2.120518 (15 degrees of freedom) #Model checking plot(fitted(ZnCd.gau), residuals(ZnCd.gau)) #> Warning in sqrt(diag(varMat)): NaNs produced #> Warning in sqrt(diag(varMat)): NaNs produced #Gaussian function with Box-Cox transform ZnCd.gau2 <- drm(IR~conc, data=ZnCd, fct=gaussian(), na.action=na.omit, bcVal = 0, bcAdd = 10) summary(ZnCd.gau2) #>  #> Model fitted: Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                 Estimate Std. Error t-value   p-value     #> b:(Intercept)   0.040953   0.033257  1.2314   0.23712     #> c:(Intercept)   0.579361   1.245712  0.4651   0.64855     #> d:(Intercept) 329.177040 137.561825  2.3929   0.03024 *   #> e:(Intercept)   2.899653   0.048979 59.2017 < 2.2e-16 *** #> f:(Intercept)   0.475406   0.075453  6.3007 1.422e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  0.1078937 (15 degrees of freedom) #>  #> Non-normality/heterogeneity adjustment through optimal Box-Cox transformation #>  #> Specified lambda: 0 #Model checking plot(fitted(ZnCd.gau2), residuals(ZnCd.gau2), ylim = c(-0.15, 0.15)) #lgaussian function ZnCd.lgau <- drm(IR~conc, data=ZnCd, fct=lgaussian(), na.action=na.omit) summary(ZnCd.lgau) #>  #> Model fitted: Log-Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                Estimate Std. Error  t-value   p-value     #> b:(Intercept)  0.555663   0.015419  36.0386 4.972e-16 *** #> c:(Intercept) -0.274800   1.032831  -0.2661    0.7938     #> d:(Intercept) 42.904780   0.747360  57.4084 < 2.2e-16 *** #> e:(Intercept)  2.321479   0.018413 126.0780 < 2.2e-16 *** #> f:(Intercept)  2.028462   0.164444  12.3353 2.963e-09 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  0.834502 (15 degrees of freedom) #Model checking plot(fitted(ZnCd.lgau), residuals(ZnCd.lgau), ylim = c(-2, 2)) #lgaussian function with Box-Cox transform ZnCd.lgau2 <- drm(IR~conc, data=ZnCd, fct=lgaussian(), na.action=na.omit, bcVal = 0, bcAdd = 10) summary(ZnCd.lgau2) #>  #> Model fitted: Log-Gaussian (5 parms) #>  #> Parameter estimates: #>  #>                Estimate Std. Error  t-value   p-value     #> b:(Intercept)  0.555188   0.015215  36.4897 4.508e-16 *** #> c:(Intercept) -0.290451   0.583670  -0.4976     0.626     #> d:(Intercept) 42.910857   0.943991  45.4568 < 2.2e-16 *** #> e:(Intercept)  2.321820   0.012425 186.8676 < 2.2e-16 *** #> f:(Intercept)  2.024344   0.134982  14.9971 1.947e-10 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  0.02654068 (15 degrees of freedom) #>  #> Non-normality/heterogeneity adjustment through optimal Box-Cox transformation #>  #> Specified lambda: 0 #Model checking plot(fitted(ZnCd.lgau2), residuals(ZnCd.lgau2), ylim = c(-0.06, 0.06)) #plot the models fitted plot(ZnCd.gau, type = \"obs\", col= \"black\", ylim = c(0, 300)) plot(ZnCd.gau, type = \"none\", add = TRUE, col = \"red\") plot(ZnCd.gau2, type = \"none\", add = TRUE, col = \"yellow\") plot(ZnCd.lgau, type = \"none\", add = TRUE, col = \"blue\") plot(ZnCd.lgau2, type = \"none\", add = TRUE, col = \"green\") #Effective doses (D(p)) calculation ED(ZnCd.lgau2, -50, interval = \"delta\", bound = FALSE) #>  #> Estimated effective doses #>  #>         Estimate Std. Error    Lower    Upper #> e:1:-50 1.209189   0.014632 1.178001 1.240377 ED(ZnCd.lgau2, 99.9,interval = \"delta\") #>  #> Estimated effective doses #>  #>          Estimate Std. Error    Lower    Upper #> e:1:99.9 2.382456   0.015367 2.349703 2.415210 ED(ZnCd.lgau2, 50,interval = \"delta\") #>  #> Estimated effective doses #>  #>        Estimate Std. Error    Lower    Upper #> e:1:50 4.458236   0.053948 4.343248 4.573224 # Additivity predictions and departures from additivity indicesFct(0.355,list(ZnCd.lgau2, Zn.lgau2, Cd.lgau2), c(-0.2, -0.5, -1, -2, -5, -10, -20, -30, -40, -50, -60, -70, -80, -90, -99, 99, 80, 70, 60, 50, 40, 30, 20, 10, 5, 2, 1, 0.5, 0.2)) #plot CIx plotFACI(0.355,list(ZnCd.lgau2, Zn.lgau2, Cd.lgau2), \"x\", ylim = c(-0.2, 2.2), faValues = c(-10, -20, -30, -40, -50, -60, -70, -80, -90, -99, 99, 90, 80, 70, 60, 50, 40, 30, 20, 10), showPoints = TRUE) title(\"Combination index for x axis\")  #plot CIy plotFACI(0.355,list(ZnCd.lgau2, Zn.lgau2, Cd.lgau2), \"y\", ylim = c(-0.2, 2.2), faValues = c(-10, -20, -30, -40, -50, -60, -70, -80, -90, -99, 99, 90, 80, 70, 60, 50, 40, 30, 20, 10), showPoints = TRUE) title(\"Combination index for y axis\")"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Advanced_Fitting-a-biphasic-dose-reponse-model.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Fitting a biphasic dose-reponse model","text":"Martin-Betancor K, Ritz C, Fernández-Piñas F, Leganés F, Rodea-Palomares . Defining additivity framework mixture research inducible whole-cell biosensors. Sci Rep. 2015 Nov 26;5:17200. doi: 10.1038/srep17200. PMID: 26606975; PMCID: PMC4660423. https://stackoverflow.com/questions/72472432/--fit--biphasic-dose-response-curve-using-r","code":""},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Count_Data.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Count Data in General","text":"Quasi-Poisson vs. negative binomial regression: model overdispersed count data? Revisiting analysis pipeline overdispersed Poisson binomial data Check -dispersion Stackoverflow post overdispersion Poisson","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/EFSA-Criteria.html","id":"model-comparison-criteria","dir":"Articles","previous_headings":"","what":"Model Comparison Criteria","title":"EFSA Criteria","text":"Apart generic model comparison criteria, like scaled residuals, visual fit, AIC, lack--fit test goodness--fit test, etc., EFSA proposed use following two criteria: Normalized Width Normalized width can calculated calc Overlapping EC10, 20 50 Certainty protection classified based relationship EC10 EC20/EC50 confidence intervals Steepness curve Steepness defined ratio EC\\(_{10}\\) EC\\(_{50}\\). certainty protection level steepness curves can calculated calcSteepnessOverlap(mod = mod, trend = \"Decrease\").","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/EFSA-Criteria.html","id":"us-epa-bmds-criteria","dir":"Articles","previous_headings":"Additional Criteria in Guidances","what":"US EPA BMDS Criteria","title":"EFSA Criteria","text":"BMD:BMDL ratio used BMDS program part model choice criteria. indicator uncertainty level (much information provided) BMD given experimental design assuming model correct. similar NW criteria used EFSA. BMD:BMDL ratio >20 results model placed “questionable” bin BMD:BMDL ratio >5 result “caution” flag. User-specified modifications decision logic also possible.","code":""},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Equivalence-Testing.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Equivalence Testing","text":"Equivalence testing statistical testing approach used determine whether two treatments interventions produce effects practically within predefined margin difference. method particularly useful fields like pharmaceuticals, demonstrating new drug worse existing one specified margin crucial. Equivalence testing closely related difference testing. latter focuses identifying whether difference exists controlling false positive error, former aims show difference within predefined acceptable range, controlling false negative error difference testing scenario. design interpretation equivalence tests can complex traditional difference tests, requiring careful consideration equivalence margin, called biologically relevant effect size. null alternative hypotheses equivalence tests \\[ H_0: | \\mu_1 - \\mu_0 | > \\Delta \\] \\[ H_1: | \\mu_1 - \\mu_0 | \\leq \\Delta \\] \\(\\mu_1\\) \\(\\mu_0\\) means two treatments, \\(\\Delta\\) equivalence margin. also non-inferiority non-superiority cases, nulls \\(\\mu_1 - \\mu_0 <= - \\Delta\\) \\(\\mu_1 - \\mu_0 > \\Delta\\) Sometimes tests can formulated standardized differences means. Depending frame problem, multiplicative model ratios--control comparisons used. Related approach available \\(\\alpha\\) level equivalence testing probability making Type error, occurs null hypothesis incorrectly rejected. means concluding treatments equivalent . \\(\\beta\\) difference testing probability making Type II error, occurs null hypothesis rejected . means failing detect difference (concluding equivalence) one actually exists. equivalence testing, low alpha level crucial ensure conclusion equivalence reliable. Conversely, difference testing, low beta level important ensure true difference detected. hand, \\(\\alpha\\) level difference testing playing similarly complementary role \\(\\beta\\) equivalence testing.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Equivalence-Testing.html","id":"impacts-of-changing-from-difference-to-equivalence-testing","dir":"Articles","previous_headings":"","what":"Impacts of changing from difference to equivalence testing","title":"Equivalence Testing","text":"Equivalence testing often requires larger sample sizes achieve sufficient power, can resource-intensive. design interpretation equivalence tests can complex traditional difference tests, requiring careful consideration “equivalence margin”.","code":""},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Equivalence-Testing.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Equivalence Testing","text":"Dilba, G., Bretz, F., Guiard, V., Hothorn, L. . (2004). Simultaneous confidence intervals ratios applications comparison several treatments control. Methods Information Medicine 43, 465–469. Djira G, Hasler M, Gerhard D, Segbehoe L, Schaarschmidt F (2025). mratios: Ratios Coefficients General Linear Model. R package version 1.4.4, https://CRAN.R-project.org/package=mratios.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Example_Analysis_Workflow.html","id":"example-routine-analysis-flowcharts","dir":"Articles","previous_headings":"","what":"Example Routine Analysis Flowcharts","title":"Example_Analysis_Workflow","text":"Typical flowcharts derving NOECs ECx shown Figure 1 , Figure 2 Figure 3 . dose response hormesis effects highly erratic, consulted statistician. Figure 1: NOEC Monotonic Figure 2: NOEC non-Monotonic Figure 3: ECx Monotonic vignette, going use dataset oecd201 example.","code":""},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Example_Analysis_Workflow.html","id":"assumptions-check","dir":"Articles","previous_headings":"","what":"Assumptions check","title":"Example_Analysis_Workflow","text":"Pretests checks conducted testing, however, assumptions also checked modelling, using holistic approach. follow routine procedure. allow choice appropriate statistical tests, normality homoscedasticity (variance homogeneity) certain endpoint (e.g., shoot dry weight) data checked.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Example_Analysis_Workflow.html","id":"an-overview-of-the-endpoints","dir":"Articles","previous_headings":"","what":"An overview of the endpoints","title":"Example_Analysis_Workflow","text":"NOER, LOER, ER25 ER50 survival, plant height shoot dry weight expressed g .s./ha summarized plant species final assessment (21 days application) can found following tables.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Example_RSCABS.html","id":"rscabs","dir":"Articles","previous_headings":"","what":"RSCABS","title":"RSCABS","text":"RSCABS (Rao-Scott Cochran-Armitage slice) designed analyze histopathological results standard toxicology experiments, example MEOGRT. Steps testing procedure: 1.Cochran-Armitage (CA) trend test used test set organisms increase presences (score \\(>\\) 0) absence (score = 0) effect increase dose concentration treatments. 2. Rao-Scott (RS) adjustment controls similarity experiment unit / apparatus (e.g., fish tank) calculating adjustment CA test statistic correlation organisms within apparatuses. 3. slices (BS) part allows testing severity score (e.g., 1 5) instead just presences absence. slices works splitting severity scores associated endpoint two groups based severity score tested. RSCA test statistic calculated based two groups. 4. Carry step-procedure excluding highest treatment level analysis recalculate RSCA test statistic test stats significant control group left. Take subset F2-females 16 weeks age, run RSCABS. Note R.score table shows scores occured respective treatment groups.   ## Using R’s prop.test instead","code":"library(drcHelper) library(tidyverse) data(\"exampleHistData\") exampleHistData <- exampleHistData %>% as_tibble %>% mutate(across(where(is.integer),as.numeric)) %>% as.data.frame(.) #Take the subset corresponding to F0-females of 16 weeks of age  subIndex<-which(exampleHistData$Generation=='F2' &                   exampleHistData$Genotypic_Sex=='Female' &                   exampleHistData$Age=='16_wk' ) exampleHistData.Sub<-exampleHistData[subIndex, ] #Run RSCABS exampleResults<-runRSCABS(exampleHistData.Sub,'Treatment',                           'Replicate',test.type='RS') exampleResults %>% knitr::kable(.,digits=3) ggplot(exampleHistData.Sub,aes(x=Treatment,fill=factor(Gon_Asynch_Dev)))+geom_bar()+scale_fill_viridis_d()+labs(title=\"Example Data: Gon_Asynch_Dev\",subtitle = \"subset: F2 generation, 16 week age and female\") cids <-which(apply(exampleHistData.Sub[,-(1:5)],2,max)>0) responses <- names(cids)    ggplot(exampleHistData.Sub[,c(1:5,5+cids)]%>%tidyr::pivot_longer(-(1:5),values_to = \"Response\",names_to = \"Endpoint\"),aes(x=Treatment,fill=factor(Response)))+geom_bar()+scale_fill_viridis_d()+labs(title=\"Example Histopath Data\",subtitle = \"subset: F2 generation, 15 week age and female\")+facet_wrap(~Endpoint) library(scales) dat1 <- exampleHistData.Sub[,c(1:5,5+cids)]%>%tidyr::pivot_longer(-(1:5),values_to = \"Response\",names_to = \"Endpoint\") %>% group_by(Endpoint,Treatment,Response) %>% summarise(counts=n())%>% group_by(Endpoint,Treatment) %>% mutate(total=sum(counts))  ggplot(dat1,aes(x=Treatment,fill=factor(Response)))+geom_bar(aes(y=counts/total),stat = \"identity\")+scale_fill_viridis_d()+labs(title=\"Example Histopath Data\",subtitle = \"subset: F2 generation, 15 week age and female\")+facet_wrap(~Endpoint,drop = T)+ scale_y_continuous(labels = percent)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Example_RSCABS.html","id":"alternative-nonparametric-tests","dir":"Articles","previous_headings":"","what":"Alternative Nonparametric Tests","title":"RSCABS","text":"example taken Hothorn’s paper. independence_test function coin package can used test independence two sets variables measured arbitrary scales. Transformations can done via trafo various test statsitcs can calculated, including Pearson \\(\\chi^2\\) test, generalized Cochran-Mantel-Haenszel test, Spearman correlation test, Fisher-Pitman permutation test, Wilcoxon-Mann-Whitney test, Kruskal-Wallis test family weighted logrank tests censored data. However, Williams’ contrast Williams’ test, just like multcomp package.","code":"ifcoin <- require(coin) data(exampleHistData) subIndex<-which(exampleHistData$Generation==\"F1\" & exampleHistData$Genotypic_Sex==\"Male\" & exampleHistData$Age==\"8_wk\") LH<-exampleHistData[subIndex, ] lh<-LH[, c(2,6)] lh$Gon<-as.numeric(lh$Gon_Phenotype) lh$EP1<-ifelse(lh$Gon >1,1,0) lh$EP2<-ifelse(lh$Gon >2,1,0) lh$EP3<-ifelse(lh$Gon >3,1,0) lh$treat<-as.factor(lh$Treatment) lhh<-droplevels(lh[lh$treat!=6, ]) Lhh<-droplevels(lhh[lhh$treat!=3, ]) library(\"coin\") library(\"multcomp\") Co1 <- function(data) trafo(data, factor_trafo = function(x) model.matrix(~x - 1) %*% t(contrMat(table(x), \"Dunnett\"))) Codu <-independence_test(EP1 +EP2+EP3~ treat, data = Lhh, teststat = \"maximum\", distribution = \"approximate\", xtrafo=Co1, alternative=\"greater\") pvalCODU <-pvalue(Codu, method=\"single-step\") pvalCODU CoW <- function(data) trafo(data, factor_trafo = function(x) model.matrix(~x - 1) %*% t(contrMat(table(x), \"Williams\"))) Cowi <-independence_test(EP1 +EP2+EP3~ treat, data = Lhh, teststat = \"maximum\", distribution = \"approximate\", xtrafo=CoW, alternative=\"greater\") pvalCOWI <-pvalue(Cowi, method=\"single-step\") pvalCOWI"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Example_RSCABS.html","id":"multi-quantile-jt","dir":"Articles","previous_headings":"","what":"Multi-quantile JT","title":"RSCABS","text":": John Green, personal communication.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Example_RSCABS.html","id":"function-notes","dir":"Articles","previous_headings":"","what":"Function Notes","title":"RSCABS","text":"","code":"library(DependenciesGraphs) dep <- funDependencies(\"package:drcHelper\", \"runRSCABS\")  # visualization plot(dep)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Example_RSCABS.html","id":"notes-on-rscabs-functions","dir":"Articles","previous_headings":"Function Notes","what":"Notes on RSCABS functions","title":"RSCABS","text":"Select responses maximum value > 0 smaller 20 (limited ranks response). tested response/endpoints, convert2score tested response/endpoints, prepDataRSCABS, prepare data matrix/table format, treatment column, replicate row tested response/endpoints, stepKRSCABS. results look like : combine results big matrix.","code":"$Gon_Asynch_Dev            Effect Treatment R.Score Statistic     P.Value Signif 1 Gon_Asynch_Dev1         5       1  2.622022 0.004370488     ** 2 Gon_Asynch_Dev1         4       1       NaN 1.000000000      . 3 Gon_Asynch_Dev1         4       1       NaN 1.000000000      . 4 Gon_Asynch_Dev2         5       2  2.622022 0.004370488     ** 5 Gon_Asynch_Dev2         4       2       NaN 1.000000000      . 6 Gon_Asynch_Dev2         4       2       NaN 1.000000000      ."},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Example_RSCABS.html","id":"validation-by-sas","dir":"Articles","previous_headings":"","what":"Validation by SAS","title":"RSCABS","text":"RSCABS code R written validated SAS Chen Meng also RSCABS procedure archived statCharrms package.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Example_RSCABS.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"RSCABS","text":"Green, John W. Springer, Timothy . Saulnier, Amy N. Swintek, Joe, (2014) Statistical analysis histopathological endpoints. Environmental Toxicology Chemistry, 33(5), 1108-1116 Hothorn, T., Hornik, K., van de Wiel, M. . Zeileis, . (2008). Implementing class permutation tests: coin package. Journal Statistical Software 28(8), 1–23. doi: 10.18637/jss.v028.i08","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples using NLS.html","id":"using-ggprism","dir":"Articles","previous_headings":"","what":"using ggprism","title":"Using nls to fit arbitrary dose-response models","text":"","code":"library(ggprism)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples using NLS.html","id":"example-dataset","dir":"Articles","previous_headings":"","what":"Example Dataset","title":"Using nls to fit arbitrary dose-response models","text":"","code":"# construct the data.frame, log10 transform the agonist concentration # convert the data.frame to long format, then remove any rows with NA df <- data.frame(   agonist = c(1e-10, 1e-8, 3e-8, 1e-7, 3e-7, 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4),   ctr1 = c(0, 11, 125, 190, 258, 322, 354, 348, NA, 412, NA),   ctr2 = c(3, 33, 141, 218, 289, 353, 359, 298, NA, 378, NA),   ctr3 = c(2, 25, 160, 196, 345, 328, 369, 372, NA, 399, NA),   trt1 = c(3, NA, 11, 52, 80, 171, 289, 272, 359, 352, 389),   trt2 = c(5, NA, 25, 55, 77, 195, 230, 333, 306, 320, 338),    trt3 = c(4, NA, 28, 61, 44, 246, 243, 310, 297, 365, NA) ) %>%    mutate(log.agonist = log10(agonist)) %>%    pivot_longer(     c(-agonist, -log.agonist),      names_pattern = \"(.{3})([0-9])\",      names_to = c(\"treatment\", \"rep\"),     values_to = \"response\"   ) %>%    filter(!is.na(response))  head(df) #> # A tibble: 6 × 5 #>        agonist log.agonist treatment rep   response #>          <dbl>       <dbl> <chr>     <chr>    <dbl> #> 1 0.0000000001         -10 ctr       1            0 #> 2 0.0000000001         -10 ctr       2            3 #> 3 0.0000000001         -10 ctr       3            2 #> 4 0.0000000001         -10 trt       1            3 #> 5 0.0000000001         -10 trt       2            5 #> 6 0.0000000001         -10 trt       3            4 dose_resp <- y ~ min + ((max - min) / (1 + exp(hill_coefficient * (ec50 - x))))  ggplot(df, aes(x = log.agonist, y = response)) +    geom_smooth(     aes(colour = treatment),     method = \"nls\", formula = dose_resp, se = FALSE,     method.args = list(start = list(min = 1.67, max = 397, ec50 = -7, hill_coefficient = 1))   ) +    scale_colour_manual(labels = c(\"No inhibitor\", \"Inhibitor\"),                       values = c(\"#00167B\", \"#9FA3FE\")) +    ggnewscale::new_scale_colour() +   geom_point(aes(colour = treatment, shape = treatment), size = 3) +    scale_colour_prism(palette = \"winter_bright\",                       labels = c(\"No inhibitor\", \"Inhibitor\")) +    scale_shape_prism(labels = c(\"No inhibitor\", \"Inhibitor\")) +    theme_prism(palette = \"winter_bright\", base_size = 16) +    scale_y_continuous(limits = c(-100, 500),                       breaks = seq(-100, 500, 100),                      guide = \"prism_offset\") +    scale_x_continuous(     limits = c(-10, -3),      breaks = -10:-3,     guide = \"prism_offset_minor\",     minor_breaks = log10(rep(1:9, 7)*(10^rep(-10:-4, each = 9))),     labels = function(lab) {       do.call(         expression,         lapply(paste(lab), function(x) bquote(bold(\"10\"^.(x))))       )     }   ) +    theme(axis.title.y = element_blank(),         legend.title = element_blank(),         legend.position.inside = c(0.05, 0.95),         legend.justification = c(0.05, 0.95)) +    labs(x = \"[Agonist], M\") #> Warning: The S3 guide system was deprecated in ggplot2 3.5.0. #> ℹ It has been replaced by a ggproto system that can be extended. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated."},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples using NLS.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Using nls to fit arbitrary dose-response models","text":"https://csdaw.github.io/ggprism/articles/web-/ex1-dose.html","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples_oecd201.html","id":"evaluation-of-dataset-oecd201","dir":"Articles","previous_headings":"","what":"Evaluation of Dataset oecd201","title":"Examples_oecd201","text":"","code":"data(\"oecd201\")"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples_oecd201.html","id":"data-overview","dir":"Articles","previous_headings":"Evaluation of Dataset oecd201","what":"Data Overview","title":"Examples_oecd201","text":"Yield Summary Time 72h","code":"sum1 <- oecd201 %>% group_by(Time,Treatment) %>% summarise(Yield_mean=mean(Yield),Yield_sd=sd(Yield),GrowthRate_mean=mean(GrowthRate),GrowthRate_sd=sd(GrowthRate)) sum0 <- sum1%>%filter(Treatment==\"Control\")%>%rename(Yield0=Yield_mean,GrowthRate0=GrowthRate_mean)%>%dplyr::select(c(Time,Yield0,GrowthRate0)) # sum0 sumtab <- left_join(sum1%>%filter(Time>0),sum0) %>% mutate(Yield_Inhibition=(Yield0-Yield_mean)/Yield0*100,GrowthRate_Inhibition=(GrowthRate0-GrowthRate_mean)/GrowthRate0*100) %>% dplyr::select(c(Time,Treatment,Yield_mean,Yield_sd,Yield_Inhibition,GrowthRate_mean,GrowthRate_sd,GrowthRate_Inhibition)) sumtab%>%dplyr::select(c(Yield_mean,Yield_sd,Yield_Inhibition))%>%filter(Time==72)%>%knitr::kable(.,digits = 2,caption=\"<center><strong>Yield Summary at Time 72h<strong><center>\",escape = FALSE)%>% kableExtra::kable_styling(bootstrap_options = \"striped\")##%>%kableExtra::kable_classic_2() sumtab%>%dplyr::select(c(GrowthRate_mean,GrowthRate_sd,GrowthRate_Inhibition))%>%filter(Time==72)%>%knitr::kable(.,digits = 2,caption=\"<center><strong>Growth Rate Summary at Time 72h<strong><center>\",escape = FALSE)##%>%kableExtra::kable_classic()"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples_oecd201.html","id":"model-fitting-and-comparison-for-yield","dir":"Articles","previous_headings":"Evaluation of Dataset oecd201","what":"Model Fitting and Comparison For Yield","title":"Examples_oecd201","text":"14 day TSL Yield 14 day TSL Yield, Model Comparison Yield Model Fits   Total Shoot Length Growth Yield 14 day","code":"datTn<- subset(oecd201,Time==72)  mod <- drm(Yield~Concentration,data=datTn,fct=LL.3()) fctList <- list(LL2.3(),W2.3(),W1.3(),EXD.3(),EXD.2(),LN.3(),W2.4(),LL.4(),LL2.4()) plot(mod,type=\"all\") res <- mselect.plus(mod,fctList = fctList ) modList <- res$modList edResTab <- mselect.ED(modList = modList,respLev = c(10,20,50),trend=datTn$Trend_Yield[1]) plot.edList(edResTab) resComp <- drcCompare(modRes = res,trend=\"Decrease\") knitr::kable(edResTab,caption = \"14 day TSL Yield\",digits = 3) knitr::kable(resComp,caption = \"14 day TSL Yield, Model Comparison\",digits = 3) plot.modList(modList,scale=\"logx\") plot.modList(modList[c(1,2,3,4)],scale=\"logx\",npts=40) p <-plot.modList(modList[c(1)],scale=\"logx\",npts=80)+theme(legend.position = \"none\")+ggtitle(\"14 day Total Shoot Length, \\n3-parameter type II Weibull Model Fit\") addECxCI(p=p,object=modList[[1]],EDres=NULL,trend=\"Decrease\",endpoint=\"EC\", respLev=c(10,20,50),                      textAjust.x=0.01,textAjust.y=1,useObsCtr=FALSE,d0=NULL,textsize = 4,lineheight = 1,xmin=0.012)+ylab(\"Total Shoot Length [cm]\") + xlab(\"Concentration [µg a.s./L]\") ggsave(\"TSL_14d_Yield.png\") resED <- t(edResTab[1:3, c(2,4,5,6)]) colnames(resED) <- paste(\"EC\", c(10,20,50)) knitr::kable(resED,caption = \"Total Shoot Length Growth Yield at 14 day\",digits = 3) mod <-modList[[1]] edres <- ED.plus(mod,c(5,10,20,50),trend=\"Decrease\") pander::pander(as.data.frame(edres)) modsum <- summary(mod) pander::pander(coef(modsum))"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples_using_drda.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Examples using drda","text":"","code":"?voropm2  head(voropm2) #>   response dose log_dose weight #> 1   1.1263 1.00     0.00   0.87 #> 2   1.0329 1.00     0.00   1.04 #> 3   1.1139 1.00     0.00   0.94 #> 4   1.1146 1.93     0.66   0.96 #> 5   1.0664 1.93     0.66   0.91 #> 6   1.0163 1.93     0.66   0.93"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples_using_drda.html","id":"default-fitting","dir":"Articles","previous_headings":"","what":"Default Fitting","title":"Examples using drda","text":"","code":"# by default `drda` uses a 4-parameter logistic function for model fitting  # common R API for fitting models fit <- drda(response ~ log_dose, data = voropm2)  # get a general overview of the results summary(fit) #>  #> Call: drda(formula = response ~ log_dose, data = voropm2) #>  #> Pearson Residuals: #>      Min        1Q    Median        3Q       Max   #> -1.88121  -0.72514   0.06395   0.47905   2.17210   #>  #> Parameters: #>                   Estimate Std. Error Lower .95 Upper .95 #> Maximum            1.04147    0.01022   1.02144     1.061 #> Height            -1.05659    0.02059  -1.09694    -1.016 #> Growth rate        3.07405    0.31744   2.45187     3.696 #> Midpoint at        6.53555    0.03593   6.46512     6.606 #> Residual std err.  0.05069    0.00574   0.03944     0.062 #>  #> Residual standard error on 41 degrees of freedom #>  #> Log-likelihood: 72.432 #> AIC: -134.86 #> BIC: -125.83 #>  #> Optimization algorithm converged in 285 iterations  # get parameter estimates by using generic functions... coef(fit) #>     alpha     delta       eta       phi  #>  1.041465 -1.056586  3.074053  6.535548 sigma(fit) #> [1] 0.05069196  # ... or accessing the variables directly fit$coefficients #>     alpha     delta       eta       phi  #>  1.041465 -1.056586  3.074053  6.535548 fit$sigma #> [1] 0.05069196  # compare the estimated model against a flat horizontal line, or the full # 5-parameter logistic model, using AIC, BIC, and the Likelihood Ratio Test # (LRT) # # note that the LRT is testing the null hypothesis of a flat horizontal line # being as a good fit as the chosen model, therefore we expect the test to be # significant # # if the test is not significant, a horizontal line is probably a better model anova(fit) #> Analysis of Deviance Table #>  #> Model 1: a #> Model 2: a + d / (1 + exp(-e * (x - p))) (Fit) #> Model 3: a + d / (1 + n * exp(-e * (x - p)))^(1 / n) (Full) #>  #> Model 3 is the best model according to the Akaike Information Criterion. #>  #>         Resid. Df Resid. Dev Df      AIC     BIC Deviance    LRT  Pr(>Chi)     #> Model 1        44     9.0879  1   59.717   63.33                               #> Model 2        41     0.1054  4 -134.864 -125.83  -8.9825 200.58 < 2.2e-16 *** #> Model 3        40     0.0773  5 -146.774 -135.93  -0.0280  13.91 0.0001917 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples_using_drda.html","id":"other-models","dir":"Articles","previous_headings":"","what":"Other models","title":"Examples using drda","text":"","code":"# use the `mean_function` argument to select a different model fit_l2 <- drda(response ~ log_dose, data = voropm2, mean_function = \"logistic2\") fit_l4 <- drda(response ~ log_dose, data = voropm2, mean_function = \"logistic4\") fit_l5 <- drda(response ~ log_dose, data = voropm2, mean_function = \"logistic5\") fit_gz <- drda(response ~ log_dose, data = voropm2, mean_function = \"gompertz\")  # which model should be chosen? anova(fit_l2, fit_l4, fit_l5, fit_gz) #> Analysis of Deviance Table #>  #> Model 1: a #> Model 2: 1 - 1 / (1 + exp(-e * (x - p))) #> Model 3: a + d * exp(-exp(-e * (x - p))) #> Model 4: a + d / (1 + exp(-e * (x - p))) #> Model 5: a + d / (1 + n * exp(-e * (x - p)))^(1 / n) (Full) #>  #> Model 5 is the best model according to the Akaike Information Criterion. #>  #>         Resid. Df Resid. Dev Df      AIC     BIC Deviance     LRT  Pr(>Chi)     #> Model 1        44     9.0879      59.717   63.33                                #> Model 2        43     0.1493  1 -123.180 -117.76  -8.9386 184.897 < 2.2e-16 *** #> Model 3        41     0.1438  2 -120.873 -111.84  -0.0055   1.692 0.4291117     #> Model 4        41     0.1054  0 -134.864 -125.83  -0.0384  13.991               #> Model 5        40     0.0773  1 -146.774 -135.93  -0.0280  13.910 0.0001917 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # 5-parameter logistic function provides the best fit (AIC and BIC are minimum)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples_using_drda.html","id":"weighted-fit","dir":"Articles","previous_headings":"","what":"Weighted fit","title":"Examples using drda","text":"","code":"# it is possible to give each observation its own weight fit_weighted <- drda(response ~ log_dose, data = voropm2, weights = weight)  # all the commands shown so far are available for a weighted fit as well"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples_using_drda.html","id":"constrained-optimization","dir":"Articles","previous_headings":"","what":"Constrained optimization","title":"Examples using drda","text":"","code":"# it is possible to fix parameter values by setting the `lower_bound` and # `upper_bound` appropriately # # unconstrained parameters have a lower bound of `-Inf` and an upper bound of # `Inf` # # Important: be careful when deciding the constraints, because the optimization #            problem might become very difficult to solve within a reasonable #            number of iterations. # # In this particular example we are: #   - fixing the alpha parameter to 1 #   - fixing the delta parameter to -1 #   - constraining the growth rate to be between 1 and 5 #   - not constraining the `phi` parameter, i.e. the `log(EC50)` lb <- c(1, -1, 1, -Inf) ub <- c(1, -1, 5,  Inf)  fit <- drda(   response ~ log_dose, data = voropm2, lower_bound = lb, upper_bound = ub,   max_iter = 260 )  summary(fit) #>  #> Call: drda(formula = response ~ log_dose, data = voropm2, lower_bound = lb,  #>     upper_bound = ub, max_iter = 260) #>  #> Pearson Residuals: #>      Min        1Q    Median        3Q       Max   #> -1.76281  -0.14933   0.08862   0.81283   2.56716   #>  #> Parameters: #>                   Estimate Std. Error Lower .95 Upper .95 #> Maximum            1.00000         NA        NA        NA #> Height            -1.00000         NA        NA        NA #> Growth rate        3.58631   0.448736   2.70680     4.466 #> Midpoint at        6.57056   0.035336   6.50130     6.640 #> Residual std err.  0.05893   0.006432   0.04632     0.072 #>  #> Residual standard error on 43 degrees of freedom #>  #> Log-likelihood: 64.584 #> AIC: -123.17 #> BIC: -117.75 #>  #> Optimization algorithm DID NOT converge in 260 iterations  # if the algorithm does not converge, we can try to increase the maximum number # of iterations or provide our own starting point fit <- drda(   response ~ log_dose, data = voropm2, lower_bound = lb, upper_bound = ub,   start = c(1, -1, 2.6, 5), max_iter = 10000 )  summary(fit) #>  #> Call: drda(formula = response ~ log_dose, data = voropm2, lower_bound = lb,  #>     upper_bound = ub, start = c(1, -1, 2.6, 5), max_iter = 10000) #>  #> Pearson Residuals: #>      Min        1Q    Median        3Q       Max   #> -1.78762  -0.14935   0.08877   0.81294   2.56740   #>  #> Parameters: #>                   Estimate Std. Error Lower .95 Upper .95 #> Maximum            1.00000         NA        NA        NA #> Height            -1.00000         NA        NA        NA #> Growth rate        3.62475   0.464908   2.71355     4.536 #> Midpoint at        6.56865   0.035267   6.49953     6.638 #> Residual std err.  0.05892   0.006429   0.04632     0.072 #>  #> Residual standard error on 43 degrees of freedom #>  #> Log-likelihood: 64.59 #> AIC: -123.18 #> BIC: -117.76 #>  #> Optimization algorithm converged in 284 iterations"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples_using_drda.html","id":"plotting","dir":"Articles","previous_headings":"","what":"Plotting","title":"Examples using drda","text":"","code":"fit_l5 <- drda(response ~ log_dose, data = voropm2, mean_function = \"logistic5\")  # plot the data used for fitting, the maximum likelihood curve, and # *approximate* confidence intervals for the curve plot(fit_l5) # combine all curves in the same plot fit_l2 <- drda(response ~ log_dose, data = voropm2, mean_function = \"logistic2\") fit_l4 <- drda(response ~ log_dose, data = voropm2, mean_function = \"logistic4\") plot(fit_l2, fit_l4, fit_l5) # modify default plotting options # use `legend_show = FALSE` to remove the legend altogether plot(   fit_l5, base = \"10\", col = \"magenta\", xlab = \"x\", ylab = \"y\", level = 0.9,   midpoint = FALSE, main = \"Example plot\", legend_location = \"topright\",   legend = \"5-parameter logistic function\" )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Examples_using_drda.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Examples using drda","text":"Malyutina , Tang J, Pessia (2023). drda: R package dose-response data analysis using logistic functions. Journal Statistical Software, 106(4), 1-26. doi:10.18637/jss.v106.i04","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Introduction.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting Started","title":"Introduction","text":"Getting Started section, introduce main helper functions serve foundation analyses. , ’ll find typical workflows illustrate navigate functionalities package. ’s important note multiple approaches analyzing dataset, choice method can significantly influence results. dataset contains information degree certainty, different analytical methods converge similar conclusions. Conversely, data lacks sufficient information address specific question, disparate methods may yield distinct potentially conflicting results. Thus, characterizing uncertainty crucial component statistical analysis, package emphasizes aspect throughout functionalities. routine analyses, adopt drc package core analysis tool, given robust capabilities dose-response modeling. However, recognize numerous packages designed similar analyses, drda, DoseFinding, others. Articles section, provide examples reproduce analyses using alternative packages, allowing users explore different methodologies gain insights similar analyses can conducted across various frameworks. Additionally, specific types data, offer supplementary analyses utilizing testing modeling approaches, ensuring well-rounded perspective statistical evaluation.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Introduction.html","id":"regulatory-stats","dir":"Articles","previous_headings":"","what":"Regulatory Stats","title":"Introduction","text":"Regulatory Stats section explains regulatory concepts showcases practical applications package’s functionalities, demonstrating implement analyses real-world scenarios. examples serve guide users replicate analyses adapt datasets.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Introduction.html","id":"articles","dir":"Articles","previous_headings":"","what":"Articles","title":"Introduction","text":"Articles section, delve deeper various topics related dose-response analysis, providing insights, comparisons, discussions methodologies. section aims broaden understanding statistical approaches implications research.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Introduction.html","id":"validation","dir":"Articles","previous_headings":"","what":"Validation","title":"Introduction","text":"Lastly, Validation section outlines validation processes employed within package. , discuss ensure reliability accuracy methods implemented, providing users confidence results generated analyses.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Introduction.html","id":"other-useful-packages","dir":"Articles","previous_headings":"","what":"Other useful packages","title":"Introduction","text":"several packages find useful statistical analysis (eco)toxicological data. ecotoxicology package Implementation EPA’s Ecological Exposure Research Division (EERD) tools (discontinued 1999) Probit Trimmed Spearman-Karber Analysis. Probit tables Finney’s book (code-generated, copied) generating functions included. Control correction: Abbott, Schneider-Orelli, Henderson-Tilton, Sun-Shepard. Toxicity scales: Horsfall-Barratt, Archer, Gauhl-Stover, Fullerton-Olsen, etc. drda - Gompertz log-gompertz function - growth curves dose-response analysis - Probably better optimiation algorithms compared drc. bayesnec -Effect-Concentration estimation NSEC: Introducing -Significant-Effect Concentration","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Limit-Test.html","id":"preliminary-assessment","dir":"Articles","previous_headings":"","what":"Preliminary Assessment","title":"Limit Test","text":"","code":"ctr <- testdata %>% filter(Dose == \"0\") ctr0 <- mean(ctr$Response) sres <- testdata %>% group_by(Dose)%>% dplyr::summarize(Mean=mean(Response),SD=sd(Response)) %>% mutate(`% Inhibition` = - ((Mean-ctr0)/ctr0)*100) sres %>% knitr::kable(.,digits = 3)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Limit-Test.html","id":"outlier-check","dir":"Articles","previous_headings":"","what":"Outlier Check","title":"Limit Test","text":"","code":"dixon.test <- outliers::dixon.test DixonQ[DixonQ$n==6,\"Q_critical\"] #> [1] 0.56 outRes <- testdata %>% group_by(Dose) %>% nest() %>% mutate(normtest=map(data,~dixon.test(.x$Response)),                                        tidies= map(normtest,broom::tidy)) outRes <- outRes %>% dplyr::select(-c(data,normtest)) %>% unnest(c(tidies)) outRes %>% knitr::kable(.,digits = 3) ## by specifying the opposite: a logical indicating whether you want to check not the value with largest difference from the mean, but opposite (lowest, if most suspicious is highest etc.) outRes <- testdata %>% group_by(Dose) %>% nest() %>% mutate(normtest=map(data,~dixon.test(.x$Response,opposite = TRUE)),                                        tidies= map(normtest,broom::tidy)) outRes <- outRes %>% dplyr::select(-c(data,normtest)) %>% unnest(c(tidies)) outRes %>% knitr::kable(.,digits = 3)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Limit-Test.html","id":"normality-check","dir":"Articles","previous_headings":"","what":"Normality Check","title":"Limit Test","text":"Welch’s t-test requires normality group. Normality residuals Normality group Variance Homogeneity","code":"mod <- lm(Response~factor(Dose),data=testdata) normalres <- shapiro.test(residuals(mod)) normRes <- testdata %>% group_by(Dose) %>% nest() %>% mutate(normtest=map(data,~shapiro.test(.x$Response)),                                        tidies= map(normtest,broom::tidy)) normRes <- normRes %>% dplyr::select(-c(data,normtest)) %>% unnest(c(tidies)) normRes %>% knitr::kable(.,digits = 3) (varTest <- car::leveneTest(Response~factor(Dose),data=testdata,center=mean)) #> Levene's Test for Homogeneity of Variance (center = mean) #>       Df F value Pr(>F) #> group  1  1.4879 0.2505 #>       10 ## lawstat::levene.test(testdata$Response,testdata$Dose,location=\"mean\") car::leveneTest(Response~factor(Dose),data=testdata,center=median) #> Levene's Test for Homogeneity of Variance (center = median) #>       Df F value Pr(>F) #> group  1  1.5126 0.2469 #>       10"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Limit-Test.html","id":"students-t-test","dir":"Articles","previous_headings":"","what":"Student’s t-test","title":"Limit Test","text":"two-sided one-sided greater Note using formula, greater means test group greater control one-sided smaller Note using formula, greater means test group greater control","code":"(res <- t.test(testdata$Response~testdata$Dose,var.equal = TRUE)) #>  #>  Two Sample t-test #>  #> data:  testdata$Response by testdata$Dose #> t = 2.0993, df = 10, p-value = 0.06215 #> alternative hypothesis: true difference in means between group 0 and group 100 is not equal to 0 #> 95 percent confidence interval: #>  -0.0006161316  0.0207027221 #> sample estimates: #>   mean in group 0 mean in group 100  #>         0.1364410         0.1263977 tres <- broom::tidy(res) tres1 <- c(tres$estimate1,tres$estimate2, tres$parameter,sres$`% Inhibition`[2], -tres$statistic,tres$p.value) names(tres1) <- c(\"Mean Control\", \"Mean Test Item\", \"df\", \"%Inhibition\", \"T-statistic\", \"p-value\") tres1 #>   Mean Control Mean Test Item             df    %Inhibition    T-statistic  #>     0.13644102     0.12639773    10.00000000     7.36090584    -2.09934893  #>        p-value  #>     0.06214521 (res <- t.test(testdata$Response~testdata$Dose,var.equal = TRUE,alternative=\"greater\")) #>  #>  Two Sample t-test #>  #> data:  testdata$Response by testdata$Dose #> t = 2.0993, df = 10, p-value = 0.03107 #> alternative hypothesis: true difference in means between group 0 and group 100 is greater than 0 #> 95 percent confidence interval: #>  0.001372473         Inf #> sample estimates: #>   mean in group 0 mean in group 100  #>         0.1364410         0.1263977 tres <- broom::tidy(res) tres1 <- c(tres$estimate1,tres$estimate2, tres$parameter,sres$`% Inhibition`[2], -tres$statistic,tres$p.value) names(tres1) <- c(\"Mean Control\", \"Mean Test Item\", \"df\", \"%Inhibition\", \"T-statistic\", \"p-value\") tres1 #>   Mean Control Mean Test Item             df    %Inhibition    T-statistic  #>     0.13644102     0.12639773    10.00000000     7.36090584    -2.09934893  #>        p-value  #>     0.03107261 (res <- t.test(testdata$Response~testdata$Dose,var.equal = TRUE,alternative=\"less\")) #>  #>  Two Sample t-test #>  #> data:  testdata$Response by testdata$Dose #> t = 2.0993, df = 10, p-value = 0.9689 #> alternative hypothesis: true difference in means between group 0 and group 100 is less than 0 #> 95 percent confidence interval: #>        -Inf 0.01871412 #> sample estimates: #>   mean in group 0 mean in group 100  #>         0.1364410         0.1263977 tres <- broom::tidy(res) tres1 <- c(tres$estimate1,tres$estimate2, tres$parameter,sres$`% Inhibition`[2], -tres$statistic,tres$p.value) names(tres1) <- c(\"Mean Control\", \"Mean Test Item\", \"df\", \"%Inhibition\", \"T-statistic\", \"p-value\") tres1 #>   Mean Control Mean Test Item             df    %Inhibition    T-statistic  #>      0.1364410      0.1263977     10.0000000      7.3609058     -2.0993489  #>        p-value  #>      0.9689274"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Limit-Test.html","id":"welchs-t-test","dir":"Articles","previous_headings":"","what":"Welch’s t-test","title":"Limit Test","text":"Welch Two Sample t-test one-sided greagter one-sided smaller","code":"(res <- t.test(testdata$Response~testdata$Dose)) #>  #>  Welch Two Sample t-test #>  #> data:  testdata$Response by testdata$Dose #> t = 2.0993, df = 7.5933, p-value = 0.07085 #> alternative hypothesis: true difference in means between group 0 and group 100 is not equal to 0 #> 95 percent confidence interval: #>  -0.001092339  0.021178930 #> sample estimates: #>   mean in group 0 mean in group 100  #>         0.1364410         0.1263977 tres <- broom::tidy(res) tres1 <- c(tres$estimate1,tres$estimate2, tres$parameter,sres$`% Inhibition`[2], -tres$statistic,tres$p.value) names(tres1) <- c(\"Mean Control\", \"Mean Test Item\", \"df\", \"%Inhibition\", \"T-statistic\", \"p-value\") tres1 #>   Mean Control Mean Test Item             df    %Inhibition    T-statistic  #>     0.13644102     0.12639773     7.59330452     7.36090584    -2.09934893  #>        p-value  #>     0.07085487 (res <- t.test(testdata$Response~testdata$Dose,alternative =\"less\")) #>  #>  Welch Two Sample t-test #>  #> data:  testdata$Response by testdata$Dose #> t = 2.0993, df = 7.5933, p-value = 0.9646 #> alternative hypothesis: true difference in means between group 0 and group 100 is less than 0 #> 95 percent confidence interval: #>        -Inf 0.01900155 #> sample estimates: #>   mean in group 0 mean in group 100  #>         0.1364410         0.1263977 tres <- broom::tidy(res) tres1 <- c(tres$estimate1,tres$estimate2, tres$parameter,sres$`% Inhibition`[2], -tres$statistic,tres$p.value) names(tres1) <- c(\"Mean Control\", \"Mean Test Item\", \"df\", \"%Inhibition\", \"T-statistic\", \"p-value\") tres1 #>   Mean Control Mean Test Item             df    %Inhibition    T-statistic  #>      0.1364410      0.1263977      7.5933045      7.3609058     -2.0993489  #>        p-value  #>      0.9645726 (res <- t.test(testdata$Response~testdata$Dose,alternative =\"greater\")) #>  #>  Welch Two Sample t-test #>  #> data:  testdata$Response by testdata$Dose #> t = 2.0993, df = 7.5933, p-value = 0.03543 #> alternative hypothesis: true difference in means between group 0 and group 100 is greater than 0 #> 95 percent confidence interval: #>  0.001085044         Inf #> sample estimates: #>   mean in group 0 mean in group 100  #>         0.1364410         0.1263977 tres <- broom::tidy(res) tres1 <- c(tres$estimate1,tres$estimate2, tres$parameter,sres$`% Inhibition`[2], -tres$statistic,tres$p.value) names(tres1) <- c(\"Mean Control\", \"Mean Test Item\", \"df\", \"%Inhibition\", \"T-statistic\", \"p-value\") tres1 #>   Mean Control Mean Test Item             df    %Inhibition    T-statistic  #>     0.13644102     0.12639773     7.59330452     7.36090584    -2.09934893  #>        p-value  #>     0.03542743"},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Limit-Test.html","id":"kruskalwallis-test","dir":"Articles","previous_headings":"Nonparametric test","what":"Kruskal–Wallis test","title":"Limit Test","text":"","code":"(kwres <- kruskal.test(Response~Dose,data=testdata)) #>  #>  Kruskal-Wallis rank sum test #>  #> data:  Response by Dose #> Kruskal-Wallis chi-squared = 4.3333, df = 1, p-value = 0.03737"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Limit-Test.html","id":"dunns-test-many-to-one","dir":"Articles","previous_headings":"Nonparametric test","what":"Dunn’s test (many-to-one)","title":"Limit Test","text":"essence, Dunn’s test can viewed series Wilcoxon rank sum tests applied pairs groups, adjustments made multiple comparisons. link Dunn’s test Wilcoxon rank sum test lies non-parametric nature rank-based approach. Wilcoxon rank sum test used comparing two groups, Dunn’s test extends concept rank comparisons multiple groups following Kruskal-Wallis test.","code":"## (dunnres <- PMCMRplus::kwManyOneDunnTest(Response~factor(Dose),data=testdata))"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Limit-Test.html","id":"wilcoxon-test","dir":"Articles","previous_headings":"Nonparametric test","what":"Wilcoxon test","title":"Limit Test","text":"Wilcoxon rank-sum test (also called Mann–Whitney U-test) data deviates significantly normal.","code":"(Ures <- wilcox.test(Response~factor(Dose),data=testdata)) #>  #>  Wilcoxon rank sum exact test #>  #> data:  Response by factor(Dose) #> W = 31, p-value = 0.04113 #> alternative hypothesis: true location shift is not equal to 0 ## same results would be obtained by specifying x and y instead of a formula ## wilcox.test(testdata$Response[testdata$Dose==\"0\"],testdata$Response[testdata$Dose==\"100\"],data=testdata) (Ures <- wilcox.test(Response~factor(Dose),data=testdata,alternative=\"greater\")) #>  #>  Wilcoxon rank sum exact test #>  #> data:  Response by factor(Dose) #> W = 31, p-value = 0.02056 #> alternative hypothesis: true location shift is greater than 0 (Ures <- wilcox.test(Response~factor(Dose),data=testdata,alternative=\"less\")) #>  #>  Wilcoxon rank sum exact test #>  #> data:  Response by factor(Dose) #> W = 31, p-value = 0.987 #> alternative hypothesis: true location shift is less than 0"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/MQJT.html","id":"the-mqjt","dir":"Articles","previous_headings":"","what":"The MQJT","title":"MQJT","text":"step-Jonckheere-Terpstra test applied replicate medians conventionally used statistical analysis amphibian developmental stage. questions raised whether analysis focused median developmental stage rep sufficiently sensitive detect statistically effects developmental stage biologically important. Thus John Tim proposed modification standard step-Jonckheere-Terpstra test procedure examines multiple quantiles (.e. shifts Q20, Q30, Q40, Q50, . . . , Q80) simultaneously. assumption behind MQJT shift entire distribution expected “true” effect population. effect small portion sensitive population considered biologically relevant majority replicate affected. hand, even replicate median similar across test concentrations / doses, quantiles different, showing effect test item, significant effect identified test. Assuming common distribution test concentrations (standard assumption almost statistical analyses), difference nth quantile, Qn, median, Q50, constant across concentrations. Thus, shifts Q20, Q30, Q40, Q50, . . ., Q80, measure effect test concentrations segment frequency distribution stages. Excerpt John’s “Supplementary Approach Statistical Analysis Developmental Stage Amphibians” proposed method proceeds follows: 1. Determine 20th percentile stage distribution within rep test concentration. Perform Jonkheere-Terpstra trend test 20th percentiles across concentrations. Record P-value trend test. Repeat steps 1 2 30th 80th percentile. Determine median P-values obtained steps 1-3. median P-value probability trend distribution shifts across concentrations due chance. median p-value 0.05 greater, testing stops NOEC exceeds highest tested concentration. Otherwise, proceed step (5). Drop data highest test concentration, repeat steps 1-4, new median P-value calculated reduced data set. median p-value 0.05 greater, testing stops NOEC highest tested concentration used analysis. Otherwise, proceed step (6). Drop data highest concentration used current analysis, repeat steps 1-4, new median P-value calculated reduced data set. Continue dropping treatment groups new median P-value longer statistically significant. highest concentration occurs noobserved- effect-concentration (NOEC).","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"noec-in-general","dir":"Articles","previous_headings":"","what":"NOEC in General","title":"NOEC Calculation","text":"NOEC (Observed Effect Concentration) critical value ecotoxicology, representing highest concentration substance produce statistically significant effect test organisms compared control group. concept various names, including NOER(Observed Effect Rate), NOEDD(Observed Effect Daily Dose), NOAEL(Observed Adverse Effect Level), . important metric determining safe exposure levels chemicals assessing potential risks human health environment. relatively straightforward calulate intepret NOEC, widely used accepted regulatory world. However, also criticized limitations: focuses single concentration without statistically significant adverse effects tested study, potentially overlooking information complete dose-response study. observed responses NOEC vary studies, making harder compare studies ECx values. NOEC approach take test concentrations continuous variable, therefore allow estimation/prediction response test concentrations. heavily impacted sample size test concentration selections. Poor experimental design may yield high NOEC due decreased statistical sensitivity, desired regulatory context.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"methods-for-deriving-noec","dir":"Articles","previous_headings":"NOEC in General","what":"Methods for deriving NOEC","title":"NOEC Calculation","text":"Dunnett’s Test: used compare multiple treatment groups control group controlling Type error, Step-Williams’ test: used identify significant trend. Non-parametric tests: like Dunn’s test Kruskal-Wallis test step-Jonckheere-Terpstra trend test. Williams’ test Dunnett’s test commonly used parametric methods used NOEC calculations. Williams’ test assumes data monotonic, meaning response variable consistently increases decreases across levels independent variable. assumption crucial validity test results. contrast, Dunnett’s test require data monotonic. flexibility allows applied wider range situations. data meet monotonicity assumption, Williams’ test tends bit greater statistical power compared Dunnett’s test. means Williams’ test likely detect true effect one exists, leading fewer Type II errors (failing reject false null hypothesis). However, situations data monotonic, Dunnett’s test appropriate. may slightly less power data monotonic, robustness handling non-monotonic data makes valuable tool statistical analysis.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"dealing-with-inhomogenous-variance","dir":"Articles","previous_headings":"NOEC in General","what":"Dealing with inhomogenous variance","title":"NOEC Calculation","text":"several ways deal inhomogeneous variances. Welch’s ANOVA (adaptation ANOVA dose assume equal variance) followed Dunnett’s test inhomogeneous variances. Robust statistical techniques sandwich standard error estimations. Bootstrapping can used estimate confidence intervals NOEC without relying normality assumptions. Applying data transformations can stablize variances meet assumptions parametric tests. However, increases complexity results interpretation avoided possible. use mock Myriophyllum study illustrate NOECs derived different approaches. test full dose response study test organism Myriophyllum. 7 test concentrations. interested endpoint Growth Rate Total shoot length 14 d mock study data. Visualize data   Basic Summary Data Check normality residuals  Check homogeneity variance \\(\\alpha=0.01\\), homogeneity variance assumption rejected. \\(\\alpha=0.05\\), enough evidence reject homogeneity variance assumption. Check Monotonicity data shows significant linear trend can considered monotonic.","code":"data(\"test_cases_data\") testdata <- test_cases_data%>% filter(`Test organism` == \"Myriophyllum\" & Design ==\"NOEC/ECx\") unique(testdata$`Study ID`) #> [1] \"MOCK0065\" testdata$Dose <- as.numeric(gsub(\",\",\".\",testdata$Dose)) metainfo <- testdata[1,] design <- ifelse(metainfo$Design==\"Limit\",\"limit test\", \"full dose response study\") nconc <- length(unique(testdata$Dose)) # endpoints <- unique(testdata$Endpoint) # obsVar <- unique(testdata$`Measurement Variable`) endpoints <- unique(testdata[,c(\"Endpoint\",\"Measurement Variable\",\"Time\")]) str_endpoints <- apply(endpoints,1,function(x) paste(x[1], \"of\",x[2], \"at\",x[3])) concunit <- metainfo$Unit conctype <- metainfo$`Concentration type` #log1p <- function(x) log(x+1) ilog1p <- function(x) {   exp(x) - 1 } theme_set(theme_bw()) theme_update(plot.title =  element_text(hjust = 0.5)) ggplot(testdata,aes(x=as.character(Dose),y=Response))+geom_point() + ylab(\"Growth Rate\") + xlab(paste0(\"Test Concentration [\",conctype,\", \",concunit,\"]\"))+ggtitle(\"Total Shoot Length\") doses <- unique(testdata$Dose) ratios <- sapply(2:(length(doses) - 1), function(i) doses[i + 1] / doses[i]) ## note that the factor between the doses is around 2.95.  ggplot(testdata,aes(x=Dose,y=Response))+geom_point() + ylab(\"Growth Rate\") + xlab(paste0(\"Test Concentration [\",conctype,\", \",concunit,\"]\"))+ggtitle(\"Total Shoot Length\")+scale_x_continuous(breaks=doses,trans = scales::trans_new(\"log1p\",log1p,ilog1p))+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) ctr <- testdata %>% filter(Dose == 0) ctr0 <- mean(ctr$Response) sres <- testdata %>% group_by(Dose)%>% dplyr::summarize(Mean=mean(Response),SD=sd(Response)) %>% mutate(`% Inhibition` = - ((Mean-ctr0)/ctr0)*100, CV=SD/Mean*100) sres %>% knitr::kable(.,digits = 3) testdata <- testdata %>% mutate(Treatment=factor(Dose)) mod0 <- lm(Response~Treatment,data=testdata) shapiro.test(residuals(mod0)) ## not completely normal  #>  #>  Shapiro-Wilk normality test #>  #> data:  residuals(mod0) #> W = 0.98098, p-value = 0.851 opar <- par(no.readonly = TRUE)  qqnorm(residuals(mod0)) qqline(residuals(mod0), col = 2) #par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))  #plot(mod0,ask=F) car::leveneTest(Response~Treatment,data=testdata,center=mean) #> Levene's Test for Homogeneity of Variance (center = mean) #>       Df F value  Pr(>F)   #> group  6  3.6043 0.01148 * #>       23                   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 car::leveneTest(Response~Treatment,data=testdata,center=median) #> Levene's Test for Homogeneity of Variance (center = median) #>       Df F value  Pr(>F)   #> group  6  2.7571 0.03616 * #>       23                   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 monotonicityTest(testdata,Treatment=\"Treatment\",Response=\"Response\") #>        Test t value Pr(>|t|) Significance #> 1    Linear  -12.55  <0.0001          *** #> 2 Quadratic    1.10   0.2814            ."},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"williams-trend-test","dir":"Articles","previous_headings":"","what":"Williams’ trend test","title":"NOEC Calculation","text":"Note PMCMRplus::williamsTest produces accurate results. functions gives almost perfect agreement cases. Inside drcHelper package, getwilliamRes function get accept reject vector results.","code":"(will <- PMCMRplus::williamsTest(Response~Treatment,testdata,alternative =\"less\")) #>  #>   Williams trend test  #>  #> data:  Response by Treatment  #> alternative hypothesis:  less  #>  #> H0 #>                t'-value df t'-crit decision alpha #> mu1 - ctr >= 0    0.672 23   1.714   accept  0.05 #> mu2 - ctr >= 0    6.635 23   1.784   reject  0.05 #> mu3 - ctr >= 0   13.624 23   1.808   reject  0.05 #> mu4 - ctr >= 0   20.082 23   1.817   reject  0.05 #> mu5 - ctr >= 0   24.468 23   1.825   reject  0.05 #> mu6 - ctr >= 0   24.468 23   1.827   reject  0.05 #> --- drcHelper::williamsTest_JG(testdata,trt =\"Treatment\",res =\"Response\") #>    Treatment Y.Tilde     Y0  Se.Diff DF    Will TCrit Signif #> Q6        10 0.02885 0.1264 0.003987 23 24.4700 1.827      * #> Q5      3.39 0.02885 0.1264 0.003987 23 24.4700 1.825      * #> Q4      1.15 0.04630 0.1264 0.003987 23 20.0900 1.817      * #> Q3      0.39 0.07210 0.1264 0.003987 23 13.6200 1.808      * #> Q2     0.132 0.09990 0.1264 0.003987 23  6.6470 1.785      * #>       0.0448 0.12370 0.1264 0.003987 23  0.6772 1.714      . getwilliamRes(will) #> [1] \"accept\" \"reject\" \"reject\" \"reject\" \"reject\" \"reject\""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"dunnetts-tests","dir":"Articles","previous_headings":"","what":"Dunnett’s Tests","title":"NOEC Calculation","text":"Dunnett’s test multiple comparison test determining significant differences mean values several test groups control normally distributed errors homogeneous variance. Dunnett’s test robust slight violations normality variance homogeneity assumptions. several packages providing function calculation Dunnett’s test. function DunnettTest library DescTools calculating correct results “two.sided” direction. function dunnettTest library PMCMRplus can used calculate test options “less” (smaller), “greater” “two.sided”. results “two.sided” option differ cases results DunnettTest library DescTools, due multivariate normal calculations. Another commonly used tool glht function Dunnet type contrast multcomp package. also possibilities use packages. example, emmeans package estimating marginal means conducting pairwise comparisons. Another approach use emmeans package. Note p value adjustment method different called “dunnettx”. However t statistic, df, SE estimations . NOEC lowest tested concentration rdoses[2]`.","code":"library(multcomp) (dun <- summary(glht(aov(Response~Treatment, data=testdata), linfct=mcp(Treatment=\"Dunnett\"),alternative=\"less\"))) #>  #>   Simultaneous Tests for General Linear Hypotheses #>  #> Multiple Comparisons of Means: Dunnett Contrasts #>  #>  #> Fit: aov(formula = Response ~ Treatment, data = testdata) #>  #> Linear Hypotheses: #>                  Estimate Std. Error t value Pr(<t)     #> 0.0448 - 0 >= 0 -0.002679   0.003987  -0.672  0.648     #> 0.132 - 0 >= 0  -0.026454   0.003987  -6.635 <1e-04 *** #> 0.39 - 0 >= 0   -0.054314   0.003987 -13.624 <1e-04 *** #> 1.15 - 0 >= 0   -0.080064   0.003987 -20.082 <1e-04 *** #> 3.39 - 0 >= 0   -0.098517   0.003987 -24.711 <1e-04 *** #> 10 - 0 >= 0     -0.096580   0.003987 -24.225 <1e-04 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> (Adjusted p values reported -- single-step method)  (dun <- summary(glht(aov(Response~Treatment, data=testdata), linfct=mcp(Treatment=\"Dunnett\"),alternative=\"two.sided\"))) #>  #>   Simultaneous Tests for General Linear Hypotheses #>  #> Multiple Comparisons of Means: Dunnett Contrasts #>  #>  #> Fit: aov(formula = Response ~ Treatment, data = testdata) #>  #> Linear Hypotheses: #>                  Estimate Std. Error t value Pr(>|t|)     #> 0.0448 - 0 == 0 -0.002679   0.003987  -0.672     0.97     #> 0.132 - 0 == 0  -0.026454   0.003987  -6.635   <1e-04 *** #> 0.39 - 0 == 0   -0.054314   0.003987 -13.624   <1e-04 *** #> 1.15 - 0 == 0   -0.080064   0.003987 -20.082   <1e-04 *** #> 3.39 - 0 == 0   -0.098517   0.003987 -24.711   <1e-04 *** #> 10 - 0 == 0     -0.096580   0.003987 -24.225   <1e-04 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> (Adjusted p values reported -- single-step method) library(emmeans) m <- aov(Response~Treatment, data=testdata) emmeans(m, specs = trt.vs.ctrl ~ Treatment) #> $emmeans #>  Treatment emmean      SE df lower.CL upper.CL #>  0         0.1264 0.00252 23   0.1212   0.1316 #>  0.0448    0.1237 0.00309 23   0.1173   0.1301 #>  0.132     0.0999 0.00309 23   0.0936   0.1063 #>  0.39      0.0721 0.00309 23   0.0657   0.0785 #>  1.15      0.0463 0.00309 23   0.0399   0.0527 #>  3.39      0.0279 0.00309 23   0.0215   0.0343 #>  10        0.0298 0.00309 23   0.0234   0.0362 #>  #> Confidence level used: 0.95  #>  #> $contrasts #>  contrast                     estimate      SE df t.ratio p.value #>  Treatment0.0448 - Treatment0 -0.00268 0.00399 23  -0.672  0.9198 #>  Treatment0.132 - Treatment0  -0.02645 0.00399 23  -6.635  <.0001 #>  Treatment0.39 - Treatment0   -0.05431 0.00399 23 -13.624  <.0001 #>  Treatment1.15 - Treatment0   -0.08006 0.00399 23 -20.082  <.0001 #>  Treatment3.39 - Treatment0   -0.09852 0.00399 23 -24.711  <.0001 #>  Treatment10 - Treatment0     -0.09658 0.00399 23 -24.225  <.0001 #>  #> P value adjustment: dunnettx method for 6 tests"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"dunnetts-tests-with-inhomogenious-variance","dir":"Articles","previous_headings":"","what":"Dunnett’s Tests with Inhomogenious Variance","title":"NOEC Calculation","text":"","code":"inhomo <- require(nlme) library(nlme) gls0 <- gls(Response ~ Treatment, data=testdata,weights=varIdent(form= ~1|Treatment)) plot(gls0,Treatment ~ fitted(.)) ## Looking at the variances plot(gls0,Treatment ~ resid(.),abline=0) treat.means <- emmeans(gls0, ~ Treatment) ## no adjustment for p-values ## contrast(treat.means, adjust=\"none\", method=\"dunnett\", ref = 1) contrast(treat.means, method=\"dunnett\", ref = 1) #>  contrast                     estimate      SE   df t.ratio p.value #>  Treatment0.0448 - Treatment0 -0.00268 0.00345 6.66  -0.776  0.8819 #>  Treatment0.132 - Treatment0  -0.02645 0.00413 5.43  -6.402  0.0043 #>  Treatment0.39 - Treatment0   -0.05431 0.00623 3.89  -8.715  0.0042 #>  Treatment1.15 - Treatment0   -0.08006 0.00297 7.96 -26.947  <.0001 #>  Treatment3.39 - Treatment0   -0.09852 0.00279 8.01 -35.321  <.0001 #>  Treatment10 - Treatment0     -0.09658 0.00248 6.99 -38.867  <.0001 #>  #> Degrees-of-freedom method: satterthwaite  #> P value adjustment: dunnettx method for 6 tests summary(glht(gls0,mcp(Treatment=\"Dunnett\")))  #>  #>   Simultaneous Tests for General Linear Hypotheses #>  #> Multiple Comparisons of Means: Dunnett Contrasts #>  #>  #> Fit: gls(model = Response ~ Treatment, data = testdata, weights = varIdent(form = ~1 |  #>     Treatment)) #>  #> Linear Hypotheses: #>                  Estimate Std. Error z value Pr(>|z|)     #> 0.0448 - 0 == 0 -0.002679   0.003453  -0.776    0.935     #> 0.132 - 0 == 0  -0.026454   0.004132  -6.402   <1e-05 *** #> 0.39 - 0 == 0   -0.054314   0.006232  -8.715   <1e-05 *** #> 1.15 - 0 == 0   -0.080064   0.002971 -26.947   <1e-05 *** #> 3.39 - 0 == 0   -0.098517   0.002789 -35.321   <1e-05 *** #> 10 - 0 == 0     -0.096580   0.002485 -38.867   <1e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> (Adjusted p values reported -- single-step method) fit <- aov(Response ~ Treatment, data=testdata) summary(PMCMRplus::welchManyOneTTest(fit, alternative = \"two.sided\", p.adjust=\"holm\")) #>                 t value   Pr(>|t|)     #> 0.0448 - 0 == 0  -0.776  0.4640697     #> 0.132 - 0 == 0   -6.402  0.0030079  ** #> 0.39 - 0 == 0    -8.715  0.0030079  ** #> 1.15 - 0 == 0   -26.947 1.9119e-08 *** #> 3.39 - 0 == 0   -35.321 2.7730e-09 *** #> 10 - 0 == 0     -38.867 1.0129e-08 *** summary(glht(gls0,mcp(Treatment=\"Dunnett\")),test = adjusted(\"holm\"))  #>  #>   Simultaneous Tests for General Linear Hypotheses #>  #> Multiple Comparisons of Means: Dunnett Contrasts #>  #>  #> Fit: gls(model = Response ~ Treatment, data = testdata, weights = varIdent(form = ~1 |  #>     Treatment)) #>  #> Linear Hypotheses: #>                  Estimate Std. Error z value Pr(>|z|)     #> 0.0448 - 0 == 0 -0.002679   0.003453  -0.776    0.438     #> 0.132 - 0 == 0  -0.026454   0.004132  -6.402 3.07e-10 *** #> 0.39 - 0 == 0   -0.054314   0.006232  -8.715  < 2e-16 *** #> 1.15 - 0 == 0   -0.080064   0.002971 -26.947  < 2e-16 *** #> 3.39 - 0 == 0   -0.098517   0.002789 -35.321  < 2e-16 *** #> 10 - 0 == 0     -0.096580   0.002485 -38.867  < 2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> (Adjusted p values reported -- holm method)"},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"jonckheere-terpstra-test","dir":"Articles","previous_headings":"Nonparametric Tests","what":"Jonckheere-Terpstra test","title":"NOEC Calculation","text":"","code":"jonckheere <- PMCMRplus::stepDownTrendTest(Response~Treatment, data=testdata,test = \"jonckheereTest\",                          alternative = \"less\") summary(jonckheere) #>  #>   Step down Jonckheere-Terpstra test  #> data:  Response by Treatment  #> alternative hypothesis:  less  #> P value adjustment method:  none  #> H0 #>                           z value     Pr(<z)     #> 0.0448 >= 0                -1.066  0.1432110     #> 0.132 >= 0.0448 >= 0       -2.946  0.0016081  ** #> 0.39 >= 0.132 >= ... >= 0  -4.181 1.4491e-05 *** #> 1.15 >= 0.39 >= ... >= 0   -5.150 1.3033e-07 *** #> 3.39 >= 1.15 >= ... >= 0   -5.968 1.2019e-09 *** #> 10 >= 3.39 >= ... >= 0     -6.290 1.5913e-10 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"dunns-test","dir":"Articles","previous_headings":"Nonparametric Tests","what":"Dunn’s test","title":"NOEC Calculation","text":"Dunn’s test powerful general.","code":"m <- lm(Response~Treatment, data=testdata) dunn <- PMCMRplus::kwManyOneDunnTest(Response~Treatment, data=testdata,alternative = \"less\") summary(dunn) #>                 z value     Pr(<z)     #> 0.0448 - 0 >= 0  -0.367 0.77628979     #> 0.132 - 0 >= 0   -1.378 0.30712086     #> 0.39 - 0 >= 0    -2.082 0.08745722   . #> 1.15 - 0 >= 0    -2.786 0.01442079   * #> 3.39 - 0 >= 0    -3.974 0.00022799 *** #> 10 - 0 >= 0      -3.710 0.00059525 ***"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"alternative-approaches","dir":"Articles","previous_headings":"","what":"Alternative Approaches","title":"NOEC Calculation","text":"approaches routinely used regulatory NOEC calculations. However, useful certain situations.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"tamhane-dunnetts-test","dir":"Articles","previous_headings":"Alternative Approaches","what":"Tamhane-Dunnett’s Test","title":"NOEC Calculation","text":"many--one comparisons one-factorial layout normally distributed residuals unequal variances Tamhane-Dunnett’s test can used.","code":"set.seed(245) mn <- c(1, 2, 2^2, 2^3, 2^4) x <- rep(mn, each=5) + rnorm(25) g <- factor(rep(1:5, each=5))  fit <- aov(x ~ g - 1) shapiro.test(residuals(fit)) #>  #>  Shapiro-Wilk normality test #>  #> data:  residuals(fit) #> W = 0.96609, p-value = 0.5484 bartlett.test(x ~ g - 1) #>  #>  Bartlett test of homogeneity of variances #>  #> data:  x by g #> Bartlett's K-squared = 1.0766, df = 4, p-value = 0.898 anova(fit) #> Analysis of Variance Table #>  #> Response: x #>           Df  Sum Sq Mean Sq F value    Pr(>F)     #> g          5 1761.11  352.22  171.01 1.069e-15 *** #> Residuals 20   41.19    2.06                       #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## works with object of class aov library(PMCMRplus) summary(tamhaneDunnettTest(fit, alternative = \"greater\")) #>            t value     Pr(>t)     #> 2 - 1 <= 0   1.399 0.32792374     #> 3 - 1 <= 0   3.368 0.01849964   * #> 4 - 1 <= 0   7.223 0.00021346 *** #> 5 - 1 <= 0  15.186 3.5096e-09 *** ## summary(welchManyOneTTest(fit, alternative = \"greater\", p.adjust=\"single-step\")) ## p.adjust “holm”, “hochberg”, “hommel”, “bonferroni”, “BH”, “BY”, “fdr”, “none” summary(welchManyOneTTest(fit, alternative = \"greater\", p.adjust=\"holm\")) #>            t value     Pr(>t)     #> 2 - 1 <= 0   1.399 0.10262533     #> 3 - 1 <= 0   3.368 0.01089476   * #> 4 - 1 <= 0   7.223 0.00013621 *** #> 5 - 1 <= 0  15.186 7.0276e-07 *** DescTools::DunnettTest(x~g, alternative = \"greater\") ## alternative is not taking effect in this case #>  #>   Dunnett's test for comparing several treatments with a control :   #>     95% family-wise confidence level #>  #> $`1` #>          diff     lwr.ci    upr.ci    pval     #> 2-1  1.201147 -1.2064128  3.608708  0.4992     #> 3-1  3.041632  0.6340717  5.449192  0.0110 *   #> 4-1  7.245476  4.8379162  9.653037 5.1e-08 *** #> 5-1 15.611204 13.2036439 18.018764 < 2e-16 *** #>  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 PMCMRplus::dunnettTest(x~g) #>   1       #> 2 0.511   #> 3 0.012   #> 4 1.2e-07 #> 5 < 2e-16 PMCMRplus::dunnettTest(x~g, alternative = \"greater\") #>   1       #> 2 0.2554  #> 3 0.0055  #> 4 1.9e-08 #> 5 < 2e-16"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/NOEC_Methods.html","id":"references","dir":"Articles","previous_headings":"Alternative Approaches","what":"References","title":"NOEC Calculation","text":"DUNNETT C. W. (1955): multiple comparison procedure comparing several treatments control, Journal American Statistical Association, 50, pp. 1096-1121.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Normality-Check.html","id":"standard-practices","dir":"Articles","previous_headings":"","what":"Standard Practices","title":"Normality Check","text":"context statistical analysis, conducting normality assessment preliminary step outlined decision flowchart established OECD 54 various testing guidelines. Shapiro-Wilk test robust option purpose, particularly dealing relatively small sample sizes. important note several methodologies available evaluating normality. One approach involves assessing whether data within treatment group test concentration group adheres normal distribution. Alternatively, one can examine normality residuals following ANOVA linear regression analysis. first approach assumes data within treatment group normally distributed, without imposing constraints homogeneity variance. Conversely, second approach posits residuals drawn single normal distribution, thereby providing different perspective data’s adherence normality. Check normality Significance level \\(\\alpha\\) = 0.05. p-value conditional probability obtaining test results extreme observed one given null hypothesis H0 (normal distribution) true. p smaller pre-determined significance level \\(\\alpha\\), null hypothesis rejected. Result Shapiro-Wilk’s test: Residuals ANOVA presented QQ-plot.","code":"## It is possible to use options(digits = 7) to control the number ## of digits in print outs. However, it is not necessary to do so. vgl_0 <- c(0.131517109035102, 0.117455425985384, 0.130835155683102,            0.12226548296818, 0.127485057136569, 0.128828137633933) vgl_1 <- c(0.122888192029009, 0.126866725094641, 0.128467082586674,            0.116653888503673) vgl_2 <- c(0.0906079518188219, 0.102060998252763, 0.107240263636048,            0.0998663441976353) vgl_3 <- c(0.0584507373938537, 0.066439126181113, 0.0806046608441279,            0.0828404794158172) vgl_4 <- c(0.0462632004630849, 0.0461876546375037, 0.0512317665813575,            0.0416533060961155) vgl_5 <- c(0.0267638178172436, 0.0314508456741666, 0.0237960318948956,            0.0295133681572911) vgl_6 <- c(0.0273565894468647, 0.0324226779638651, 0.0289617934362975,             0.0305317153447814)  x0 <- vgl_0 - mean(vgl_0) x1 <- vgl_1 - mean(vgl_1) x2 <- vgl_2 - mean(vgl_2) x3 <- vgl_3 - mean(vgl_3) x4 <- vgl_4 - mean(vgl_4) x5 <- vgl_5 - mean(vgl_5) x6 <- vgl_6 - mean(vgl_6)  res <- shapiro.test(c(x0, x1, x2, x3, x4, x5, x6)) res #>  #>  Shapiro-Wilk normality test #>  #> data:  c(x0, x1, x2, x3, x4, x5, x6) #> W = 0.98098, p-value = 0.851 #> Warning in tidy.numeric(res): 'tidy.numeric' is deprecated. #> See help(\"Deprecated\") #> Warning in tidy.numeric(res): 'tidy.numeric' is deprecated. #> See help(\"Deprecated\")"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Normality-Check.html","id":"limitations","dir":"Articles","previous_headings":"","what":"Limitations","title":"Normality Check","text":"example, mixture normals (almost-normal terms Q-Q plots) detected easily small sample size (smaller 1000) (taken post1).","code":"x <- replicate(   100,   {     c(       shapiro.test(rnorm(10) + c(1, 0, 2, 0, 1))$p.value, # $       shapiro.test(rnorm(100) + c(1, 0, 2, 0, 1))$p.value, # $       shapiro.test(rnorm(1000) + c(1, 0, 2, 0, 1))$p.value, # $       shapiro.test(rnorm(5000) + c(1, 0, 2, 0, 1))$p.value     )   } ) rownames(x) <- c(\"n10\", \"n100\", \"n1000\", \"n5000\") apply(x, 1, function(y) {   sum(y < 0.05) }) #>   n10  n100 n1000 n5000  #>     8     7    15    85"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Normality-Check.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Normality Check","text":"Micceri, T. (1989). unicorn, normal curve, improbable creatures. Psychological Bulletin, 105(1), 156-166.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"background","dir":"Articles","previous_headings":"","what":"Background","title":"Example Ordinal Data Analysis","text":"routine procedures regulatory frameworks ordinal data analysis yet. histopathological data, recommended use RSCABS (Rao-Scott Adjusted Cochran-Armitage Trend Test Slices) derive NOEALs. alternative MQJT. Plant visual injury data evaluated qualitatively possible analyze quantitatively","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"ordinal-data-in-general","dir":"Articles","previous_headings":"","what":"Ordinal Data in General","title":"Example Ordinal Data Analysis","text":"statistical point view, ordinal data type categorical data order matters, exact differences distances categories defined. example, survey responses like “satisfied,” “neutral,” “dissatisfied”, ranking like 1st, 2nd, 3rd, levels education like high school, bachelor, master. Market research, psychology, health studies, social sciences often use ordinal data. Since ordinal data doesn’t assume equal intervals, non-parametric methods often used. Medians rather means often basis comparisons. Chi-Square test can used determine significant association two categorical variables. Spearman’s Rank Correlation Kendall’s tau correlation measures can assess strength direction association two ranked variables. Ordinal Regression somewhat complex method can predict outcomes based ordinal data. helps understand different factors influence rankings transition ordered categories. Proportional Odds Model: Coefficient (slopes) remains constant across categories. Continuation Ratio Model: Model cumulative odds ratios. Adjacent Categories Model: useful neighboring categories influence . Due limited information potentially unequal intervals, requires clear understanding subtleties complexities, careful attentions implementing regression modelling. ecotoxicology area, histopathology assessment visual injury data ranked categorical data.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"regression-approaches","dir":"Articles","previous_headings":"","what":"Regression Approaches","title":"Example Ordinal Data Analysis","text":"Ordinal regression can conducted using MASS::polr function function provided GLMcat, R package encompasses lots models specified similar way: (ratio, cdf, design: parallel complete). transforming ordinal variable percentages proportions, also possible model continuous data using logitic regression quasibinomial assumptions. two approaches produce similar results data behaving truly like dose-response -dispersed binomial distributions. data artificial full dose-response relationship observed true ER50=12.18.","code":"library(drcHelper) dattab_new <- pvi_example ## use the example PVI data in the drcHelper package ## Note that yy2 is transformed by increment of 90/5 = 16.  dattab_new$y0 <- factor(dattab_new$y0,ordered = T) ftable(xtabs(~ y0 + dose, data = dattab_new)) ##%>% gt::gt() ## as.data.frame(.) %>% knitr::kable(.,digits = 3) #>    dose  2  4  8 16 32 64 #> y0                        #> 0        8  3  1  0  0  0 #> A        2  5  2  0  1  0 #> B        0  2  4  2  0  0 #> C        0  0  2  1  1  0 #> D        0  0  1  5  2  0 #> E        0  0  0  2  5  0 #> F        0  0  0  0  1 10 dattab_new %>% group_by(y0) %>% summarise(n=n(),meany=mean(yt),meanyy2=mean(yy2)) %>% knitr::kable(.,digits = 3) dattab_new %>% group_by(dose) %>% summarise(n=n(),meany=mean(yt),meanyy=mean(yy)) %>% knitr::kable(.,digits = 3) ggplot(dattab_new,aes(x = dose, y=yt))+geom_point() +  geom_smooth(method=\"glm\", method.args=list(family=\"quasibinomial\"), formula=\"y ~ log(x)\",                        se =TRUE, linewidth=1.5) library(drc) mod_drc  <- drm(yy2 ~ dose, data = dattab_new, fct = LL.2()) summary(mod_drc) #>  #> Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0 and upper limit at 1 (2 parms) #>  #> Parameter estimates: #>  #>               Estimate Std. Error t-value   p-value     #> b:(Intercept) -1.33865    0.13595 -9.8465 5.487e-14 *** #> e:(Intercept) 12.74391    1.03919 12.2633 < 2.2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: #>  #>  0.1446638 (58 degrees of freedom) ED(mod_drc, c(50), interval = \"delta\") #>  #> Estimated effective doses #>  #>        Estimate Std. Error   Lower   Upper #> e:1:50  12.7439     1.0392 10.6637 14.8241 suppressWarnings({ plot(mod_drc, type = \"all\",main=\"log-logistic + normal, ER50= 12.74\", broken = TRUE) plot(mod_drc, broken = TRUE, type=\"confidence\", add=TRUE) })"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"putting-quasi-binomial-and-normal-fit-together","dir":"Articles","previous_headings":"Regression Approaches","what":"Putting quasi-binomial and normal fit together","title":"Example Ordinal Data Analysis","text":"","code":"library(drc) predictCIglm <- function(mod,dat,newdata=NULL){  if(is.null(newdata)){     preddata <- with(dat, data.frame(x = seq(min(x), max(x), length = 100)))  }else predata <- newdata   preds <- predict(mod, newdata = preddata, type = \"link\", se.fit = TRUE)   critval <- 1.96 ## approx 95% CI qnorm(0.975)   upr <- preds$fit + (critval * preds$se.fit)   lwr <- preds$fit - (critval * preds$se.fit)   fit <- preds$fit   fit2 <- mod$family$linkinv(fit)   upr2 <- mod$family$linkinv(upr)   lwr2 <- mod$family$linkinv(lwr)   preddata$Prediction <- fit2    preddata$Lower <- lwr2    preddata$Upper <- upr2    return(preddata) }   preddrc <- function(mod,dat,newdata=NULL){  if(is.null(newdata)){     preddata <- with(dat, data.frame(x = seq(min(x), max(x), length = 100)))  }else predata <- newdata  preds <- predict(mod,newdata=preddata,interval=\"confidence\")  preddata <- cbind(preddata,preds)  return(preddata) } modelall <- function(dat,addbeta = FALSE){   mod1 <- glm(y~log(x),data=dat,family = \"quasibinomial\")   mod2 <- drm(y~x,fct=LL.2(),data=dat)     ## mod3 <- MASS::glmmPQL(y ~ log(x),random=~1|x,family=\"quasibinomial\",data=dat)    ## Estimation the same as quasibinomial without random effects, uncertainty estimation wider.    ## mod3 <- lme4::glmer(y ~ log(x)+ (1|x),family=\"binomial\",data=dat) ## dose not work without sample size.   # mod4 <- betareg::betareg(y ~ log(x),link=\"logit\",data=dat)   # y3<- predict(mod3,newdata=data.frame(x=dat$x),type=\"response\")   y1 <- predictCIglm(mod1,dat=dat)   y1$model <- \"quasibinomial\"    y2 <- preddrc(mod2,dat = dat)   y2$model <- \"drc LL.2\"   preddata <- rbind(y1,y2)      ec1 <- getEC50(mod1)   ec2 <- getEC50(mod2)   ec50 <- data.frame(rbind(ec1,ec2),model=c(\"quasibinomial\",\"drc LL.2\"))      if(addbeta) {     mod3 <- gam(y ~ log(x), family=betar(link=\"logit\"), data=dat)     y3 <- predictCIglm(mod3,dat=dat)     y3$model <- \"beta\"     preddata <- rbind(preddata,y3)     ec3 <- getEC50(mod3)     ec50 <- rbind(ec50,data.frame(ec3,model=c(\"beta\")))   }   #ec4 <- uniroot(function(x) predict(mod4, newdata = data.frame(x=x)) - 0.5,lower = 1, upper = 80)$root   ## 19.55968 beta regression does not perform better than binomial or quasi-binomial regression!      return(list(ec50=ec50,predata=preddata)) } modelallbeta <- function(dat){   modelall(dat,addbeta=TRUE) } dattab_new <- dattab_new %>% mutate(y=yy,x=dose)%>%as.data.frame(.) suppressWarnings({modres <- modelall(dattab_new)}) #>  #> Estimated effective doses #>  #>        Estimate Std. Error   Lower   Upper #> e:1:50  12.7097     1.0856 10.5367 14.8826 predres <- modres[[2]] knitr::kable(modres[[1]]%>%mutate(TrueEC50 = 12.18),digits=3) p_drc_qb <- ggplot(dattab_new, aes(x=dose, y=yt)) +geom_point(alpha=0.1)+ geom_point(aes(x=dose, y=yy),col=\"green\")+                 #scale_color_viridis_d()+scale_fill_viridis_d()+         # scale_color_brewer(palette = \"Reds\") +          # #geom_smooth(method=\"loess\", se = FALSE, size=1.5) +         geom_ribbon(data=predres,aes(x=x,y=Prediction,ymin=Lower,ymax=Upper,col=model,fill=model),alpha=0.2)+geom_line(data=predres,aes(x=x,y=Prediction,col=model))+         xlab(\"Concentration\") +          ylab(\"% Injury / 100\") p_drc_qb"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"on-the-gof","dir":"Articles","previous_headings":"Regression Approaches","what":"On the GoF","title":"Example Ordinal Data Analysis","text":"Normally test lack fit continuous data comparing model fit ANOVA model, achieve least square estimation. Changjian proposed use prediction intervals models estimate frequency table usually product conventional ordinal regression, followed chi-square test. common goodness fit tests measures logistic regression include deviance likelihood ratio test, Hosmer-Lemeshow Test, pseudo R-squared measures, residual analysis, classification/frequency tables.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"additional-thoughts","dir":"Articles","previous_headings":"","what":"Additional Thoughts","title":"Example Ordinal Data Analysis","text":"Thinking (quasi)-binomial type modelling data-generation process, possible assume see level B injury, many independent binary yes/outcomes (conditional observed covariate values replicate experiment unit) injury level arbitrary number categories, proportion yes answer average injury. probabilities binary outcome , may mismatch observed data rather badly. Beta regression can used model proportions , however, 0 1’s need specifically handled.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"using-conventional-ordinal-regression-approaches","dir":"Articles","previous_headings":"Additional Thoughts","what":"Using Conventional Ordinal Regression Approaches","title":"Example Ordinal Data Analysis","text":"Ordinal regression approaches works well terms prediction need specific function calculate ER50. \\(\\sum p_i Y_{ij}\\) average injury level. proportional odds model can interpreted terms probabilities transitions also terms covariate-specific cumulative probabilities. latter bit easier understand.","code":"## fit ordered logit model and store results 'm' dattab_new $y0 <- factor(dattab_new$y0, levels = c(\"0\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"),ordered = TRUE)  m <- MASS::polr(y0 ~ log(dose), data = dattab_new, Hess=TRUE) summary(m) #> Call: #> MASS::polr(formula = y0 ~ log(dose), data = dattab_new, Hess = TRUE) #>  #> Coefficients: #>           Value Std. Error t value #> log(dose) 3.465     0.4876   7.106 #>  #> Intercepts: #>     Value   Std. Error t value #> 0|A  4.0061  0.8091     4.9513 #> A|B  6.3415  1.0283     6.1669 #> B|C  8.1921  1.2395     6.6092 #> C|D  9.1287  1.3489     6.7677 #> D|E 10.9709  1.5691     6.9917 #> E|F 12.9482  1.8234     7.1011 #>  #> Residual Deviance: 128.7906  #> AIC: 142.7906 ctable <- coef(summary(m))  ## At ER50, the cumulative probability probability of the response being in a higher category is close to 1. plogis(ctable[,1] + ctable[,2]*log(12.18)) #> log(dose)       0|A       A|B       B|C       C|D       D|E       E|F  #> 0.9908438 0.9975973 0.9998653 0.9999875 0.9999963 0.9999997 1.0000000 ## calculate and store p values p <- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2  ## combined table (ctable <- cbind(ctable, \"p value\" = p)) #>               Value Std. Error  t value      p value #> log(dose)  3.465155  0.4876298 7.106117 1.193530e-12 #> 0|A        4.006148  0.8091031 4.951344 7.370254e-07 #> A|B        6.341529  1.0283150 6.166913 6.963590e-10 #> B|C        8.192141  1.2395138 6.609156 3.865169e-11 #> C|D        9.128696  1.3488680 6.767672 1.308710e-11 #> D|E       10.970866  1.5691374 6.991654 2.716642e-12 #> E|F       12.948174  1.8234117 7.101070 1.237950e-12  (ci <- confint(m)) ## profiled CI #>    2.5 %   97.5 %  #> 2.582836 4.504378  exp(cbind(coef(m),t(ci))) #>                       2.5 %   97.5 % #> log(dose) 31.98141 13.23462 90.41212 ## OR and CI exp(cbind(OR = coef(m), ci)) #>              OR       ci #> 2.5 %  31.98141 13.23462 #> 97.5 % 31.98141 90.41212  newdat <- data.frame(dose = unique(dattab_new$dose)) %>% mutate(logdose = log(dose)) (phat <- predict(object = m, newdat, type=\"p\")) #>              0            A           B           C          D            E #> 1 8.326166e-01 0.1483000019 0.016035611 0.001850927 0.00100702 0.0001635762 #> 2 3.105443e-01 0.5126011687 0.144195119 0.019598385 0.01096824 0.0018025754 #> 3 3.918687e-02 0.2573055374 0.431914336 0.144078094 0.10487831 0.0194406374 #> 4 3.342916e-04 0.0031093320 0.018073161 0.031603441 0.20832988 0.4574198047 #> 5 3.679470e-03 0.0330795765 0.158639125 0.187148646 0.41376767 0.1694853156 #> 6 3.027903e-05 0.0002825179 0.001674417 0.003066947 0.02600507 0.1569497884 #>              F #> 1 2.628958e-05 #> 2 2.902582e-04 #> 3 3.196213e-03 #> 4 2.811301e-01 #> 5 3.420020e-02 #> 6 8.119910e-01  phat %>% knitr::kable(.,digits = 3) library(GLMcat)  dattab_new <- dattab_new %>% mutate(logdose = log(dose)) mod_ref_log_c <- glmcat(formula = y0 ~ logdose, ratio = \"reference\", cdf = \"logistic\", data = as.data.frame(dattab_new),ref=\"0\",parallel = F) summary(mod_ref_log_c) #> y0 ~ logdose #>                 ratio      cdf nobs niter    logLik #> Model info: reference logistic   60    15 -61.33542 #>                 Estimate Std. Error z value Pr(>|z|)     #> (Intercept) A     -2.907      1.309  -2.221 0.026323 *   #> (Intercept) B     -5.398      1.835  -2.942 0.003260 **  #> (Intercept) C     -9.347      3.019  -3.096 0.001958 **  #> (Intercept) D    -10.923      3.065  -3.564 0.000366 *** #> (Intercept) E    -17.421      4.838  -3.600 0.000318 *** #> (Intercept) F   -118.572 109742.356  -0.001 0.999138     #> logdose A          2.179      0.993   2.194 0.028225 *   #> logdose B          3.415      1.177   2.901 0.003725 **  #> logdose C          4.803      1.504   3.194 0.001404 **  #> logdose D          5.633      1.499   3.758 0.000172 *** #> logdose E          7.692      1.895   4.058 4.94e-05 *** #> logdose F         36.404  31664.951   0.001 0.999083     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 (phat <- predict(object = mod_ref_log_c, newdat, type=\"prob\")) #>                 A            B            C            D            E #> [1,] 1.904456e-01 3.716346e-02 1.873707e-03 6.891895e-04 4.324730e-06 #> [2,] 4.075170e-01 1.873256e-01 2.472185e-02 1.616443e-02 4.225492e-04 #> [3,] 3.188563e-01 3.452649e-01 1.192710e-01 1.386298e-01 1.509628e-02 #> [4,] 5.226961e-03 3.140645e-02 7.433584e-02 2.730262e-01 5.159543e-01 #> [5,] 7.795418e-02 1.988396e-01 1.797976e-01 3.714905e-01 1.685225e-01 #> [6,] 2.602312e-12 3.683282e-11 2.281986e-10 1.489914e-09 1.172910e-08 #>                 F            0 #> [1,] 2.238527e-41 7.698238e-01 #> [2,] 9.621150e-31 3.638486e-01 #> [3,] 1.512050e-20 6.288173e-02 #> [4,] 1.000000e-01 5.029071e-05 #> [5,] 7.425066e-11 3.395654e-03 #> [6,] 1.000000e+00 5.662137e-15 phat %>% knitr::kable(.,digits = 3) ## (phat <- predict(object = mod_ref_log_c, newdat, type=\"linear.predictor\"))  mod_cum_logis <- glmcat(formula = y0 ~ logdose, ratio = \"cumulative\", cdf = \"logistic\", data = as.data.frame(dattab_new),parallel = TRUE) summary(mod_cum_logis) #> y0 ~ logdose #>                  ratio      cdf nobs niter   logLik #> Model info: cumulative logistic   60     9 -64.3953 #>               Estimate Std. Error z value Pr(>|z|)     #> (Intercept) 0   4.0061     0.8142   4.920 8.64e-07 *** #> (Intercept) A   6.3415     1.0419   6.086 1.16e-09 *** #> (Intercept) B   8.1920     1.2583   6.510 7.50e-11 *** #> (Intercept) C   9.1286     1.3693   6.666 2.62e-11 *** #> (Intercept) D  10.9707     1.6005   6.854 7.16e-12 *** #> (Intercept) E  12.9479     1.8194   7.117 1.11e-12 *** #> logdose        -3.4651     0.4912  -7.054 1.74e-12 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  (phat <- predict(object = mod_cum_logis, newdat, type=\"prob\")) #>                 0            A           B           C           D           E #> [1,] 8.326186e-01 0.1482975531 0.016035767 0.001851046 0.001007102 0.000163596 #> [2,] 3.105557e-01 0.5125923926 0.144191357 0.019598856 0.010968693 0.001802725 #> [3,] 3.919032e-02 0.2573138937 0.431903119 0.144076171 0.104878455 0.019441472 #> [4,] 3.343480e-04 0.0031097342 0.018074387 0.031605029 0.208331116 0.457408504 #> [5,] 3.679947e-03 0.0330824671 0.158642283 0.187148395 0.413758936 0.169485389 #> [6,] 3.028531e-05 0.0002825654 0.001674598 0.003067232 0.026006412 0.156948672 #>                 F #> [1,] 2.629451e-05 #> [2,] 2.903015e-04 #> [3,] 3.196565e-03 #> [4,] 2.811369e-01 #> [5,] 3.420258e-02 #> [6,] 8.119902e-01 phat %>% knitr::kable(.,digits = 3)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"other-understanding-of-the-dataset","dir":"Articles","previous_headings":"Additional Thoughts","what":"Other understanding of the dataset","title":"Example Ordinal Data Analysis","text":"principle, ordinal regression treating ranked categorical several binomial category. Note intercept 0|-1.386294e+00.","code":"fit0 <- MASS::polr(y0 ~  1,                       data = dattab_new,                       Hess= T) fit0 #> Call: #> MASS::polr(formula = y0 ~ 1, data = dattab_new, Hess = T) #>  #> No coefficients #>  #> Intercepts: #>           0|A           A|B           B|C           C|D           D|E  #> -1.386294e+00 -5.465462e-01 -1.793225e-06  2.682586e-01  8.472940e-01  #>           E|F  #>  1.493920e+00  #>  #> Residual Deviance: 228.003  #> AIC: 240.003 ## Note that   #source(\"https://github.com/rwnahhas/RMPH_Resources/raw/main/Functions_rmph.R\") ilogit <- function(x) exp(x)/(1+exp(x)) ilogit(fit0$zeta) #>       0|A       A|B       B|C       C|D       D|E       E|F  #> 0.2000001 0.3666661 0.4999996 0.5666653 0.6999992 0.8166659   sf <- function(y) {   c('Y>=1' = qlogis(mean(y >= 1)),     'Y>=2' = qlogis(mean(y >= 2)),     'Y>=3' = qlogis(mean(y >= 3)),     'Y>=4' = qlogis(mean(y >= 4)),     'Y>=5' = qlogis(mean(y >= 5)),     'Y>=6' = qlogis(mean(y >= 6))     ) } library(Hmisc) (s <- with(dattab_new,summary(as.numeric(y0) ~ (dose), fun=sf))) #> as.numeric(y0)      N= 60   #>  #> +-------+--+--+----+----------+----------+----------+----------+----------+ #> |       |  | N|Y>=1|      Y>=2|      Y>=3|      Y>=4|      Y>=5|      Y>=6| #> +-------+--+--+----+----------+----------+----------+----------+----------+ #> |   dose| 2|10| Inf|-1.3862944|      -Inf|      -Inf|      -Inf|      -Inf| #> |       | 4|10| Inf| 0.8472979|-1.3862944|      -Inf|      -Inf|      -Inf| #> |       | 8|10| Inf| 2.1972246| 0.8472979|-0.8472979|-2.1972246|      -Inf| #> |       |16|10| Inf|       Inf|       Inf| 1.3862944| 0.8472979|-1.3862944| #> |       |32|10| Inf|       Inf| 2.1972246| 2.1972246| 1.3862944| 0.4054651| #> |       |64|10| Inf|       Inf|       Inf|       Inf|       Inf|       Inf| #> +-------+--+--+----+----------+----------+----------+----------+----------+ #> |Overall|  |60| Inf| 1.3862944| 0.5465437| 0.0000000|-0.2682640|-0.8472979| #> +-------+--+--+----+----------+----------+----------+----------+----------+   mod2 <- glm(I(as.numeric(y0) >= 2) ~ factor(dose), family=\"binomial\", data = dattab_new) predict.glm(mod2,data.frame(dose=2)) #>         1  #> -1.386294 predict.glm(mod2,data.frame(dose=4)) #>         1  #> 0.8472979 mod3 <- glm(I(as.numeric(y0) >= 3) ~ factor(dose), family=\"binomial\", data = dattab_new) predict.glm(mod3,data.frame(dose=2)) #>         1  #> -20.56607 glm(I(as.numeric(y0) >= 4) ~ factor(dose), family=\"binomial\", data = dattab_new) #>  #> Call:  glm(formula = I(as.numeric(y0) >= 4) ~ factor(dose), family = \"binomial\",  #>     data = dattab_new) #>  #> Coefficients: #>    (Intercept)   factor(dose)4   factor(dose)8  factor(dose)16  factor(dose)32   #>     -2.057e+01       9.728e-12       1.972e+01       2.195e+01       2.276e+01   #> factor(dose)64   #>      4.113e+01   #>  #> Degrees of Freedom: 59 Total (i.e. Null);  54 Residual #> Null Deviance:       83.18  #> Residual Deviance: 28.73     AIC: 40.73"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"log-logistic-with-quasi-binomial-replicate-effects","dir":"Articles","previous_headings":"Additional Thoughts","what":"log-logistic with quasi-binomial: replicate effects","title":"Example Ordinal Data Analysis","text":"Original study design CRD. However, argued treat RCBD blocking effect coming replicate numbering. can compare result differences. Note glmer glmmPQL (based lme nlme pacakge) differs terms parameter estimation algorithm nlme optimized dealing crossed random effects, associated sparse design matrix. See book Pinheiro & Bates.[^1]","code":"mod <- glm(yy2~log(dose),data = dattab_new,family = quasibinomial) summary(mod) #>  #> Call: #> glm(formula = yy2 ~ log(dose), family = quasibinomial, data = dattab_new) #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  -3.5064     0.3132  -11.19 4.02e-16 *** #> log(dose)     1.3832     0.1171   11.81  < 2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for quasibinomial family taken to be 0.1163524) #>  #>     Null deviance: 31.8277  on 59  degrees of freedom #> Residual deviance:  6.3595  on 58  degrees of freedom #> AIC: NA #>  #> Number of Fisher Scoring iterations: 5 ER50 <- exp(-coef(mod)[1]/coef(mod)[2]) ER50 #> (Intercept)  #>      12.617 getEC50(mod) #>     EC50    lower    upper       se #> 1 12.617 10.77038 14.78023 1.023657 pred <- predict(mod,newdata = dattab_new,type = \"response\") dattab_new$pred <- pred dattab_new %>% group_by(y0) %>% summarise(n=n(),meany=mean(yt),meanyy2=mean(yy2),meanEst=mean(pred)) %>% knitr::kable(.,digits = 3) ## Consider Replicate effect modr <- MASS::glmmPQL(yy2~ log(dose),random=~1|rep,family=\"quasibinomial\",data=dattab_new) ER50 <- exp(-coef(modr)[1]/coef(modr)[2]) exp(-modr$coefficients$fixed[1]/ modr$coefficients$fixed[2]) #> (Intercept)  #>    12.61695 summary(modr) #> Linear mixed-effects model fit by maximum likelihood #>   Data: dattab_new  #>   AIC BIC logLik #>    NA  NA     NA #>  #> Random effects: #>  Formula: ~1 | rep #>         (Intercept)  Residual #> StdDev:   0.2256918 0.3155037 #>  #> Variance function: #>  Structure: fixed weights #>  Formula: ~invwt  #> Fixed effects:  yy2 ~ log(dose)  #>                 Value Std.Error DF   t-value p-value #> (Intercept) -3.517527 0.3039553 49 -11.57251       0 #> log(dose)    1.387562 0.1103306 49  12.57640       0 #>  Correlation:  #>           (Intr) #> log(dose) -0.907 #>  #> Standardized Within-Group Residuals: #>        Min         Q1        Med         Q3        Max  #> -3.9592347 -0.3047297  0.1382668  0.4944066  1.9353183  #>  #> Number of Observations: 60 #> Number of Groups: 10 getEC50(modr) #>              EC50    lower    upper       se #> p = 0.5: 12.61695 10.56476 15.06777 1.149772 pred <- predict(modr,newdata = dattab_new,type = \"response\") dattab_new$predr <- pred dattab_new %>% group_by(y0) %>% summarise(n=n(),meany=mean(yt),meanyy2=mean(yy2),meanEst=mean(pred),meanEstR=mean(predr)) %>% knitr::kable(.,digits = 3)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"drm-ordinal","dir":"Articles","previous_headings":"Additional Thoughts","what":"DRM Ordinal","title":"Example Ordinal Data Analysis","text":"","code":"library(drc) library(bmd) dat_ord <- dattab_new %>% group_by(y0,dose) %>% summarise(n=n()) %>% ungroup() %>% pivot_wider(names_from = y0,values_from = n) dat_ord <- dat_ord %>% mutate(across(c(`0`,`A`,`B`,`C`,`D`,`E`,`F`),~ifelse(is.na(.),0,.))) %>% mutate(total=rowSums(across(c(`0`,`A`,`B`,`C`,`D`,`E`,`F`)))) mod_ord <- drmOrdinal(levels = unique(dattab_new$y0),  weights=\"total\",dose = \"dose\", data = dat_ord, fct = LL.2()) plot(mod_ord) # uses ggplot bmdOrdinal(mod_ord, bmr = 0.5, backgType = \"modelBased\", def = \"excess\") #>                   BMD      BMDL #> A+B+C+D+E+F  3.158208  2.298134 #> B+C+D+E+F    6.467922  4.757317 #> C+D+E+F     11.313719  8.507258 #> D+E+F       14.947346 11.193140 #> E+F         26.084222 20.202286 #> F           35.967446 -1.757891"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"rscabs-and-other-testing-approaches","dir":"Articles","previous_headings":"","what":"RSCABS and other testing approaches","title":"Example Ordinal Data Analysis","text":"Details RSCABS approach can find testing approaches implemented. example post hoc testings available regression modeling independent variable dose levels factor. testing approaches targeting NOEC certain injury level. default, testing approaches actually treat test rates concentrations study categorical variable, losing continuous property therefore ER50 directly calculated. However, NOEC medium injury level comparable ER50, always smaller ER50.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"simulation-study","dir":"Articles","previous_headings":"","what":"Simulation Study","title":"Example Ordinal Data Analysis","text":"Simulation studies show glm quasibinomial nonlinear log-logistic normal perfom similarly limited dose groups. observe full dose-response curve, quasibinomial less flexible adapting data.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"data-generated-using-the-logistic-cdf-","dir":"Articles","previous_headings":"Simulation Study","what":"Data generated using the logistic CDF.","title":"Example Ordinal Data Analysis","text":"data generated using cumulative function logistic distribution. Note logistic function just inverse logit (\\(\\log(p/(1-p))\\)) function R. code modified based blog post. \\[F(x;\\mu,s) = \\frac{1}{2} + \\frac{1}{2} \\tanh((x-\\mu)/2s) = \\frac{1}{1+e^{-(x-\\mu)/s}}\\] , \\(\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\), hyperbolic tangent function maps real numbers range -1 1. Quantile CDF \\(\\mu+s\\log(\\frac{p}{1-p})\\), therefore, EC50 \\(\\mu\\) \\(\\exp(\\mu)\\). Random noises added afterwards logistic distribution CDF. simulate \\(n\\) studies concentration \\(x\\), denoted, \\(X_{1}, X_{2}, …, X_{n}\\) \\(k\\) study \\(X=(x_{1}, x_{2}, …, x_{k})\\), \\(k\\) number different studies different \\(\\mu\\) \\(\\sigma\\). Let’s say \\(k=6\\) study groups following parameter sets, \\(\\mu = \\{9,2,3,5,7,5\\}\\) \\(s=\\{2,2,4,3,4,2\\}\\)   Nonlinear log-logistic modelling normal errors seem flexible dealing different curve shapes. also fits data generation process case.","code":"generate_logit_cdf <- function(mu, s,                                 sigma_y=0.1,                                 x=seq(-5,20,0.1)) {   x_ms <- (x-mu)/s   y    <- 0.5 + 0.5 * tanh(x_ms)     y    <- abs(y + rnorm(length(x), 0, sigma_y))   ix   <- which(y>=1.0)   if(length(ix)>=1) {      y[ix] <- 1.0   }   return(y) } tanh(0) #> [1] 0 set.seed(424242) x      <- seq(-5,15,0.025)  mu_vec <- c(1,2,3,5,7,8)   # 6 Studies idx <- sapply(mu_vec,function(mu) which(x==mu))  ## we just need to know which index to find the ER50 s_vec  <- c(2,2,4,3,4,2) # Synthetic Study ID studies_df<- mapply(generate_logit_cdf,                                mu_vec,                                s_vec,                                MoreArgs = list(x=x)) # Give them names colnames(studies_df) <- c(\"Study1\", \"Study2\", \"Study3\", \"Study4\", \"Study5\", \"Study6\")    dim(studies_df) #> [1] 801   6 library(ggplot2) theme_set(theme_bw()) library(tidyverse) df_all <- tidyr::pivot_longer(data.frame(x=(1:length(x))/10,studies_df),cols=2:7) true_ec50 <- ((1:length(x))/10)[idx] colnames(df_all) <- c(\"x\", \"study\", \"y\") df_all$study <- as.factor(df_all$study)  p_quasibinomial<- ggplot(df_all, aes(x=x, y=y, color=study)) +         geom_point(alpha=0.2)  +         scale_color_viridis_d()+         # scale_color_brewer(palette = \"Reds\") +          # #geom_smooth(method=\"loess\", se = FALSE, size=1.5) +         geom_smooth(aes(group=study),method=\"glm\", method.args=list(family=\"quasibinomial\"), formula=\"y ~ log(x)\",                        se =TRUE, size=1.5) +         xlab(\"Concentration\") +          ylab(\"% Injury / 100\") + ggtitle(\"Quasibinomial Fit\")  p_quasibinomial#+scale_x_log10() p_quasibinomial +facet_wrap(~study) library(drc) p_drc<- ggplot(df_all, aes(x=x, y=y, color=study)) +         geom_point(alpha=0.2)  +         scale_color_viridis_d()+         # scale_color_brewer(palette = \"Reds\") +          # #geom_smooth(method=\"loess\", se = FALSE, size=1.5) +         geom_smooth(aes(group=study),method=\"drm\", method.args=list(fct=LL.2()), formula=\"y ~ x\",                        se = FALSE, size=1.5) +         xlab(\"Concentration\") +          ylab(\"% Injury / 100\") + ggtitle(\"DRC nonlinear normal fit\") p_drc#+scale_x_log10() p_drc + facet_wrap(~study) df_nested <- df_all %>% group_by(study) %>% nest() dfres <- df_nested %>% mutate(modres=map(data,modelall)) #>  #> Estimated effective doses #>  #>        Estimate Std. Error    Lower    Upper #> e:1:50 23.72221    0.13908 23.44921 23.99521 #>  #> Estimated effective doses #>  #>        Estimate Std. Error    Lower    Upper #> e:1:50 27.53930    0.14351 27.25759 27.82100 #>  #> Estimated effective doses #>  #>        Estimate Std. Error    Lower    Upper #> e:1:50 30.85168    0.20483 30.44961 31.25375 #>  #> Estimated effective doses #>  #>        Estimate Std. Error    Lower    Upper #> e:1:50 39.35892    0.18648 38.99287 39.72498 #>  #> Estimated effective doses #>  #>        Estimate Std. Error    Lower    Upper #> e:1:50 47.09152    0.20831 46.68262 47.50042 #>  #> Estimated effective doses #>  #>        Estimate Std. Error    Lower    Upper #> e:1:50 51.84412    0.14198 51.56541 52.12283 ec50 <- dfres %>% mutate(ec50=modres[[1]][1]) %>% dplyr::select(-c(data,modres)) %>% unnest(cols=c(ec50)) %>% ungroup() %>% mutate(TrueEC50 = rep(true_ec50,each=2)) knitr::kable(ec50) predres <- dfres %>% mutate(preds=modres[[1]][2]) %>% dplyr::select(-c(data,modres)) %>% unnest(cols=c(preds)) %>% ungroup() p_drc_qb <- ggplot(df_all, aes(x=x, y=y)) +         geom_point(alpha=0.1)  +         scale_color_viridis_d()+         # scale_color_brewer(palette = \"Reds\") +          # #geom_smooth(method=\"loess\", se = FALSE, size=1.5) +         geom_smooth(aes(group=study),method=\"drm\", method.args=list(fct=LL.2()), formula=\"y~x\",                        se = FALSE, size=1.5,col=scales::hue_pal()(2)[1]) +                        geom_smooth(aes(group=study),method=\"glm\", method.args=list(family=\"quasibinomial\"), formula=\"y~log(x)\",                        se = FALSE, size=1.5,lty=3,col=scales::hue_pal()(2)[2])+         xlab(\"Concentration\") +          ylab(\"% Injury / 100\")  + facet_wrap(~study)  p_drc_qb + geom_hline(yintercept=0.5,col=\"purple\") + geom_vline(data=ec50,aes(xintercept=TrueEC50),col=\"purple\")+ geom_ribbon(data=predres,aes(x=x,ymin=Lower,ymax=Upper,y=Prediction,fill=model),alpha=0.3) + ggtitle(\"drc and quasibinomial fit with CI\") + geom_vline(data=ec50,aes(xintercept=EC50,col=model))"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Ordinal-Data.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Example Ordinal Data Analysis","text":"Agresti, . (2002) Categorical Data Analysis, Second Edition. Hoboken, New Jersey: John Wiley & Sons, Inc. Harrell, F. E, (2001) Regression Modeling Strategies. New York: Springer-Verlag. http://www.sthda.com/english/articles/32-r-graphics-essentials/132-plot-grouped-data-box-plot-bar-plot--/","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Quantal-Data.html","id":"tarones-test","dir":"Articles","previous_headings":"","what":"Tarone’s Test","title":"Quantal Data","text":"","code":"mymatrix1 <- matrix(c(4,5,5,103),nrow=2,byrow=TRUE) colnames(mymatrix1) <- c(\"Disease\",\"Control\") rownames(mymatrix1) <- c(\"Exposure\",\"Unexposed\") mymatrix2 <- matrix(c(10,3,5,43),nrow=2,byrow=TRUE) colnames(mymatrix2) <- c(\"Disease\",\"Control\") rownames(mymatrix2) <- c(\"Exposure\",\"Unexposed\") mylist <- list(mymatrix1,mymatrix2) calcTaronesTest(mylist) ## [1] \"Pvalue for Tarone's test = 0.627420741721689\" ## $pval ## [1] 0.6274207 ##  ## $tarone ##  ## Equal-Effects Model (k = 2) ##  ## I^2 (total heterogeneity / total variability):  0.00% ## H^2 (total variability / sampling variability): 0.24 ##  ## Test for Heterogeneity:  ## Q(df = 1) = 0.2423, p-val = 0.6225 ##  ## Model Results (log scale): ##  ## estimate      se    zval    pval   ci.lb   ci.ub  ##   3.1355  0.5741  5.4613  <.0001  2.0102  4.2608  ##  ## Model Results (OR scale): ##  ## estimate   ci.lb    ci.ub  ##  23.0006  7.4652  70.8663  ##  ## Cochran-Mantel-Haenszel Test:    CMH = 36.6043, df = 1, p-val < 0.0001 ## Tarone's Test for Heterogeneity: X^2 =  0.2356, df = 1, p-val = 0.6274"},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/System_Testing.html","id":"purpose-of-testing","dir":"Articles","previous_headings":"","what":"Purpose of Testing","title":"System Testing","text":"testing phase ensure R package meets user requirements (URS) behaves end users expect. tests fail, possible detect loopholes fix errors early development. Note access security scalability taken care V-COP environment. purpose testing limited functional requirements. Interfacing ability via API V-COP file management validated run environment designed tested manually.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/TSK_method.html","id":"backaground","dir":"Articles","previous_headings":"","what":"Backaground","title":" Spearman-Karber method","text":"Spearman-Karber method still use regulatory Ecotox endpoints derivation. common approach derive EC50 interpolation smoothing line.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/TSK_method.html","id":"method","dir":"Articles","previous_headings":"","what":"Method","title":" Spearman-Karber method","text":"Two commonly used methods calculating 50% endpoint using serial dilutions Spearman-Karber method Reed Muench method. original paper written Kärber published 1931 German [Kärber, G. Archiv f. experiment. Pathol. u. Pharmakol (1931) 162: 480-483 doi:10.1007/BF01863914].","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/TSK_method.html","id":"reed-and-muench-method","dir":"Articles","previous_headings":"Method","what":"Reed and Muench method","title":" Spearman-Karber method","text":"log10 50% end point dilution = log10 dilution showing mortality next 50% - (difference logarithms × logarithm dilution factor). Generally, following formula used calculate “difference logarithms” (difference logarithms also known “proportionate distance” “interpolated value”): Difference logarithms = [(mortality dilution next 50%)-50%]/[(mortality next 50%)-(mortality next 50%)].","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/TSK_method.html","id":"spearman-karber-method","dir":"Articles","previous_headings":"Method","what":"Spearman-Karber method","title":" Spearman-Karber method","text":"log10 50% end point dilution = \\(- (x_0 - d/2 + d \\sum r_i/n_i)\\) \\(x_0\\) = log10 reciprocal highest dilution (lowest concentration) animals positive; \\(d\\) = log10 dilution factor; \\(n_i\\) = number animals used individual dilution (discounting accidental deaths); \\(r_i\\) = number positive animals (\\(n_i\\)). Summation started dilution \\(x_0\\). Newly proposed method Formula 1: log10 50% end point dilution = -[(total number animals died/number animals inoculated per dilution) + 0.5] × log dilution factor. Formula 2 (accidental death occurred): log10 50% end point dilution = -(total death score + 0.5) × log dilution factor.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/TSK_method.html","id":"some-notes","dir":"Articles","previous_headings":"","what":"Some Notes","title":" Spearman-Karber method","text":"tsk function comes R package. Since single function without dependencies, bundled helper package purpose validation verification. original goal R package replicate results DOS program used provided EPA perform trimmed Spearman-Karber method. list “expected” contains results DOS program run data Hamilton et al. NA EPA program didn’t return result. EPA program uses confidence 2*pnorm(2)-1=0.9544997 (, exactly two sigmas sides).","code":"## install.packages(\"isotone\") ## devtools::install_github(repo=\"brsr/tsk\")  library(drcHelper) #> Loading required package: drc #> Loading required package: MASS #>  #> 'drc' has been loaded. #> Please cite R and 'drc' if used for a publication, #> for references type 'citation()' and 'citation('drc')'. #>  #> Attaching package: 'drc' #> The following objects are masked from 'package:stats': #>  #>     gaussian, getInitial ## library(tsk) tsk(c(1, 10, 100, 1000), 20, c(0, 3, 17, 20)) #>  #> Trimmed Spearman-Karber method using 0 percent trim #>  #> Data was not smoothed #> Calculation done using the logs of the doses #> Estimated LD50: 31.62278 GSD of estimate: 1.296928 #> 95 percent confidence interval on LD50: #>  18.99717 52.63942  data(hamilton)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/TSK_method.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":" Spearman-Karber method","text":"https://www.cureffi.org/2015/09/20/-math-behind-spearman-karber-analysis/ Hamilton, M. .; Russo, R. C.; Thurston, R. V. Trimmed Spearman-Karber Method Estimating Median Lethal Concentrations Toxicity Bioassays. Enviro. Sci. Tech. 1977, 11 (7), 714-719. http://dx.doi.org/10.1021/es60130a004 Ibid, 1978, 12 (4), 417. http://dx.doi.org/10.1021/es60140a017","code":""},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Trend-Testing.html","id":"trend-test-by-testing-linear-and-quadratic-contrasts","dir":"Articles","previous_headings":"","what":"Trend Test by Testing Linear and Quadratic Contrasts","title":"Trend Testing","text":"Generate mock data testing","code":"library(ggplot2) mock_data <- data.frame(   treatment_var = factor(rep(c(\"Control\", \"Dose1\", \"Dose2\", \"Dose3\"), each = 10)),   response_var = c(rnorm(10, mean = 5), rnorm(10, mean = 7), rnorm(10, mean = 8), rnorm(10, mean = 10)) ) ggplot(mock_data,aes(x=treatment_var,y=response_var))+geom_point() result <- monotonicityTest(mock_data, \"treatment_var\", \"response_var\") result #>        Test t value Pr(>|t|) Significance #> 1    Linear   11.16  <0.0001          *** #> 2 Quadratic   -0.10   0.9196            ."},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Validation_MCP_tests.html","id":"multiple-comparison-tests","dir":"Articles","previous_headings":"","what":"Multiple Comparison Tests","title":"Verification and Validation of NOEC Derivation by MCP","text":"","code":"library(drcHelper) #> Loading required package: drc #> Loading required package: MASS #>  #> 'drc' has been loaded. #> Please cite R and 'drc' if used for a publication, #> for references type 'citation()' and 'citation('drc')'. #>  #> Attaching package: 'drc' #> The following objects are masked from 'package:stats': #>  #>     gaussian, getInitial library(PMCMRplus) #>  #> Attaching package: 'PMCMRplus' #> The following object is masked from 'package:drc': #>  #>     algae"},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Validation_MCP_tests.html","id":"dunnetts-test","dir":"Articles","previous_headings":"","what":"Dunnett’s Test","title":"Verification and Validation of NOEC Derivation by MCP","text":"glht pulls P-values multivariate t distribution. mvtnorm::pmvt called observed t statistics correlation matrix (actual code ). DescTools::DunnettTest roughly . emmeans uses close approximation Dunnett adjustment.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Validation_MCP_tests.html","id":"williams-test","dir":"Articles","previous_headings":"","what":"Williams’ Test","title":"Verification and Validation of NOEC Derivation by MCP","text":"PMCMRplus::williamsTest produces accurate results since regarding calculation degrees freedom, based isotonic regression. functions use tabulated critical values provided Williams (=0.05). williamsTest function PMCMRplus produces results williamsTest_JG","code":"x <- c(106, 114, 116, 127, 145,110, 125, 143, 148, 151, 136, 139, 149, 160, 174) g <- gl(3,5) levels(g) <- c(\"0\", \"I\", \"II\") PMCMRplus::williamsTest(x ~ g) #>  #>   Williams trend test  #>  #> data:  x by g  #> alternative hypothesis:  greater  #>  #> H0 #>                t'-value df t'-crit decision alpha #> mu1 - ctr <= 0    1.357 12   1.782   accept  0.05 #> mu2 - ctr <= 0    2.950 12   1.873   reject  0.05 #> --- williamsTest_JG(data.frame(treatment_var = g,response_var=x),\"response_var\",\"treatment_var\",direction=\"increasing\") #>   treatment_var Y.Tilde    Y0 Se.Diff DF  Will TCrit Signif #> 2            II   151.6 121.6   10.17 12 2.950 1.873      * #> 1             I   135.4 121.6   10.17 12 1.357 1.782      ."},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/Validation_MCP_tests.html","id":"tips","dir":"Articles","previous_headings":"","what":"Tips","title":"Verification and Validation of NOEC Derivation by MCP","text":"Note install “PMCMRplus” properly ubuntu (Platform: x86_64-pc-linux-gnu), need probably install libmpfr-dev first.","code":"sudo apt-get install libmpfr-dev"},{"path":[]},{"path":[]},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/articles/drcHelper.html","id":"dependencies","dir":"Articles","previous_headings":"","what":"Dependencies","title":"drcHelper","text":"","code":"library(drcHelper) library(DependenciesGraphs) dep <- funDependencies(\"package:drcHelper\",\"ED.plus\") plot(dep) dep <- funDependencies(\"package:drcHelper\",\"mselect.ED\") plot(dep)  #dep <- funDependencies(\"package:drcHelper\",\"mselect.plus\") #plot(dep)  dep <- envirDependencies('package:drcHelper') plot(dep)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Zhenglei Gao. Author, maintainer. Sean Ryan. Author.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Gao Z, Ryan S (2025). drcHelper: Collection verification helper functions dose-response analysis. R package version 0.0.1, https://bayer-group.github.io/drcHelper/index.html.","code":"@Manual{,   title = {drcHelper: Collection and verification of helper functions for dose-response analysis},   author = {Zhenglei Gao and Sean Ryan},   year = {2025},   note = {R package version 0.0.1},   url = {https://bayer-group.github.io/drcHelper/index.html}, }"},{"path":"https://bayer-group.github.io/drcHelper/index.html/index.html","id":"drchelper","dir":"","previous_headings":"","what":"Collection and verification of helper functions for dose-response analysis","title":"Collection and verification of helper functions for dose-response analysis","text":"goal drcHelper assist routine dose-response analysis providing collection helper functions standalone functions generic may useful beyond organization. part GLP stat pilot project, package serves cornerstone second use case, EFX Statistics. streamline GLP statistical analyses various dose-response studies test assays within registration data package. ensures analyses remain current, state---art, flexible enough adapt new regulatory requirements complying GLP standards. package also includes test cases examples help regulatory statistical community understand reasons behind different outcomes. instance, point estimations p-values may vary depending parties involved, functions used, packages selected. aims promote harmonized understanding methodologies provide foundation standardized practices regulatory statistics field plant protection product registration. Additionally, hoped project contribute ongoing OECD 54 revision process.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Collection and verification of helper functions for dose-response analysis","text":"can install development version drcHelper GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"Bayer-Group/drcHelper\")"},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/index.html","id":"data-overview","dir":"","previous_headings":"","what":"Data Overview","title":"Collection and verification of helper functions for dose-response analysis","text":"","code":"library(drcHelper) library(drc) library(dplyr) library(purrr) library(ggplot2) theme_set(theme_bw()) sum1 <- oecd201 %>% group_by(Time,Treatment) %>% summarise(Yield_mean=mean(Yield),Yield_sd=sd(Yield),GrowthRate_mean=mean(GrowthRate),GrowthRate_sd=sd(GrowthRate)) sum0 <- sum1%>%filter(Treatment==\"Control\")%>%rename(Yield0=Yield_mean,GrowthRate0=GrowthRate_mean)%>%dplyr::select(c(Time,Yield0,GrowthRate0)) # sum0 sumtab <- left_join(sum1%>%filter(Time>0),sum0) %>% mutate(Yield_Inhibition=(Yield0-Yield_mean)/Yield0*100,GrowthRate_Inhibition=(GrowthRate0-GrowthRate_mean)/GrowthRate0*100) %>% dplyr::select(c(Time,Treatment,Yield_mean,Yield_sd,Yield_Inhibition,GrowthRate_mean,GrowthRate_sd,GrowthRate_Inhibition)) sumtab%>%dplyr::select(c(Yield_mean,Yield_sd,Yield_Inhibition))%>%filter(Time==72)%>%knitr::kable(.,digits = 2,caption=\"<center><strong>Yield Summary at Time 72h<strong><center>\",escape = FALSE)##%>% kableExtra::kable_styling(bootstrap_options = \"striped\")##%>%kableExtra::kable_classic_2() sumtab%>%dplyr::select(c(GrowthRate_mean,GrowthRate_sd,GrowthRate_Inhibition))%>%filter(Time==72)%>%knitr::kable(.,digits = 2,caption=\"<center><strong>Growth Rate Summary at Time 72h<strong><center>\",escape = FALSE)##%>%kableExtra::kable_classic()"},{"path":"https://bayer-group.github.io/drcHelper/index.html/index.html","id":"model-fitting-and-comparison-for-yield","dir":"","previous_headings":"","what":"Model Fitting and Comparison For Yield","title":"Collection and verification of helper functions for dose-response analysis","text":"Note default settings, fitted models converge except LL.3 model. 14 day TSL Yield 14 day TSL Yield, Model Comparison Yield Model Fits   Total Shoot Length Growth Yield 14 day","code":"datTn<- subset(oecd201,Time==72)  mod <- drm(Yield~Concentration,data=datTn,fct=LL.3()) fctList <- list(LL2.3(),W2.3(),W1.3(),EXD.3(),EXD.2(),LN.3(),W2.4(),LL.4(),LL2.4()) plot(mod,type=\"all\") res <- mselect.plus(mod,fctList = fctList ) #> [1] \"Model not Converged, Please consult a statistician.\" #> [1] \"Model not Converged, Please consult a statistician.\" #> [1] \"Model not Converged, Please consult a statistician.\" #> [1] \"Model not Converged, Please consult a statistician.\" #> [1] \"Model not Converged, Please consult a statistician.\" #> [1] \"Model not Converged, Please consult a statistician.\" #> [1] \"Model not Converged, Please consult a statistician.\" #> [1] \"Model not Converged, Please consult a statistician.\" #> [1] \"Model not Converged, Please consult a statistician.\" modList <- res$modList edResTab <- mselect.ED(modList = modList,respLev = c(10,20,50),trend=datTn$Trend_Yield[1]) plot.edList(edResTab) resComp <- drcCompare(modRes = res,trend=\"Decrease\") knitr::kable(edResTab[1:3,],caption = \"14 day TSL Yield\",digits = 3) knitr::kable(resComp[1,],caption = \"14 day TSL Yield, Model Comparison\",digits = 3) plot.modList(modList,scale=\"logx\") plot.modList(modList[c(1,2,3,4)],scale=\"logx\",npts=40) p <-plot.modList(modList[c(1)],scale=\"logx\",npts=80)+theme(legend.position = \"none\")+ggtitle(\"14 day Total Shoot Length, \\n3-parameter type II Weibull Model Fit\") addECxCI(p=p,object=modList[[1]],EDres=NULL,trend=\"Decrease\",endpoint=\"EC\", respLev=c(10,20,50),                      textAjust.x=0.01,textAjust.y=1,useObsCtr=FALSE,d0=NULL,textsize = 4,lineheight = 1,xmin=0.012)+ylab(\"Total Shoot Length [cm]\") + xlab(\"Concentration [µg a.s./L]\") ## ggsave(\"TSL_14d_Yield.png\") resED <- t(edResTab[1:3, c(2,4,5,6)]) colnames(resED) <- paste(\"EC\", c(10,20,50)) knitr::kable(resED,caption = \"Total Shoot Length Growth Yield at 14 day\",digits = 3) mod <-modList[[1]] edres <- ED.plus(mod,c(5,10,20,50),trend=\"Decrease\") pander::pander(as.data.frame(edres)) modsum <- summary(mod) pander::pander(coef(modsum))"},{"path":"https://bayer-group.github.io/drcHelper/index.html/index.html","id":"todo","dir":"","previous_headings":"","what":"ToDo","title":"Collection and verification of helper functions for dose-response analysis","text":"Develop test cases NOEC functions Prepare templates standard outputs . Update documentation.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/index.html","id":"contribution-notes","dir":"","previous_headings":"","what":"Contribution Notes","title":"Collection and verification of helper functions for dose-response analysis","text":"Please create pull request contribute development packages. Note source branch branch currently working run gh pr create command. use pkgdown github workflow, vignettes need pre-knit pushing remote github repository.","code":"gh pr create --title \"Title of the pull request\" --body \"Description of the pull request\" gh pr create --title \"Title of the pull request\" --body \"Description of the pull request\" --base develop knitr::knit(\"vignettes/drcHelper.Rmd.orig\", output = \"vignettes/drcHelper.Rmd\") knitr::knit(\"vignettes/articles/Example_RSCABS.Rmd.orig\", output = \"vignettes/articles/Example_RSCABS.Rmd\") knitr::knit(\"vignettes/articles/Examples using NLS.Rmd.orig\", output = \"vignettes/articles/Examples using NLS.Rmd\") knitr::knit(\"vignettes/articles/Examples_drc.Rmd.orig\", output = \"vignettes/articles/Examples_drc.Rmd\") knitr::knit(\"vignettes/articles/Examples_oecd201.Rmd.orig\", output = \"vignettes/articles/Examples_oecd201.Rmd\")"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/DixonQ.html","id":null,"dir":"Reference","previous_headings":"","what":"Dixon's outlier test critical Q table — DixonQ","title":"Dixon's outlier test critical Q table — DixonQ","text":"Dixon's outlier test critical Q table","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/DixonQ.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dixon's outlier test critical Q table — DixonQ","text":"","code":"DixonQ"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/DixonQ.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dixon's outlier test critical Q table — DixonQ","text":"object class data.frame 14 rows 2 columns.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/DixonQ.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Dixon's outlier test critical Q table — DixonQ","text":"DIXON, W. J. (1950) Analysis extreme values. Ann. Math. Stat. 21, 488–506. DEAN, R. B., DIXON, W. J. (1951) Simplified statistics small numbers observation. Anal. Chem. 23, 636–638.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/DixonQ.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Dixon's outlier test critical Q table — DixonQ","text":"Zhenglei Gao","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/ECx_rating.html","id":null,"dir":"Reference","previous_headings":"","what":"Title — ECx_rating","title":"Title — ECx_rating","text":"Title","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/ECx_rating.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Title — ECx_rating","text":"","code":"ECx_rating(x)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/ECx_rating.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Title — ECx_rating","text":"x","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/ED.ZG.html","id":null,"dir":"Reference","previous_headings":"","what":"Deprecated helper function for ED calculation — ED.ZG","title":"Deprecated helper function for ED calculation — ED.ZG","text":"Deprecated helper function ED calculation","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/ED.ZG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated helper function for ED calculation — ED.ZG","text":"","code":"# S3 method for class 'ZG' ED(   object,   respLev,   maxEff = TRUE,   trend = \"Increase\",   range = \"Percentage\",   CI = c(\"delta\", \"inv\", \"bmd-inv\"),   ... )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/ED.ZG.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Deprecated helper function for ED calculation — ED.ZG","text":"Due old ECxHelper development ED.plus defined ED.ZG","code":""},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/ED.plus.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculating ED following the regulatrory ED definition. — ED.plus","title":"Calculating ED following the regulatrory ED definition. — ED.plus","text":"Calculating ED following regulatrory ED definition.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/ED.plus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculating ED following the regulatrory ED definition. — ED.plus","text":"","code":"# S3 method for class 'plus' ED(   object,   respLev,   maxEff = TRUE,   trend = \"Increase\",   range = \"Percentage\",   CI = c(\"delta\", \"inv\", \"bmd-inv\"),   ... )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/ED.plus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculating ED following the regulatrory ED definition. — ED.plus","text":"object fitted model using drc::drm respLev response level, x ECx. maxEff used now. maximum effect. trend Increase Decrease, whether dose response decreasing increasing compared control. range using Percentage, used now. ... parameters ED function interval methods calculate confidence intervals!","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/ED.plus.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculating ED following the regulatrory ED definition. — ED.plus","text":"Back calculated regulatory ECx","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/NTP_example.html","id":null,"dir":"Reference","previous_headings":"","what":"NTP_example data — NTP_example","title":"NTP_example data — NTP_example","text":"NTP_example data","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/NTP_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"NTP_example data — NTP_example","text":"","code":"NTP_example"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/NTP_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"NTP_example data — NTP_example","text":"NTP example data corrected uncorrected emergence, survival, reduction.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/NTP_example_rate.html","id":null,"dir":"Reference","previous_headings":"","what":"Expected processed data from the NTP_example — NTP_example_rate","title":"Expected processed data from the NTP_example — NTP_example_rate","text":"Expected processed data NTP_example","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/NTP_example_rate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expected processed data from the NTP_example — NTP_example_rate","text":"","code":"NTP_example_rate"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/NTP_example_rate.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Expected processed data from the NTP_example — NTP_example_rate","text":"NTP example rate data","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/RSCABK.html","id":null,"dir":"Reference","previous_headings":"","what":"Runs the kth slice of RSCABS — RSCABK","title":"Runs the kth slice of RSCABS — RSCABK","text":"Runs kth slice RSCABS","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/RSCABK.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Runs the kth slice of RSCABS — RSCABK","text":"","code":"RSCABK(x.i.j, n.i.j, m.i, TestK, test.type)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/RSCABK.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Runs the kth slice of RSCABS — RSCABK","text":"x..j matrix containing number observed \"successes\" replicate treatment j. n..j matrix containing number observations replicate treatment j. m.matrix number units treatment/replicate combination. TestK kth severity score tested. test.type Indicate type  analysis performed. Use \"RS\" select Rao-Scott adjustment Cochran-Armitage test \"CA\" ignore adjustment.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/RSCABK.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Runs the kth slice of RSCABS — RSCABK","text":"Returns list following values: Response endpoint tested. Treatment treatment level. R-Score severity score histology. Statistic test statistic corresponding row's endpoint treatment level R-Score. P-Value corresponding p-value Signif significance flag cutoffs stars dot 0, 1e-04, 0.001, 0.01,0.05,1.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/RSCABK.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Runs the kth slice of RSCABS — RSCABK","text":"Joe Swintek","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/Tarone.test.html","id":null,"dir":"Reference","previous_headings":"","what":"Tarone's Z Test — Tarone.test","title":"Tarone's Z Test — Tarone.test","text":"Tests goodness fit binomial distribution.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/Tarone.test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tarone's Z Test — Tarone.test","text":"","code":"Tarone.test(N, M)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/Tarone.test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tarone's Z Test — Tarone.test","text":"N Trials M Counts","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/Tarone.test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tarone's Z Test — Tarone.test","text":"htest object","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/Tarone.test.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Tarone's Z Test — Tarone.test","text":"https://stats.stackexchange.com//410376/6378 R. E. TARONE, Testing goodness fit binomial distribution, Biometrika, Volume 66, Issue 3, December 1979, Pages 585–590, https://doi.org/10.1093/biomet/66.3.585","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/Tarone.test.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Tarone's Z Test — Tarone.test","text":"Ben O'Neill","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/Tarone.test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tarone's Z Test — Tarone.test","text":"","code":"#Generate example data N <- c(30, 32, 40, 28, 29, 35, 30, 34, 31, 39) M <- c( 9, 10, 22, 15,  8, 19, 16, 19, 15, 10) Tarone.test(N, M) #>  #> \tTarone's Z test #>  #> data:  M successes from N trials #> z = 2.5988, p-value = 0.009355 #> alternative hypothesis: true dispersion parameter is greater than 0 #> sample estimates: #> proportion parameter  #>            0.4359756  #>"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/addECxCI.html","id":null,"dir":"Reference","previous_headings":"","what":"adding ECx estimation and interval to the model output plot — addECxCI","title":"adding ECx estimation and interval to the model output plot — addECxCI","text":"adding ECx estimation interval model output plot","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/addECxCI.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"adding ECx estimation and interval to the model output plot — addECxCI","text":"","code":"addECxCI(   p = NULL,   object,   EDres = NULL,   trend = \"Decrease\",   endpoint = \"ErC\",   respLev = c(10, 20, 50),   textAjust.x = 0.1,   textAjust.y = 0.05,   useObsCtr = FALSE,   d0 = NULL,   textsize = 2,   lineheight = 1,   xmin = 0.05,   ... )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/addECxCI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"adding ECx estimation and interval to the model output plot — addECxCI","text":"p model plot ggplot2 output object fitted drc object EDres EDresults corresponding respLev trend \"Decrease\" \"Increase\" endpoint ErC EbC EyC EC LD respLev reponse levels textAjust.x label ECx textAjust.y label ECx useObsCtr whether use observed control mean d0 used control mean. textsize label text size lineheight errorbar height xmin confidence intervals wide including even negative values. ...","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/backCalcSE.html","id":null,"dir":"Reference","previous_headings":"","what":"Back Calculate Standard Error from Lognormal Parameters — backCalcSE","title":"Back Calculate Standard Error from Lognormal Parameters — backCalcSE","text":"function calculates standard error (SE) lognormally distributed variable based mean (mu) standard error logarithm (se).","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/backCalcSE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Back Calculate Standard Error from Lognormal Parameters — backCalcSE","text":"","code":"backCalcSE(se, mu, approximate = FALSE)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/backCalcSE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Back Calculate Standard Error from Lognormal Parameters — backCalcSE","text":"se standard error logarithm. mu mean logarithm. approximate Logical. TRUE, uses approximation; otherwise, calculates based variance lognormal distribution.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/backCalcSE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Back Calculate Standard Error from Lognormal Parameters — backCalcSE","text":"back-calculated standard error.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/backCalcSE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Back Calculate Standard Error from Lognormal Parameters — backCalcSE","text":"","code":"backCalcSE(se = 0.1, mu = 0) #> [1] 0.100753 backCalcSE(se = 0.1, mu = 0, approximate = TRUE) #> [1] 0.1"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcNW.html","id":null,"dir":"Reference","previous_headings":"","what":"Title — calcNW","title":"Title — calcNW","text":"Title","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcNW.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Title — calcNW","text":"","code":"calcNW(x, ED = \"ZG\")"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcNW.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Title — calcNW","text":"ED","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcSteepnessOverlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Steepness and overlap calcuation — calcSteepnessOverlap","title":"Steepness and overlap calcuation — calcSteepnessOverlap","text":"Steepness overlap calcuation","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcSteepnessOverlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Steepness and overlap calcuation — calcSteepnessOverlap","text":"","code":"calcSteepnessOverlap(   mod = NULL,   obj = NULL,   trend = \"Decrease\",   CI = \"inv\",   ... )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcSteepnessOverlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Steepness and overlap calcuation — calcSteepnessOverlap","text":"mod fitted object drm model fitting obj Calculated ED 10, 20, 50 object available. mod set NULL case trend \"Decrease\" \"Increase\" ... parameters passed ED.plus","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcSteepnessOverlap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Steepness and overlap calcuation — calcSteepnessOverlap","text":"table certainty potection level steepness models","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcSteepnessOverlap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Steepness and overlap calcuation — calcSteepnessOverlap","text":"","code":"if (FALSE) { # \\dontrun{  datTn<- subset(oecd201,Time==72)  mod <- drm(Yield~Concentration,data=datTn,fct=LL.3())  calcSteepnessOverlap(mod = mod, trend = \"Decrease\") } # }"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcTaronesTest.html","id":null,"dir":"Reference","previous_headings":"","what":"Tarones test for Stratified OR (integrated from https://github.com/the8thday/ClinStats/blob/master/R/Tarones.R) — calcTaronesTest","title":"Tarones test for Stratified OR (integrated from https://github.com/the8thday/ClinStats/blob/master/R/Tarones.R) — calcTaronesTest","text":"calcTaronesTest used test whether different strata different","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcTaronesTest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tarones test for Stratified OR (integrated from https://github.com/the8thday/ClinStats/blob/master/R/Tarones.R) — calcTaronesTest","text":"","code":"calcTaronesTest(mylist, referencerow = 2)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcTaronesTest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tarones test for Stratified OR (integrated from https://github.com/the8thday/ClinStats/blob/master/R/Tarones.R) — calcTaronesTest","text":"mylist list matrix Stratified Variable referencerow Unexposed row","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcTaronesTest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tarones test for Stratified OR (integrated from https://github.com/the8thday/ClinStats/blob/master/R/Tarones.R) — calcTaronesTest","text":"string list p-values","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcTaronesTest.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tarones test for Stratified OR (integrated from https://github.com/the8thday/ClinStats/blob/master/R/Tarones.R) — calcTaronesTest","text":"modified output return p-values, need metafor package run","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calcTaronesTest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tarones test for Stratified OR (integrated from https://github.com/the8thday/ClinStats/blob/master/R/Tarones.R) — calcTaronesTest","text":"","code":"mymatrix1 <- matrix(c(4,5,5,103),nrow=2,byrow=TRUE) colnames(mymatrix1) <- c(\"Disease\",\"Control\") rownames(mymatrix1) <- c(\"Exposure\",\"Unexposed\") mymatrix2 <- matrix(c(10,3,5,43),nrow=2,byrow=TRUE) colnames(mymatrix2) <- c(\"Disease\",\"Control\") rownames(mymatrix2) <- c(\"Exposure\",\"Unexposed\") mylist <- list(mymatrix1,mymatrix2) calcTaronesTest(mylist) #> [1] \"Pvalue for Tarone's test = 0.627420741721689\" #> $pval #> [1] 0.6274207 #>  #> $tarone #>  #> Equal-Effects Model (k = 2) #>  #> I^2 (total heterogeneity / total variability):  0.00% #> H^2 (total variability / sampling variability): 0.24 #>  #> Test for Heterogeneity:  #> Q(df = 1) = 0.2423, p-val = 0.6225 #>  #> Model Results (log scale): #>  #> estimate      se    zval    pval   ci.lb   ci.ub  #>   3.1355  0.5741  5.4613  <.0001  2.0102  4.2608  #>  #> Model Results (OR scale): #>  #> estimate   ci.lb    ci.ub  #>  23.0006  7.4652  70.8663  #>  #> Cochran-Mantel-Haenszel Test:    CMH = 36.6043, df = 1, p-val < 0.0001 #> Tarone's Test for Heterogeneity: X^2 =  0.2356, df = 1, p-val = 0.6274 #>  #>  if (FALSE) { # \\dontrun{ calcTaronesTest(mymatrix1) } # }"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calpha.test.html","id":null,"dir":"Reference","previous_headings":"","what":"C(alpha) test from the epiphy package. — calpha.test","title":"C(alpha) test from the epiphy package. — calpha.test","text":"C(alpha) test test binomial distribution alternative beta-binomial distribution.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calpha.test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"C(alpha) test from the epiphy package. — calpha.test","text":"","code":"calpha.test(x, ...)  # S3 method for class 'fisher' calpha.test(x, ...)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calpha.test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"C(alpha) test from the epiphy package. — calpha.test","text":"x output agg_index function method = \"fisher\" parameter. ... yet implemented.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calpha.test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"C(alpha) test from the epiphy package. — calpha.test","text":"kind object one returns stats chisq.test function example.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calpha.test.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"C(alpha) test from the epiphy package. — calpha.test","text":"based calculation test statistic, z, asymptotic standard normal distribution null hypothesis. one-sided (way alternative aggregation, just \"non-randomness\"), thus confidence level 95%, null hypothesis rejected z > 1.64. sampling units contain total number individuals, n, test statistic calculated : z = (n(N - 1)- Nn)/(2Nn(n - 1))^(1/2) N number sampling units, , Fisher's index aggregation incidence data.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calpha.test.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"C(alpha) test from the epiphy package. — calpha.test","text":"Neyman J. 1959. Optimal asymptotic tests composite statistical hypotheses. : Probability Statistics, 213-234. Wiley, New York. Tarone RE. 1979. Testing goodness fit binomial distribution. Biometrika, 66(3): 585-590.","code":""},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/calpha.test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"C(alpha) test from the epiphy package. — calpha.test","text":"","code":"# For incidence data: # my_incidence <- epiphy::incidence(epiphy::tobacco_viruses) # my_fisher <- epiphy::agg_index(my_incidence, method = \"fisher\") my_fisher <- structure(list(index = 3.14396182555326, name = \"Fisher's index of dispersion\", flavor = \"incidence\", N = 75L, n = 40L), class = c(\"fisher\", \"agg_index\")) calpha.test(my_fisher) #> Error in calpha.test(my_fisher): could not find function \"calpha.test\""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/collembola_juveniles.html","id":null,"dir":"Reference","previous_headings":"","what":"Fake data from collembola juveniles — collembola_juveniles","title":"Fake data from collembola juveniles — collembola_juveniles","text":"Fake data collembola juveniles","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/collembola_juveniles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fake data from collembola juveniles — collembola_juveniles","text":"","code":"collembola_juveniles"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/collembola_juveniles.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fake data from collembola juveniles — collembola_juveniles","text":"collembola_juveniles wide format","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/contEndpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"get Endpoint for continuous data according to results of a series of tests — contEndpoint","title":"get Endpoint for continuous data according to results of a series of tests — contEndpoint","text":"get Endpoint continuous data according results series tests","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/contEndpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get Endpoint for continuous data according to results of a series of tests — contEndpoint","text":"","code":"contEndpoint(   paov,   pks,   pnormal,   phomogeneity,   monotonicity,   william,   dunnett,   dunn,   jonckheere,   procedure = \"stepDown\",   doses = c(\"A\", \"B\", \"C\", \"D\") )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/contEndpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get Endpoint for continuous data according to results of a series of tests — contEndpoint","text":"paov Anova p-value pks Kolmogorov-Smirnov test p-value pnormal Normal test p-value phomogeneity Homogeneity variance test p-value monotonicity Monotonicity test p-value william Williams' test p-value dunnett Dunnett's test p-value dunn Dunn's test p-value jonckheere Jonckheere-Tepstra test p-value procedure step doses tested doses","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/contEndpoint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"get Endpoint for continuous data according to results of a series of tests — contEndpoint","text":"NOEC along attribute indicating test used.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/contEndpoint.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"get Endpoint for continuous data according to results of a series of tests — contEndpoint","text":"Purpose: contEndpoint function computes Observed Effect Concentration (NOEC) based results various statistical tests. evaluates tests' p-values determine statistical analysis appropriate.function first checks significance monotonicity test. Depending results, selects appropriate p-value relevant test (Dunnett, Dunn, Williams, Jonckheere) compute NOEC. function returns NOEC along attribute indicating test used. Original function used directly williams' test dunnett's test objects input, can cause conflicts using different functions get pvals.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/contEndpoint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"get Endpoint for continuous data according to results of a series of tests — contEndpoint","text":"","code":"contEndpoint(paov = 0.01, pks = 0.03, pnormal = 0.06, phomogeneity = 0.07, monotonicity = c(0.1,0.2), william = c(0.07, 0.06, 0.03, 0.01), dunnett = c(0.07, 0.06, 0.03, 0.01), dunn = c(0.07, 0.06, 0.03, 0.01), jonckheere = c(0.07, 0.06, 0.03, 0.01), procedure = \"stepDown\", doses = c(\"Control\",\"A\", \"B\", \"C\", \"D\")) #> [1] \"B\" #> attr(,\"test\") #> [1] \"Williams\""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/convert2Score.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Values to Scores — convert2Score","title":"Convert Values to Scores — convert2Score","text":"Converts non-positive numbers NA.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/convert2Score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Values to Scores — convert2Score","text":"","code":"convert2Score(Dvec)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/convert2Score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Values to Scores — convert2Score","text":"Dvec numeric vector converted.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/convert2Score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Values to Scores — convert2Score","text":"numeric vector non-positive values set NA.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/dose.p.glmmPQL.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Dose Probability for GLMM using PQL — dose.p.glmmPQL","title":"Calculate Dose Probability for GLMM using PQL — dose.p.glmmPQL","text":"function calculates dose probability generalized linear mixed model (GLMM) using Penalized Quasi-Likelihood (PQL).","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/dose.p.glmmPQL.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Dose Probability for GLMM using PQL — dose.p.glmmPQL","text":"","code":"dose.p.glmmPQL(obj, cf = 1:2, p = 0.5)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/dose.p.glmmPQL.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Dose Probability for GLMM using PQL — dose.p.glmmPQL","text":"obj fitted GLMM object class \"glmmPQL\". cf numeric vector specifying coefficients used calculation (default 1:2). p numeric value representing probability level (default 0.5).","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/dose.p.glmmPQL.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Dose Probability for GLMM using PQL — dose.p.glmmPQL","text":"structured object class \"glm.dose\" containing estimated dose probability standard error.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/dose.p.glmmPQL.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Dose Probability for GLMM using PQL — dose.p.glmmPQL","text":"","code":"# Assuming `glmm_model` is a fitted glmmPQL model library(MASS) glmm_model <- glmmPQL(yt ~ dose,random= ~1 | Obs,family= quasibinomial(link=\"logit\"),data=pvi_example) #> iteration 1 #> iteration 2 #> iteration 3 #> iteration 4 #> iteration 5 #> iteration 6 #> iteration 7 #> iteration 8 #> iteration 9 #> iteration 10 dose_result <- dose.p.glmmPQL(glmm_model) print(dose_result) #>              Dose       SE #> p = 0.5: 21.22061 1.455262"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/drcCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare models with multiple criteria — drcCompare","title":"Compare models with multiple criteria — drcCompare","text":"Compare models multiple criteria","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/drcCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare models with multiple criteria — drcCompare","text":"","code":"drcCompare(   modRes = NULL,   modList = NULL,   trend = \"Decrease\",   CI = c(\"delta\", \"inv\", \"bmd-inv\"),   ... )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/drcCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare models with multiple criteria — drcCompare","text":"...","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/drcHelper-package.html","id":null,"dir":"Reference","previous_headings":"","what":"drcHelper: Collection and verification of helper functions for dose-response analysis — drcHelper-package","title":"drcHelper: Collection and verification of helper functions for dose-response analysis — drcHelper-package","text":"Helper functions drc package dose-response analysis functions.","code":""},{"path":[]},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/drcHelper-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"drcHelper: Collection and verification of helper functions for dose-response analysis — drcHelper-package","text":"Maintainer: Zhenglei Gao zhenglei.gao@bayer.com (ORCID) Authors: Sean Ryan sryan@exponent.com","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEC50.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the EC50 Estimate from a Model — getEC50","title":"Get the EC50 Estimate from a Model — getEC50","text":"function calculates EC50 (dose 50% maximum effect observed) fitted model. supports generalized linear models (GLMs) generalized linear mixed models (GLMMs).","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEC50.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the EC50 Estimate from a Model — getEC50","text":"","code":"getEC50(mod, approximate = FALSE)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEC50.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the EC50 Estimate from a Model — getEC50","text":"mod fitted model object. can class \"glm\" \"glmmPQL\", models compatible ED function.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEC50.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the EC50 Estimate from a Model — getEC50","text":"data frame containing following columns: EC50 estimated EC50 value. lower lower bound 95% confidence interval EC50. upper upper bound 95% confidence interval EC50. se standard error EC50 estimate.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEC50.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the EC50 Estimate from a Model — getEC50","text":"","code":"# Assuming `fit` is a fitted glm model fit <- glm(yt ~ log(dose),family= quasibinomial(link=\"logit\"),data=pvi_example) ec50_result <- getEC50(fit) print(ec50_result) #>       EC50    lower    upper       se #> 1 12.91593 11.03755 15.11396 1.040638"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEndpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain Endpoint (NOEC) according to a series of p-values — getEndpoint","title":"Obtain Endpoint (NOEC) according to a series of p-values — getEndpoint","text":"Obtain Endpoint (NOEC) according series p-values","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEndpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain Endpoint (NOEC) according to a series of p-values — getEndpoint","text":"","code":"getEndpoint(pvals, doses = c(\"Control\", \"B\", \"C\", \"D\"), procedure = \"stepDown\")"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEndpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain Endpoint (NOEC) according to a series of p-values — getEndpoint","text":"pvals pvals tests doses corresponding doses procedure procedure obtain NOEC","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEndpoint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain Endpoint (NOEC) according to a series of p-values — getEndpoint","text":"NOEC","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEndpoint.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Obtain Endpoint (NOEC) according to a series of p-values — getEndpoint","text":"p-values NA, initializes zeros. fewer p-values doses, pads p-values zeros.checks doses significant comparing p-values threshold (0.05).Depending specified procedure, either steps highest dose find NOEC steps lowest dose..","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getEndpoint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Obtain Endpoint (NOEC) according to a series of p-values — getEndpoint","text":"","code":"pvals <- c(0.01, 0.03, 0.07) doses <- c(\"Control\", \"B\", \"C\", \"D\") getEndpoint(pvals, doses) #> [1] \"D\""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getModelName.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Model Name — getModelName","title":"Get Model Name — getModelName","text":"Get Model Name","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getModelName.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Model Name — getModelName","text":"","code":"getModelName(fname = NULL)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getModelName.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Model Name — getModelName","text":"fname","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getModelName.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Model Name — getModelName","text":"model name","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getModelName.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Model Name — getModelName","text":"","code":"getModelName(\"LL.2\") #> Error in if (grep(\"EXD\", fname)) {    noParm <- stringr::str_split(fname, fixed(\".\"))[[1]][2]    ModelName <- paste0(noParm, \"-parameter exponential decay model\")} else {    ModelName <- getMeanFunctions(fname = resComp$Model[1])    ModelName <- paste0(ModelName[[1]]$noParm, \"-parameter \",         ModelName[[1]]$text)}: argument is of length zero"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getwilliamRes.html","id":null,"dir":"Reference","previous_headings":"","what":"get from william res accept/reject — getwilliamRes","title":"get from william res accept/reject — getwilliamRes","text":"get william res accept/reject","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getwilliamRes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get from william res accept/reject — getwilliamRes","text":"","code":"getwilliamRes(william, n = NULL)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getwilliamRes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get from william res accept/reject — getwilliamRes","text":"william william test results n number hypotheses tested. description","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getwilliamRes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"get from william res accept/reject — getwilliamRes","text":"vector accept rejects.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/getwilliamRes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"get from william res accept/reject — getwilliamRes","text":"","code":"## Example from Sachs (1997, p. 402) x <- c(106, 114, 116, 127, 145,        110, 125, 143, 148, 151,        136, 139, 149, 160, 174) g <- gl(3,5) levels(g) <- c(\"0\", \"I\", \"II\")  ## Williams Test res <- williamsTest(x ~ g) #> Error in williamsTest(x ~ g): could not find function \"williamsTest\" getwilliamRes(res) #> Error: object 'res' not found"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/hamilton.html","id":null,"dir":"Reference","previous_headings":"","what":"This is hamilton data — hamilton","title":"This is hamilton data — hamilton","text":"Example dose-response data given Hamilton (1977). Note , per Hamilton (1978), confidence intervals given Hamilton (1977) data sets incorrect.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/hamilton.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This is hamilton data — hamilton","text":"","code":"hamilton"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/hamilton.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"This is hamilton data — hamilton","text":"list containing ten data frames: dr1a, dr1b, dr1c, dr1d, dr1e, dr4a, dr4b, dr4c, dr4d, dr4e,","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/hamilton.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"This is hamilton data — hamilton","text":"Hamilton, 1977.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/hamilton.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"This is hamilton data — hamilton","text":"https://github.com/brsr/tsk","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/hamilton.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"This is hamilton data — hamilton","text":"B R S Recht","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/invlogxp.html","id":null,"dir":"Reference","previous_headings":"","what":"inverse transformation of logxp — invlogxp","title":"inverse transformation of logxp — invlogxp","text":"inverse transformation logxp","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/invlogxp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"inverse transformation of logxp — invlogxp","text":"","code":"invlogxp(x, a)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/invlogxp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"inverse transformation of logxp — invlogxp","text":"x numeric vector numeric vector","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/invlogxp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"inverse transformation of logxp — invlogxp","text":"inverse log x ","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/invlogxp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"inverse transformation of logxp — invlogxp","text":"","code":"invlogxp(3, 1) #> [1] 19.08554"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/logxp.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper function for scaling x-axis — logxp","title":"Helper function for scaling x-axis — logxp","text":"Helper function scaling x-axis","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/logxp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper function for scaling x-axis — logxp","text":"","code":"logxp(x, a)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/logxp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper function for scaling x-axis — logxp","text":"x vector numeric vector nummeric","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/logxp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helper function for scaling x-axis — logxp","text":"log(x+)","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/logxp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Helper function for scaling x-axis — logxp","text":"","code":"logxp(0, 1) #> [1] 0"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/monotonicityTest.html","id":null,"dir":"Reference","previous_headings":"","what":"Testing Monotonicity — monotonicityTest","title":"Testing Monotonicity — monotonicityTest","text":"Testing Monotonicity","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/monotonicityTest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Testing Monotonicity — monotonicityTest","text":"","code":"monotonicityTest(Data, Treatment, Response)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/monotonicityTest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Testing Monotonicity — monotonicityTest","text":"Data Data frame Treatment name treatment variable Response name response variable","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/monotonicityTest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Testing Monotonicity — monotonicityTest","text":"monotonicity table","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/monotonicityTest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Testing Monotonicity — monotonicityTest","text":"","code":"if (FALSE) { # \\dontrun{ x <- c(106, 114, 116, 127, 145,110, 125, 143, 148, 151, 136, 139, 149, 160, 174) g <- gl(3,5) levels(g) <- c(\"0\", \"I\", \"II\") monotonicityTest(data.frame(treatment_var = g,response_var=x), \"treatment_var\", \"response_var\") mock_data <- data.frame( treatment_var = factor(rep(c(\"Control\", \"Dose1\", \"Dose2\", \"Dose3\"), each = 10)), response_var = c(rnorm(10, mean = 5), rnorm(10, mean = 7), rnorm(10, mean = 8), rnorm(10, mean = 10)) ) monotonicityTest(mock_data, \"treatment_var\", \"response_var\") } # }"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/mselect.ED.html","id":null,"dir":"Reference","previous_headings":"","what":"ECx calculation together with normalized width proposed by EFSA SO. — mselect.ED","title":"ECx calculation together with normalized width proposed by EFSA SO. — mselect.ED","text":"ECx calculation together normalized width proposed EFSA .","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/mselect.ED.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ECx calculation together with normalized width proposed by EFSA SO. — mselect.ED","text":"","code":"mselect.ED(modList, respLev = c(10, 20, 50), trend = \"Decrease\", ...)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/mselect.ED.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ECx calculation together with normalized width proposed by EFSA SO. — mselect.ED","text":"modList list models ...","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/mselect.ZG.html","id":null,"dir":"Reference","previous_headings":"","what":"Deprecated helper function for mselect — mselect.ZG","title":"Deprecated helper function for mselect — mselect.ZG","text":"Deprecated helper function mselect","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/mselect.ZG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated helper function for mselect — mselect.ZG","text":"","code":"mselect.ZG(   object = NULL,   fctList = NULL,   nested = FALSE,   sorted = c(\"IC\", \"Res var\", \"Lack of fit\", \"no\"),   linreg = FALSE,   icfct = AIC,   respCol = \"effect\",   doseCol = \"dose\",   data = NULL,   type = \"continuous\",   additionalReliability = c(\"EFSA\") )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/mselect.ZG.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Deprecated helper function for mselect — mselect.ZG","text":"Due old ECxHelper development mselect.plus defined mselect.ZG","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/mselect.plus.html","id":null,"dir":"Reference","previous_headings":"","what":"added functionality for mselect — mselect.plus","title":"added functionality for mselect — mselect.plus","text":"added functionality mselect","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/mselect.plus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"added functionality for mselect — mselect.plus","text":"","code":"mselect.plus(   object = NULL,   fctList = NULL,   nested = FALSE,   sorted = c(\"IC\", \"Res var\", \"Lack of fit\", \"no\"),   linreg = FALSE,   icfct = AIC,   respCol = \"effect\",   doseCol = \"dose\",   data = NULL,   type = \"continuous\",   additionalReliability = c(\"EFSA\") )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/mselect.plus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"added functionality for mselect — mselect.plus","text":"additionalReliability additional reliability need calculated","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/mselect.plus.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"added functionality for mselect — mselect.plus","text":"mselect.plus","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/oecd201.html","id":null,"dir":"Reference","previous_headings":"","what":"An example dataset from study type OECD 201 — oecd201","title":"An example dataset from study type OECD 201 — oecd201","text":"example dataset study type OECD 201","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/oecd201.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"An example dataset from study type OECD 201 — oecd201","text":"data frame 112 rows 14 columns","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/oecd201.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"An example dataset from study type OECD 201 — oecd201","text":"data originally provided Ecotox colleague Andreas preprocessed GrowthRate included.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/oecd201.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"An example dataset from study type OECD 201 — oecd201","text":"OECD Test . 201: Freshwater Alga Cyanobacteria, Growth Inhibition Test https://www.oecd.org/en/publications/test--201-alga-growth-inhibition-test_9789264069923-en.html","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/oecd201.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"An example dataset from study type OECD 201 — oecd201","text":"BCS","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/pavaMean.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Means Using the Pool Adjacent Violators Algorithm (PAVA) — pavaMean","title":"Calculate Means Using the Pool Adjacent Violators Algorithm (PAVA) — pavaMean","text":"function computes adjusted means standard errors groups defined factor variable using Pool Adjacent Violators Algorithm (PAVA).","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/pavaMean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Means Using the Pool Adjacent Violators Algorithm (PAVA) — pavaMean","text":"","code":"pavaMean(x, g, alternative = \"greater\")"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/pavaMean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Means Using the Pool Adjacent Violators Algorithm (PAVA) — pavaMean","text":"x numeric vector observations. g factor variable defines groups calculate means. alternative character string specifying alternative hypothesis. Options \"greater\" (default) \"less\".","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/pavaMean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Means Using the Pool Adjacent Violators Algorithm (PAVA) — pavaMean","text":"data frame containing following columns: pavaMean adjusted means group. SE.diff standard errors differences.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/pavaMean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Means Using the Pool Adjacent Violators Algorithm (PAVA) — pavaMean","text":"","code":"# Example usage: x <- c(1, 2, 3, 4, 5, 6) g <- factor(c(\"A\", \"A\", \"B\", \"B\", \"C\", \"C\")) result <- pavaMean(x, g) #> Loading required package: PMCMRplus #>  #> Attaching package: ‘PMCMRplus’ #> The following object is masked from ‘package:drc’: #>  #>     algae print(result) #>   pavaMean   SE.diff #> A      1.5 0.7071068 #> B      3.5 0.7071068 #> C      5.5 0.7071068 x <- c(106, 114, 116, 127, 145,        110, 125, 143, 148, 151,        136, 139, 149, 160, 174) g <- gl(3,5) levels(g) <- c(\"0\", \"I\", \"II\") pavaMean(x,g) #>    pavaMean SE.diff #> 0     121.6 10.1712 #> I     135.4 10.1712 #> II    151.6 10.1712"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/plot.edList.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the ECx estimation and confidence intervals from the list of models. — plot.edList","title":"Plot the ECx estimation and confidence intervals from the list of models. — plot.edList","text":"Plot ECx estimation confidence intervals list models.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/plot.edList.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the ECx estimation and confidence intervals from the list of models. — plot.edList","text":"","code":"# S3 method for class 'edList' plot(edList, fctNames, ...)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/plot.edList.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the ECx estimation and confidence intervals from the list of models. — plot.edList","text":"edList ECx lists fitted set models fctNames fct function names ... additional parameters","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/plot.modList.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a list of models together. — plot.modList","title":"Plot a list of models together. — plot.modList","text":"Plot list models together.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/plot.modList.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a list of models together. — plot.modList","text":"","code":"# S3 method for class 'modList' plot(   modList,   respLev = NULL,   data = NULL,   xmin,   xmax,   scale = c(\"logx\", \"logy\", \"logxy\", \"orig\"),   npts = 100,   plot_respLev = FALSE,   xbreaks = NULL,   ymin = NULL,   ymax = NULL,   ... )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/plot.modList.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a list of models together. — plot.modList","text":"modList list drc models. respLev calculated ECx levels, example, c(10,20,50). data data used fit set models xmin minimum value xaxis, greater 0 fit log scale. xmax maximum value xaxis, can missing. scale \"logx\", \"logy\", \"logxy\", \"orig\". npts number points used plotting prediction confidence bands plot_respLev logical, whether add estimated ECx CIs xbreaks breaks x-axis labeling ... addition parameters passed plotting functions user input use want use data provided modList","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot1.html","id":null,"dir":"Reference","previous_headings":"","what":"Preliminary Plot 1 for Dose Response Data — prelimPlot1","title":"Preliminary Plot 1 for Dose Response Data — prelimPlot1","text":"function generates scatter plot response nominal dose.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preliminary Plot 1 for Dose Response Data — prelimPlot1","text":"","code":"prelimPlot1(   testdata,   ylab = \"Response\",   xlab = \"Test Concentration [nominal, mg a.s./L]\",   title = \"Measured Variable\" )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preliminary Plot 1 for Dose Response Data — prelimPlot1","text":"testdata data frame containing dose response data. ylab string y-axis label. Default \"Response\". xlab string x-axis label. Default \"Test Concentration nominal, mg .s./L\". title string plot title. Default \"Measured Variable\".","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preliminary Plot 1 for Dose Response Data — prelimPlot1","text":"ggplot object.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot2.html","id":null,"dir":"Reference","previous_headings":"","what":"Preliminary Plot 2 for Dose Response Data — prelimPlot2","title":"Preliminary Plot 2 for Dose Response Data — prelimPlot2","text":"function generates scatter plot response dose log1p scale.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preliminary Plot 2 for Dose Response Data — prelimPlot2","text":"","code":"prelimPlot2(   testdata,   ylab = \"Response\",   xlab = \"Test Concentration [nominal, mg a.s./L]\",   title = \"Measured Variable\",   dosecol = \"Dose\" )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preliminary Plot 2 for Dose Response Data — prelimPlot2","text":"testdata data frame containing dose response data. ylab string y-axis label. Default \"Response\". xlab string x-axis label. Default \"Test Concentration nominal, mg .s./L\". title string plot title. Default \"Measured Variable\".","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preliminary Plot 2 for Dose Response Data — prelimPlot2","text":"ggplot object.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot3.html","id":null,"dir":"Reference","previous_headings":"","what":"Preliminary Plot 3 for Dose Response Data — prelimPlot3","title":"Preliminary Plot 3 for Dose Response Data — prelimPlot3","text":"function generates scatter plot response nominal dose.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot3.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preliminary Plot 3 for Dose Response Data — prelimPlot3","text":"","code":"prelimPlot3(   testdata,   ylab = \"Response\",   xlab = \"Test Concentration [nominal, mg a.s./L]\",   title = \"Measured Variable\",   a = 1.96 )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot3.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preliminary Plot 3 for Dose Response Data — prelimPlot3","text":"testdata data frame containing dose response data. ylab string y-axis label. Default \"Response\". xlab string x-axis label. Default \"Test Concentration nominal, mg .s./L\". title string plot title. Default \"Measured Variable\". quantile corresponding CI mean. default qnorm(0.975).","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimPlot3.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preliminary Plot 3 for Dose Response Data — prelimPlot3","text":"ggplot object.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimSummary.html","id":null,"dir":"Reference","previous_headings":"","what":"Preliminary Summary of Dose Response Data — prelimSummary","title":"Preliminary Summary of Dose Response Data — prelimSummary","text":"function calculates mean response, standard deviation, percent inhibition, coefficient variation dose level.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimSummary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preliminary Summary of Dose Response Data — prelimSummary","text":"","code":"prelimSummary(testdata)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimSummary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preliminary Summary of Dose Response Data — prelimSummary","text":"testdata data frame containing dose response data.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prelimSummary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preliminary Summary of Dose Response Data — prelimSummary","text":"data frame summarizing mean, standard deviation, percent inhibition, coefficient variation dose.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prepDataRSCABS.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepares data for an RSCABS analysis — prepDataRSCABS","title":"Prepares data for an RSCABS analysis — prepDataRSCABS","text":"Prepares data RSCABS analysis","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prepDataRSCABS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepares data for an RSCABS analysis — prepDataRSCABS","text":"","code":"prepDataRSCABS(   Effect = \"\",   Data = {  },   Treatment = \"\",   Replicate = \"\" )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prepDataRSCABS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepares data for an RSCABS analysis — prepDataRSCABS","text":"Effect endpoint converted. Data tall formatted data set. Treatment name treatment variable. Replicate name replicate variable.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prepDataRSCABS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepares data for an RSCABS analysis — prepDataRSCABS","text":"Returns list containing: x..j matrix containing number observed \"successes\" replicate treatment j. n..j matrix containing number observations replicate treatment j. m.matrix number replicates treatment-replicate combination. K.max maximum severity score endpoint.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/prepDataRSCABS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prepares data for an RSCABS analysis — prepDataRSCABS","text":"Joe Swintek","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/print.drcComp.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for drcComp Objects — print.drcComp","title":"Print Method for drcComp Objects — print.drcComp","text":"function provides custom print method objects class drcComp. prints Comparison EFSA components object.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/print.drcComp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for drcComp Objects — print.drcComp","text":"","code":"# S3 method for class 'drcComp' print(x, ...)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/print.drcComp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for drcComp Objects — print.drcComp","text":"x object class drcComp. ... Additional arguments (currently used).","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/print.drcComp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Method for drcComp Objects — print.drcComp","text":"function prints Comparison EFSA components object default.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/pvi_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Fake data as an example of PVI data — pvi_example","title":"Fake data as an example of PVI data — pvi_example","text":"Fake data example PVI data","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/pvi_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fake data as an example of PVI data — pvi_example","text":"","code":"pvi_example"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/pvi_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fake data as an example of PVI data — pvi_example","text":"collembola_juveniles wide format","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/reshape_drcData.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape the wide data to long data — reshape_drcData","title":"Reshape the wide data to long data — reshape_drcData","text":"Reshape wide data long data","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/reshape_drcData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape the wide data to long data — reshape_drcData","text":"","code":"reshape_drcData(dat)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/reshape_drcData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape the wide data to long data — reshape_drcData","text":"dat data table Replicate columns dose groups","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/reshape_drcData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape the wide data to long data — reshape_drcData","text":"long format dat","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/reshape_drcData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reshape the wide data to long data — reshape_drcData","text":"","code":"reshape_drcData(collembola_juveniles) ## note collembola_juveniles is fake data. #> Loading required package: dplyr #>  #> Attaching package: ‘dplyr’ #> The following object is masked from ‘package:MASS’: #>  #>     select #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union #> Error in pivot_longer(., -Replicates, names_to = \"Treatment\", values_to = \"Response\"): could not find function \"pivot_longer\""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/runRSCABS.html","id":null,"dir":"Reference","previous_headings":"","what":"Run RSCABS test — runRSCABS","title":"Run RSCABS test — runRSCABS","text":"Runs Rao-Scott adjusted Cochran-Armitage trend test slices (RSCABS) analysis.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/runRSCABS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run RSCABS test — runRSCABS","text":"","code":"runRSCABS(Data, Treatment, Replicate = \"\", Effects = \"\", test.type = \"RS\")"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/runRSCABS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run RSCABS test — runRSCABS","text":"Data standard data set tall format.  Every row indicates organism. data set must contain columns treatment level every tested histological endpoint. Treatment name column contains information treatment level. Increasing values indicate higher treatments. Replicate name column contains information replicate structure.  replicate specified default running \"CA\" test type. Effects endpoint tested.  Defaults columns integers less 20. analysis assumes higher scores indicate worse outcome. test.type Indicate type analysis performed.  Use \"RS\" select Rao-Scott adjustment Cochran-Armitage test \"CA\" ignore adjustment.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/runRSCABS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run RSCABS test — runRSCABS","text":"table test results treatment injury score.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/runRSCABS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Run RSCABS test — runRSCABS","text":"Green, John W. Springer, Timothy . Saulnier, Amy N. Swintek, Joe, (2014) Statistical analysis histopathological endpoints. Environmental Toxicology Chemistry, 33(5), 1108-1116","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/runRSCABS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Run RSCABS test — runRSCABS","text":"Joe Swintek","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/runRSCABS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run RSCABS test — runRSCABS","text":"","code":"if (FALSE) { # \\dontrun{ ## Not run: #Take the subset corresponding to F0-females of 16 weeks of age data(exampleHistData) exampleHistData.sub<-exampleHistData[which(exampleHistData$Generation=='F2' &              exampleHistData$Genotypic_Sex=='Female' & exampleHistData$Age=='16_wk' ),  ]  #Run RSCABS  eampleResults<-runRSCABS(exampleHistData.sub,'Treatment',                           'Replicate',test.type='RS') } # }"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/simplifyTreatment.html","id":null,"dir":"Reference","previous_headings":"","what":"Resolve the excel datasheet number issues — simplifyTreatment","title":"Resolve the excel datasheet number issues — simplifyTreatment","text":"simplifyTreatment function designed handle simplify treatment vector, can either factor character vector. primary goal function resolve issues numeric precision often arise Excel datasheets, numbers represented excessive decimal places, like 12.300000000000001 8.3579999999999.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/simplifyTreatment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resolve the excel datasheet number issues — simplifyTreatment","text":"","code":"simplifyTreatment(trt)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/simplifyTreatment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resolve the excel datasheet number issues — simplifyTreatment","text":"trt treatment vector, either factor character","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/simplifyTreatment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resolve the excel datasheet number issues — simplifyTreatment","text":"simplified vector, either factor character vector, depending x","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/simplifyTreatment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Resolve the excel datasheet number issues — simplifyTreatment","text":"","code":"x <- structure(c(   1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 3L, 3L, 3L,   3L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 1L, 1L, 1L,   1L, 1L, 1L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 5L,   5L, 5L, 5L, 6L, 6L, 6L, 6L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L,   2L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 6L, 6L, 6L,   6L ), levels = c(   \"Control\", \"1\", \"3.51\", \"12.300000000000001\",   \"42.9\", \"150\" ), class = \"factor\") simplifyTreatment(x) #>  [1] Control Control Control Control Control Control 1       1       1       #> [10] 1       3.51    3.51    3.51    3.51    12.3    12.3    12.3    12.3    #> [19] 42.9    42.9    42.9    42.9    150     150     150     150     Control #> [28] Control Control Control Control Control 1       1       1       1       #> [37] 3.51    3.51    3.51    3.51    12.3    12.3    12.3    12.3    42.9    #> [46] 42.9    42.9    42.9    150     150     150     150     Control Control #> [55] Control Control Control Control 1       1       1       1       3.51    #> [64] 3.51    3.51    3.51    12.3    12.3    12.3    12.3    42.9    42.9    #> [73] 42.9    42.9    150     150     150     150     #> Levels: Control 1 3.51 12.3 42.9 150"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepDownRSCABS.html","id":null,"dir":"Reference","previous_headings":"","what":"Performs the step down aspect of RSCABS — stepDownRSCABS","title":"Performs the step down aspect of RSCABS — stepDownRSCABS","text":"Performs step aspect RSCABS","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepDownRSCABS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performs the step down aspect of RSCABS — stepDownRSCABS","text":"","code":"stepDownRSCABS(TestK, x.i.j, n.i.j, m.i, Effect, test.type)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepDownRSCABS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performs the step down aspect of RSCABS — stepDownRSCABS","text":"TestK severity score tested x..j Matrix containing number observed \"successes\" replicate treatment j. n..j Matrix containing number observations replicate treatment j. m.Matrix number units treatment/replicate combination. Effect end point tested. test.type Indicate type  analysis performed. Use \"RS\" select Rao-Scott adjustment Cochran-Armitage test \"CA\" ignore adjustment","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepDownRSCABS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Performs the step down aspect of RSCABS — stepDownRSCABS","text":"Result.K intermediary result.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepDownRSCABS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Performs the step down aspect of RSCABS — stepDownRSCABS","text":"Joe Swintek","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepKRSCABS.html","id":null,"dir":"Reference","previous_headings":"","what":"plotRSCABS — stepKRSCABS","title":"plotRSCABS — stepKRSCABS","text":"Steps severity score given effect","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepKRSCABS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plotRSCABS — stepKRSCABS","text":"","code":"stepKRSCABS(Effect, Data.Prep, Treatment, Replicate, test.type)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepKRSCABS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plotRSCABS — stepKRSCABS","text":"Effect Endpoint tested. Data.Prep Data prepared prepDataRSCABS. Treatment Name treatment variable. Replicate Name replicate variable. test.type Indicate type analysis performed.  Use \"RS\" select Rao-Scott adjustment Cochran-Armitage test \"CA\" ignore adjustment.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepKRSCABS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plotRSCABS — stepKRSCABS","text":"Results.Effect Results.Effect intermediary step results.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepKRSCABS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"plotRSCABS — stepKRSCABS","text":"internal function stepping severity score endpoint.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/stepKRSCABS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"plotRSCABS — stepKRSCABS","text":"Joe Swintek","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/summaryZG.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Williams Test results. — summaryZG","title":"Summary Williams Test results. — summaryZG","text":"Summary Williams Test results.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/summaryZG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary Williams Test results. — summaryZG","text":"","code":"summaryZG(object, verbose = F, ...)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/summaryZG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary Williams Test results. — summaryZG","text":"object William test result object ... additional parameters passed function, placeholder","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/summaryZG.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary Williams Test results. — summaryZG","text":"William test results","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/summaryZG.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary Williams Test results. — summaryZG","text":"","code":"## Example from Sachs (1997, p. 402) x <- c(106, 114, 116, 127, 145,        110, 125, 143, 148, 151,        136, 139, 149, 160, 174) g <- gl(3,5) levels(g) <- c(\"0\", \"I\", \"II\")  ## Williams Test res <- williamsTest(x ~ g) summaryZG(res) ## return a data frame instead of a list. #>                t'-value df t'-crit decision alpha #> mu1 - ctr <= 0    1.357 12   1.782   accept  0.05 #> mu2 - ctr <= 0    2.950 12   1.873   reject  0.05"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/treatment2dose.html","id":null,"dir":"Reference","previous_headings":"","what":"Change Treatment groups to numerical dose — treatment2dose","title":"Change Treatment groups to numerical dose — treatment2dose","text":"Change Treatment groups numerical dose","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/treatment2dose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Change Treatment groups to numerical dose — treatment2dose","text":"","code":"treatment2dose(x)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/treatment2dose.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Change Treatment groups to numerical dose — treatment2dose","text":"x treatment groups numbers \"Control\"","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/treatment2dose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Change Treatment groups to numerical dose — treatment2dose","text":"numeric vector dose","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/treatment2dose.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Change Treatment groups to numerical dose — treatment2dose","text":"","code":"treatment2dose(c(\"Control\",\"0.1\",\"1\",\"10\")) #> [1]  0.0  0.1  1.0 10.0"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.data.frame.html","id":null,"dir":"Reference","previous_headings":"","what":"TSK Analysis for Data Frame Input — tsk.data.frame","title":"TSK Analysis for Data Frame Input — tsk.data.frame","text":"function performs TSK analysis data frame input.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.data.frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TSK Analysis for Data Frame Input — tsk.data.frame","text":"","code":"# S3 method for class 'data.frame' tsk(input, control = 0, trim = 0, conf.level = 0.95, use.log.doses = TRUE, ...)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.data.frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"TSK Analysis for Data Frame Input — tsk.data.frame","text":"input data frame containing columns x (doses), n (total counts), r (response counts). control numeric value indicating control dose (default 0). trim numeric value indicating trim level (default 0). conf.level numeric value indicating confidence level (default 0.95). use.log.doses logical value indicating whether use log-transformed doses (default TRUE). ... Additional arguments passed function.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.data.frame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"TSK Analysis for Data Frame Input — tsk.data.frame","text":"result TSK analysis.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.html","id":null,"dir":"Reference","previous_headings":"","what":"Trimmed Spearman-Karber Method by brsr — tsk","title":"Trimmed Spearman-Karber Method by brsr — tsk","text":"Trimmed Spearman-Karber Method brsr","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trimmed Spearman-Karber Method by brsr — tsk","text":"","code":"tsk(...)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trimmed Spearman-Karber Method by brsr — tsk","text":"... inputs","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trimmed Spearman-Karber Method by brsr — tsk","text":"tsk estimations","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Trimmed Spearman-Karber Method by brsr — tsk","text":"https://github.com/brsr/tsk","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.numeric.html","id":null,"dir":"Reference","previous_headings":"","what":"TSK Analysis for Numeric Input — tsk.numeric","title":"TSK Analysis for Numeric Input — tsk.numeric","text":"function performs TSK analysis numeric input.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.numeric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TSK Analysis for Numeric Input — tsk.numeric","text":"","code":"# S3 method for class 'numeric' tsk(   x,   n,   r,   control = 0,   trim = 0,   conf.level = 0.95,   use.log.doses = TRUE,   ... )"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.numeric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"TSK Analysis for Numeric Input — tsk.numeric","text":"x numeric vector doses. n numeric vector total counts. r numeric vector response counts. control numeric value indicating control dose (default 0). trim numeric value indicating trim level (default 0). conf.level numeric value indicating confidence level (default 0.95). use.log.doses logical value indicating whether use log-transformed doses (default TRUE). ... Additional arguments passed function.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/tsk.numeric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"TSK Analysis for Numeric Input — tsk.numeric","text":"result TSK analysis.","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/wide2long.html","id":null,"dir":"Reference","previous_headings":"","what":"change from wide format to long format — wide2long","title":"change from wide format to long format — wide2long","text":"change wide format long format","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/wide2long.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"change from wide format to long format — wide2long","text":"","code":"wide2long(widedat, cnames = 1, repnames = 1)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/wide2long.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"change from wide format to long format — wide2long","text":"widedat data wide format cnames whether use 1st row column names, 1 means 1st row need converted headers whether 1st column Replicates column","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/wide2long.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"change from wide format to long format — wide2long","text":"dataframe long format","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/wide2long.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"change from wide format to long format — wide2long","text":"","code":"# Create a sample wide dataset widedat <- data.frame(   Replicates = c(\"Rep1\", \"Rep2\", \"Rep3\"),   Treatment1 = c(1.1, 2.2, 3.3),   Treatment2 = c(4.4, 5.5, 6.6) ) drcHelper:::wide2long(widedat, cnames = FALSE) #> # A tibble: 6 × 3 #>   Replicates Treatment  Response #>   <chr>      <chr>         <dbl> #> 1 Rep1       Treatment1      1.1 #> 2 Rep1       Treatment2      4.4 #> 3 Rep2       Treatment1      2.2 #> 4 Rep2       Treatment2      5.5 #> 5 Rep3       Treatment1      3.3 #> 6 Rep3       Treatment2      6.6"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/williamsTest_JG.html","id":null,"dir":"Reference","previous_headings":"","what":"Williams Test from the StatCharrms Package — williamsTest_JG","title":"Williams Test from the StatCharrms Package — williamsTest_JG","text":"Williams Test StatCharrms Package","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/williamsTest_JG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Williams Test from the StatCharrms Package — williamsTest_JG","text":"","code":"williamsTest_JG(df, resp, trt, direction = \"decreasing\", SeIn = NULL)"},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/williamsTest_JG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Williams Test from the StatCharrms Package — williamsTest_JG","text":"df data frame resp name response string trt name response string direction direction test 'decreasing' 'increasing' SeIn standard error, default program selected. WilliamsTest can take different value case repeated measures","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/williamsTest_JG.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Williams Test from the StatCharrms Package — williamsTest_JG","text":"Williams' test result","code":""},{"path":"https://bayer-group.github.io/drcHelper/index.html/reference/williamsTest_JG.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Williams Test from the StatCharrms Package — williamsTest_JG","text":"","code":"## Williams Test x <- c(106, 114, 116, 127, 145,110, 125, 143, 148, 151, 136, 139, 149, 160, 174) g <- gl(3,5) levels(g) <- c(\"0\", \"I\", \"II\") PMCMRplus::williamsTest(x ~ g) #>  #> \t Williams trend test  #>  #> data:  x by g  #> alternative hypothesis:  greater  #>  #> H0 #>                t'-value df t'-crit decision alpha #> mu1 - ctr <= 0    1.357 12   1.782   accept  0.05 #> mu2 - ctr <= 0    2.950 12   1.873   reject  0.05 #> --- williamsTest_JG(data.frame(treatment_var = g,response_var=x),\"response_var\",\"treatment_var\",direction=\"increasing\") #>   treatment_var Y.Tilde    Y0 Se.Diff DF  Will TCrit Signif #> 2            II   151.6 121.6   10.17 12 2.950 1.873      * #> 1             I   135.4 121.6   10.17 12 1.357 1.782      ."}]
